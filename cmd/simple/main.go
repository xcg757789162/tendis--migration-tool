package main

import (
	"context"
	"encoding/json"
	"fmt"
	"net/http"
	"runtime"
	"sort"
	"strconv"
	"strings"
	"sync"
	"sync/atomic"
	"time"

	"github.com/go-redis/redis/v8"
	"github.com/google/uuid"
	"tendis-migrate/pkg/logger"
)

type Task struct {
	ID             string  `json:"id"`
	Name           string  `json:"name"`
	Status         string  `json:"status"`
	Progress       float64 `json:"progress"`
	SourceCluster  string  `json:"source_cluster"`
	TargetCluster  string  `json:"target_cluster"`
	SourcePassword string  `json:"-"`
	TargetPassword string  `json:"-"`
	MigrationMode  string  `json:"migration_mode"` // full_only, full_and_incremental
	CreatedAt      string  `json:"created_at"`
	UpdatedAt      string  `json:"updated_at"`
	FullStartAt    string  `json:"full_start_at,omitempty"`    // å…¨é‡è¿ç§»å¼€å§‹æ—¶é—´
	IncrStartAt    string  `json:"incr_start_at,omitempty"`    // å¢é‡è¿ç§»å¼€å§‹æ—¶é—´
	StartedAt      string  `json:"started_at,omitempty"`       // ä»»åŠ¡å¼€å§‹æ—¶é—´ï¼ˆç”¨äºè®¡ç®—å·²è€—æ—¶é—´ï¼‰
	KeysTotal      int64   `json:"keys_total"`
	KeysMigrated   int64   `json:"keys_migrated"`
	KeysFailed     int64   `json:"keys_failed"`
	KeysSkipped    int64   `json:"keys_skipped"`
	KeysFiltered   int64   `json:"keys_filtered"`
	BytesMigrated  int64   `json:"bytes_migrated"`
	BytesTotal     int64   `json:"bytes_total"`
	Speed          int64   `json:"speed"`
	Phase          string  `json:"phase"` // full, incremental, completed
	ActiveWorkers  int     `json:"active_workers,omitempty"`   // å½“å‰æ´»è·ƒWorkeræ•°
	// é…ç½®é€‰é¡¹
	Options *TaskOptions `json:"options,omitempty"`
	// å†…éƒ¨å­—æ®µï¼ˆä¸åºåˆ—åŒ–ï¼‰
	workerPool *DynamicWorkerPool `json:"-"`
}

// TaskTemplate ä»»åŠ¡æ¨¡æ¿
type TaskTemplate struct {
	ID            string       `json:"id"`
	Name          string       `json:"name"`
	Description   string       `json:"description"`
	SourceCluster string       `json:"source_cluster"`
	TargetCluster string       `json:"target_cluster"`
	SourcePassword string      `json:"source_password,omitempty"`
	TargetPassword string      `json:"target_password,omitempty"`
	MigrationMode string       `json:"migration_mode"`
	Options       *TaskOptions `json:"options,omitempty"`
	CreatedAt     string       `json:"created_at"`
	UpdatedAt     string       `json:"updated_at"`
}

// TaskOptions ä»»åŠ¡é…ç½®é€‰é¡¹
type TaskOptions struct {
	WorkerCount          int          `json:"worker_count"`
	ScanBatchSize        int          `json:"scan_batch_size"`
	ConflictPolicy       string       `json:"conflict_policy"`     // skip, replace, error, skip_full_only
	LargeKeyThreshold    int64        `json:"large_key_threshold"`
	EnableCompression    bool         `json:"enable_compression"`
	SkipFullSync         bool         `json:"skip_full_sync"`
	SkipIncremental      bool         `json:"skip_incremental"`
	KeyFilter            *KeyFilter   `json:"key_filter,omitempty"`
	RateLimit            *RateLimit   `json:"rate_limit,omitempty"`
	RetryConfig          *RetryConfig `json:"retry_config,omitempty"`
}

// RetryConfig é‡è¯•é…ç½®
type RetryConfig struct {
	MaxRetries         int `json:"max_retries"`          // æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤3
	FullRetryIntervalMs  int `json:"full_retry_interval_ms"`   // å…¨é‡è¿ç§»é‡è¯•é—´éš”åŸºæ•°(æ¯«ç§’)ï¼Œé»˜è®¤100
	IncrRetryIntervalMs  int `json:"incr_retry_interval_ms"`   // å¢é‡åŒæ­¥é‡è¯•é—´éš”åŸºæ•°(æ¯«ç§’)ï¼Œé»˜è®¤1000
}

// RateLimit é™é€Ÿé…ç½®
type RateLimit struct {
	SourceQPS         int `json:"source_qps"`          // æºç«¯QPSé™åˆ¶ï¼Œ0è¡¨ç¤ºä¸é™åˆ¶
	TargetQPS         int `json:"target_qps"`          // ç›®æ ‡ç«¯QPSé™åˆ¶ï¼Œ0è¡¨ç¤ºä¸é™åˆ¶
	SourceConnections int `json:"source_connections"`  // æºç«¯è¿æ¥æ•°
	TargetConnections int `json:"target_connections"`  // ç›®æ ‡ç«¯è¿æ¥æ•°
}

// ClusterInfo é›†ç¾¤ä¿¡æ¯ï¼ˆç”¨äºæ¨èé…ç½®ï¼‰
type ClusterInfo struct {
	Addrs            []string `json:"addrs"`
	IsCluster        bool     `json:"is_cluster"`
	MasterCount      int      `json:"master_count"`
	TotalKeys        int64    `json:"total_keys"`
	UsedMemory       int64    `json:"used_memory"`
	UsedMemoryHuman  string   `json:"used_memory_human"`
	MaxMemory        int64    `json:"max_memory"`
	MaxClients       int      `json:"max_clients"`
	ConnectedClients int      `json:"connected_clients"`
	InstantaneousOPS int64    `json:"instantaneous_ops"`
	Version          string   `json:"version"`
	AvgKeySize       int64    `json:"avg_key_size"`       // ä¼°ç®—çš„å¹³å‡keyå¤§å°
	LargeKeyCount    int64    `json:"large_key_count"`    // å¤§keyæ•°é‡ä¼°ç®—
}

// RecommendedConfig æ¨èé…ç½®
type RecommendedConfig struct {
	WorkerCount       int    `json:"worker_count"`
	ScanBatchSize     int    `json:"scan_batch_size"`
	SourceQPS         int    `json:"source_qps"`
	TargetQPS         int    `json:"target_qps"`
	SourceConnections int    `json:"source_connections"`
	TargetConnections int    `json:"target_connections"`
	LargeKeyThreshold int64  `json:"large_key_threshold"`
	EstimatedTime     string `json:"estimated_time"`      // é¢„è®¡è€—æ—¶
	EstimatedSpeed    int64  `json:"estimated_speed"`     // é¢„è®¡é€Ÿåº¦ keys/s
	Reason            string `json:"reason"`              // æ¨èç†ç”±
}

// KeyFilter Keyè¿‡æ»¤é…ç½®
type KeyFilter struct {
	Mode            string   `json:"mode"` // all, prefix, pattern
	Prefixes        []string `json:"prefixes"`
	ExcludePrefixes []string `json:"exclude_prefixes"`
	Patterns        []string `json:"patterns"`
}

// ErrorKey è®°å½•è¿ç§»å¤±è´¥æˆ–è·³è¿‡çš„Key
type ErrorKey struct {
	Key       string `json:"key"`
	Type      string `json:"type"`
	Reason    string `json:"reason"`
	Detail    string `json:"detail"`
	Timestamp string `json:"timestamp"`
}

// getRetryConfig è·å–é‡è¯•é…ç½®ï¼Œè¿”å›é»˜è®¤å€¼å¦‚æœæœªé…ç½®
func getRetryConfig(opts *TaskOptions) (maxRetries int, fullIntervalMs int, incrIntervalMs int) {
	// é»˜è®¤å€¼
	maxRetries = 3
	fullIntervalMs = 100
	incrIntervalMs = 1000

	if opts != nil && opts.RetryConfig != nil {
		if opts.RetryConfig.MaxRetries > 0 {
			maxRetries = opts.RetryConfig.MaxRetries
		}
		if opts.RetryConfig.FullRetryIntervalMs > 0 {
			fullIntervalMs = opts.RetryConfig.FullRetryIntervalMs
		}
		if opts.RetryConfig.IncrRetryIntervalMs > 0 {
			incrIntervalMs = opts.RetryConfig.IncrRetryIntervalMs
		}
	}
	return
}

var (
	tasks      = make(map[string]*Task)
	tasksMu    sync.RWMutex
	templates  = make(map[string]*TaskTemplate)
	templateMu sync.RWMutex
	errorKeys  = make(map[string][]ErrorKey) // taskID -> error keys
	errorKeyMu sync.RWMutex
	startTime  time.Time
)

func main() {
	startTime = time.Now()
	
	// åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
	if err := logger.Init("./logs", logger.DEBUG); err != nil {
		fmt.Printf("Failed to init logger: %v\n", err)
	}
	
	logger.Info("ğŸš€ Tendis Migration Tool starting", map[string]interface{}{
		"port":    8088,
		"version": "1.0.0",
		"pid":     fmt.Sprintf("%d", getPID()),
	})

	initDemoData()

	// ä½¿ç”¨è‡ªå®šä¹‰ handler ç»Ÿä¸€å¤„ç†
	server := &http.Server{
		Addr:         ":8088",
		Handler:      http.HandlerFunc(mainHandler),
		ReadTimeout:  30 * time.Second,
		WriteTimeout: 30 * time.Second,
	}

	logger.Info("Server listening on http://localhost:8088")
	if err := server.ListenAndServe(); err != nil {
		logger.Fatal("Server failed to start", map[string]interface{}{"error": err.Error()})
	}
}

func mainHandler(w http.ResponseWriter, r *http.Request) {
	// ç”Ÿæˆè¯·æ±‚ID
	requestID := uuid.New().String()
	startTime := time.Now()
	
	// CORS
	w.Header().Set("Access-Control-Allow-Origin", "*")
	w.Header().Set("Access-Control-Allow-Methods", "GET, POST, PUT, DELETE, OPTIONS")
	w.Header().Set("Access-Control-Allow-Headers", "*")
	w.Header().Set("X-Request-ID", requestID)
	
	if r.Method == "OPTIONS" {
		w.WriteHeader(http.StatusOK)
		return
	}

	path := r.URL.Path
	log := logger.WithRequest(requestID)
	
	// è®°å½•è¯·æ±‚æ—¥å¿—
	log.Info("Request started", map[string]interface{}{
		"method": r.Method,
		"path":   path,
		"query":  r.URL.RawQuery,
		"remote": r.RemoteAddr,
		"ua":     r.UserAgent(),
	})

	// åŒ…è£… ResponseWriter ä»¥æ•è·çŠ¶æ€ç 
	rw := &responseWriter{ResponseWriter: w, statusCode: 200}

	// è·¯ç”±å¤„ç†
	switch {
	// æ—¥å¿—ç›¸å…³ API
	case path == "/api/v1/logs":
		logsHandler(rw, r, log)
	case path == "/api/v1/logs/export":
		logsExportHandler(rw, r, log)
	case path == "/api/v1/logs/clear":
		logsClearHandler(rw, r, log)
	case path == "/api/v1/logs/stats":
		logsStatsHandler(rw, r, log)
		
	// ä¸šåŠ¡ API
	case path == "/api/v1/health":
		healthHandler(rw, r, log)
	case path == "/api/v1/tasks":
		tasksHandler(rw, r, log)
	case strings.HasPrefix(path, "/api/v1/tasks/"):
		taskHandler(rw, r, log)
	case path == "/api/v1/system/status":
		systemHandler(rw, r, log)
	case path == "/api/v1/test-connection":
		testConnectionHandler(rw, r, log)
	case path == "/api/v1/analyze-cluster":
		analyzeClusterHandler(rw, r, log)
	case path == "/api/v1/recommend-config":
		recommendConfigHandler(rw, r, log)
	case path == "/api/v1/templates":
		templatesHandler(rw, r, log)
	case strings.HasPrefix(path, "/api/v1/templates/"):
		templateHandler(rw, r, log)
		
	// é™æ€èµ„æº
	case strings.HasPrefix(path, "/assets/"):
		http.FileServer(http.Dir("./web/dist")).ServeHTTP(rw, r)
		
	// SPA å…¥å£
	default:
		http.ServeFile(rw, r, "./web/dist/index.html")
	}

	// è®°å½•å“åº”æ—¥å¿—
	duration := time.Since(startTime)
	log.Info("Request completed", map[string]interface{}{
		"status":      rw.statusCode,
		"duration_ms": duration.Milliseconds(),
	})
}

// responseWriter åŒ…è£…å™¨
type responseWriter struct {
	http.ResponseWriter
	statusCode int
}

func (rw *responseWriter) WriteHeader(code int) {
	rw.statusCode = code
	rw.ResponseWriter.WriteHeader(code)
}

func getPID() int {
	return 1 // ç®€åŒ–å¤„ç†
}

func jsonResponse(w http.ResponseWriter, data interface{}) {
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(data)
}

func initDemoData() {
	// é¢„ç½®æ¨¡æ¿ä»»åŠ¡
	now := time.Now().Format("2006-01-02 15:04:05")
	templates["template-default"] = &TaskTemplate{
		ID:            "template-default",
		Name:          "Template",
		Description:   "é¢„ç½®è¿ç§»æ¨¡æ¿ï¼šæºç«¯åˆ°ç›®æ ‡ç«¯çš„å…¨é‡+å¢é‡è¿ç§»",
		SourceCluster: "10.248.37.11:8901,10.248.37.11:8902,10.248.37.11:8903",
		TargetCluster: "10.31.165.39:8901,10.31.165.39:8902,10.31.165.39:8903",
		MigrationMode: "full_and_incremental",
		Options: &TaskOptions{
			WorkerCount:       8,
			ScanBatchSize:     1000,
			ConflictPolicy:    "skip",
			LargeKeyThreshold: 10485760, // 10MB
			KeyFilter: &KeyFilter{
				Mode:     "prefix",
				Prefixes: []string{"testkey"},
			},
			RateLimit: &RateLimit{
				SourceQPS:         0,
				TargetQPS:         0,
				SourceConnections: 50,
				TargetConnections: 50,
			},
			RetryConfig: &RetryConfig{
				MaxRetries:          3,
				FullRetryIntervalMs: 100,
				IncrRetryIntervalMs: 1000,
			},
		},
		CreatedAt: now,
		UpdatedAt: now,
	}
	logger.Info("System initialized", map[string]interface{}{
		"mode":      "production",
		"templates": len(templates),
	})
}

// ==================== æ—¥å¿— API ====================

func logsHandler(w http.ResponseWriter, r *http.Request, log *logger.RequestLogger) {
	if r.Method != "GET" {
		http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
		return
	}

	q := r.URL.Query()
	limit, _ := strconv.Atoi(q.Get("limit"))
	offset, _ := strconv.Atoi(q.Get("offset"))
	if limit == 0 {
		limit = 100
	}

	filter := logger.LogFilter{
		Level:     q.Get("level"),
		RequestID: q.Get("request_id"),
		TaskID:    q.Get("task_id"),
		Keyword:   q.Get("keyword"),
		StartTime: q.Get("start_time"),
		EndTime:   q.Get("end_time"),
		Offset:    offset,
		Limit:     limit,
	}

	entries := logger.Default().GetEntries(filter)
	total := logger.Default().GetTotalCount(filter)

	log.Debug("Logs queried", map[string]interface{}{
		"filter": filter,
		"count":  len(entries),
		"total":  total,
	})

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data": map[string]interface{}{
			"items":  entries,
			"total":  total,
			"offset": offset,
			"limit":  limit,
		},
	})
}

func logsExportHandler(w http.ResponseWriter, r *http.Request, log *logger.RequestLogger) {
	if r.Method != "GET" {
		http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
		return
	}

	q := r.URL.Query()
	format := q.Get("format")
	if format == "" {
		format = "text"
	}

	taskID := q.Get("task_id")
	filter := logger.LogFilter{
		Level:     q.Get("level"),
		RequestID: q.Get("request_id"),
		TaskID:    taskID,
		Keyword:   q.Get("keyword"),
		StartTime: q.Get("start_time"),
		EndTime:   q.Get("end_time"),
	}

	// å¦‚æœæŒ‡å®šäº†ä»»åŠ¡IDï¼Œè·å–ä»»åŠ¡åç§°ç”¨äºæ–‡ä»¶å
	taskName := ""
	if taskID != "" {
		tasksMu.RLock()
		for _, t := range tasks {
			if t.ID == taskID || strings.HasPrefix(t.ID, taskID) {
				taskName = t.Name
				filter.TaskID = t.ID // ä½¿ç”¨å®Œæ•´ID
				break
			}
		}
		tasksMu.RUnlock()
	}

	data, err := logger.Default().Export(filter, format)
	if err != nil {
		log.Error("Failed to export logs", map[string]interface{}{"error": err.Error()})
		jsonResponse(w, map[string]interface{}{"code": 500, "message": err.Error()})
		return
	}

	// ç”Ÿæˆæ–‡ä»¶å
	var filename string
	timestamp := time.Now().Format("20060102-150405")
	ext := "txt"
	if format == "json" {
		ext = "json"
	}
	if taskID != "" {
		shortID := taskID
		if len(shortID) > 8 {
			shortID = shortID[:8]
		}
		if taskName != "" {
			// æ¸…ç†ä»»åŠ¡åä¸­çš„ç‰¹æ®Šå­—ç¬¦
			safeName := strings.Map(func(r rune) rune {
				if r == '/' || r == '\\' || r == ':' || r == '*' || r == '?' || r == '"' || r == '<' || r == '>' || r == '|' {
					return '-'
				}
				return r
			}, taskName)
			filename = fmt.Sprintf("task-%s-%s-%s.%s", shortID, safeName, timestamp, ext)
		} else {
			filename = fmt.Sprintf("task-%s-logs-%s.%s", shortID, timestamp, ext)
		}
	} else {
		filename = fmt.Sprintf("tendis-migrate-logs-%s.%s", timestamp, ext)
	}
	
	if format == "json" {
		w.Header().Set("Content-Type", "application/json")
	} else {
		w.Header().Set("Content-Type", "text/plain; charset=utf-8")
	}
	w.Header().Set("Content-Disposition", fmt.Sprintf("attachment; filename=\"%s\"", filename))
	w.Header().Set("Content-Length", strconv.Itoa(len(data)))
	w.Write(data)

	log.Info("Logs exported", map[string]interface{}{
		"format":   format,
		"size":     len(data),
		"filename": filename,
		"task_id":  taskID,
	})
}

func logsClearHandler(w http.ResponseWriter, r *http.Request, log *logger.RequestLogger) {
	if r.Method != "POST" {
		http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
		return
	}

	logger.Default().Clear()
	log.Info("Logs cleared")

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
	})
}

func logsStatsHandler(w http.ResponseWriter, r *http.Request, log *logger.RequestLogger) {
	if r.Method != "GET" {
		http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
		return
	}

	l := logger.Default()
	
	// ç»Ÿè®¡å„çº§åˆ«æ—¥å¿—æ•°é‡
	stats := map[string]int{
		"DEBUG": 0,
		"INFO":  0,
		"WARN":  0,
		"ERROR": 0,
		"FATAL": 0,
	}
	
	allEntries := l.GetEntries(logger.LogFilter{Limit: 0})
	for _, e := range allEntries {
		stats[e.Level]++
	}

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data": map[string]interface{}{
			"total":     len(allEntries),
			"by_level":  stats,
			"uptime":    time.Since(startTime).String(),
			"memory_mb": getMemoryUsage(),
		},
	})
}

func getMemoryUsage() float64 {
	var m runtime.MemStats
	runtime.ReadMemStats(&m)
	return float64(m.Alloc) / 1024 / 1024
}

// ==================== ä¸šåŠ¡ API ====================

func healthHandler(w http.ResponseWriter, r *http.Request, log *logger.RequestLogger) {
	log.Debug("Health check")
	jsonResponse(w, map[string]interface{}{
		"status": "healthy",
		"time":   time.Now().Format(time.RFC3339),
	})
}

func tasksHandler(w http.ResponseWriter, r *http.Request, log *logger.RequestLogger) {
	switch r.Method {
	case "GET":
		listTasksHandler(w, r, log)
	case "POST":
		createTaskHandler(w, r, log)
	default:
		http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
	}
}

func listTasksHandler(w http.ResponseWriter, r *http.Request, log *logger.RequestLogger) {
	tasksMu.RLock()
	defer tasksMu.RUnlock()

	var items []map[string]interface{}
	for _, t := range tasks {
		items = append(items, map[string]interface{}{
			"id":         t.ID,
			"name":       t.Name,
			"status":     t.Status,
			"created_at": t.CreatedAt,
			"updated_at": t.UpdatedAt,
			"progress": map[string]interface{}{
				"percentage":    t.Progress,
				"keys_total":    t.KeysTotal,
				"keys_migrated": t.KeysMigrated,
				"speed":         t.Speed,
			},
		})
	}

	log.Debug("Tasks listed", map[string]interface{}{"count": len(items)})

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data": map[string]interface{}{
			"items": items,
			"total": len(items),
		},
	})
}

func createTaskHandler(w http.ResponseWriter, r *http.Request, log *logger.RequestLogger) {
	var req struct {
		Name          string `json:"name"`
		MigrationMode string `json:"migration_mode"`
		SourceCluster struct {
			Addrs    []string `json:"addrs"`
			Password string   `json:"password"`
		} `json:"source_cluster"`
		TargetCluster struct {
			Addrs    []string `json:"addrs"`
			Password string   `json:"password"`
		} `json:"target_cluster"`
		Options *TaskOptions `json:"options"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		log.Error("Failed to decode request", map[string]interface{}{"error": err.Error()})
		jsonResponse(w, map[string]interface{}{"code": 400, "message": err.Error()})
		return
	}

	mode := req.MigrationMode
	if mode == "" {
		mode = "full_and_incremental"
	}

	// è®¾ç½®é»˜è®¤é€‰é¡¹
	options := req.Options
	if options == nil {
		options = &TaskOptions{
			WorkerCount:       4,
			ScanBatchSize:     1000,
			ConflictPolicy:    "skip_full_only",
			LargeKeyThreshold: 10485760,
			EnableCompression: true,
			KeyFilter: &KeyFilter{
				Mode: "all",
			},
		}
	} else {
		// ç¡®ä¿ KeyFilter æœ‰é»˜è®¤å€¼
		if options.KeyFilter == nil {
			options.KeyFilter = &KeyFilter{Mode: "all"}
		} else if options.KeyFilter.Mode == "" {
			options.KeyFilter.Mode = "all"
		}
		// ç¡®ä¿å…¶ä»–é€‰é¡¹æœ‰é»˜è®¤å€¼
		if options.WorkerCount == 0 {
			options.WorkerCount = 4
		}
		if options.ScanBatchSize == 0 {
			options.ScanBatchSize = 1000
		}
		if options.ConflictPolicy == "" {
			options.ConflictPolicy = "skip_full_only"
		}
		if options.LargeKeyThreshold == 0 {
			options.LargeKeyThreshold = 10485760
		}
	}

	task := &Task{
		ID:             uuid.New().String(),
		Name:           req.Name,
		Status:         "pending",
		Progress:       0,
		SourceCluster:  strings.Join(req.SourceCluster.Addrs, ","),
		TargetCluster:  strings.Join(req.TargetCluster.Addrs, ","),
		SourcePassword: req.SourceCluster.Password,
		TargetPassword: req.TargetCluster.Password,
		MigrationMode:  mode,
		CreatedAt:      time.Now().Format(time.RFC3339),
		UpdatedAt:      time.Now().Format(time.RFC3339),
		Phase:          "full",
		Options:        options,
	}

	tasksMu.Lock()
	tasks[task.ID] = task
	
	// ä¿ç•™æœ€è¿‘3ä¸ªä»»åŠ¡ï¼Œæ¸…ç†æ—§ä»»åŠ¡
	cleanupOldTasks()
	tasksMu.Unlock()

	log.Info("Task created", map[string]interface{}{
		"task_id":        task.ID,
		"task_name":      task.Name,
		"migration_mode": mode,
	})

	// åŒæ—¶è®°å½•ä»»åŠ¡æ—¥å¿—
	logger.WithTask(task.ID).Info("Task created", map[string]interface{}{
		"name":           task.Name,
		"source":         task.SourceCluster,
		"target":         task.TargetCluster,
		"migration_mode": mode,
	})

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data":    map[string]string{"task_id": task.ID},
	})
}

// cleanupOldTasks æ¸…ç†æ—§ä»»åŠ¡ï¼Œä¿ç•™æœ€è¿‘3ä¸ªï¼ˆå¿…é¡»åœ¨tasksMué”å†…è°ƒç”¨ï¼‰
func cleanupOldTasks() {
	const maxTasks = 3
	if len(tasks) <= maxTasks {
		return
	}
	
	// è·å–æ‰€æœ‰ä»»åŠ¡å¹¶æŒ‰åˆ›å»ºæ—¶é—´æ’åº
	taskList := make([]*Task, 0, len(tasks))
	for _, t := range tasks {
		taskList = append(taskList, t)
	}
	sort.Slice(taskList, func(i, j int) bool {
		return taskList[i].CreatedAt > taskList[j].CreatedAt // é™åºï¼Œæœ€æ–°åœ¨å‰
	})
	
	// åˆ é™¤è¶…å‡ºé™åˆ¶çš„æ—§ä»»åŠ¡
	for i := maxTasks; i < len(taskList); i++ {
		oldTask := taskList[i]
		// å¦‚æœä»»åŠ¡æ­£åœ¨è¿è¡Œï¼Œè·³è¿‡
		if oldTask.Status == "running" {
			continue
		}
		// æ¸…ç†è¯¥ä»»åŠ¡çš„æ—¥å¿—
		logger.Default().ClearTaskLogs(oldTask.ID)
		// åˆ é™¤ä»»åŠ¡
		delete(tasks, oldTask.ID)
		logger.Default().Info("Old task cleaned up", map[string]interface{}{
			"task_id":   oldTask.ID,
			"task_name": oldTask.Name,
		})
	}
}

func taskHandler(w http.ResponseWriter, r *http.Request, log *logger.RequestLogger) {
	path := strings.TrimPrefix(r.URL.Path, "/api/v1/tasks/")
	parts := strings.Split(path, "/")

	if len(parts) == 0 || parts[0] == "" {
		http.NotFound(w, r)
		return
	}

	id := parts[0]
	action := ""
	if len(parts) > 1 {
		action = parts[1]
	}

	taskLog := logger.WithTask(id)

	switch {
	case action == "" && r.Method == "GET":
		getTaskHandler(w, r, id, log)
	case action == "" && r.Method == "DELETE":
		deleteTaskHandler(w, r, id, log, taskLog)
	case action == "config" && r.Method == "PUT":
		updateTaskConfigHandler(w, r, id, log, taskLog)
	case action == "start" && r.Method == "POST":
		startTaskHandler(w, r, id, log, taskLog)
	case action == "pause" && r.Method == "POST":
		pauseTaskHandler(w, r, id, log, taskLog)
	case action == "resume" && r.Method == "POST":
		resumeTaskHandler(w, r, id, log, taskLog)
	case action == "progress" && r.Method == "GET":
		progressHandler(w, r, id, log)
	case action == "logs" && r.Method == "GET":
		taskLogsHandler(w, r, id, log)
	case action == "verify" && r.Method == "POST":
		triggerVerifyHandler(w, r, id, log)
	case strings.HasPrefix(action, "verify") && r.Method == "GET":
		verifyResultsHandler(w, r, id, log)
	case action == "error-keys" && r.Method == "GET":
		errorKeysHandler(w, r, id, log)
	case strings.HasPrefix(action, "error-keys/download") && r.Method == "GET":
		downloadErrorKeysHandler(w, r, id, log)
	default:
		http.NotFound(w, r)
	}
}

func getTaskHandler(w http.ResponseWriter, r *http.Request, id string, log *logger.RequestLogger) {
	tasksMu.RLock()
	task, ok := tasks[id]
	tasksMu.RUnlock()

	if !ok {
		log.Warn("Task not found", map[string]interface{}{"task_id": id})
		jsonResponse(w, map[string]interface{}{"code": 404, "message": "Task not found"})
		return
	}

	log.Debug("Task retrieved", map[string]interface{}{"task_id": id})

	phase := task.Phase
	if phase == "" {
		phase = "full"
	}

	// æ„å»ºé…ç½®ä¿¡æ¯
	configData := map[string]interface{}{
		"worker_count":       8,
		"scan_batch_size":    1000,
		"conflict_policy":    "skip",
		"large_key_threshold": 10485760,
		"rate_limit": map[string]interface{}{
			"source_qps":         0,
			"target_qps":         0,
			"source_connections": 50,
			"target_connections": 50,
		},
	}
	if task.Options != nil {
		configData["worker_count"] = task.Options.WorkerCount
		configData["scan_batch_size"] = task.Options.ScanBatchSize
		configData["conflict_policy"] = task.Options.ConflictPolicy
		configData["large_key_threshold"] = task.Options.LargeKeyThreshold
		if task.Options.RateLimit != nil {
			configData["rate_limit"] = map[string]interface{}{
				"source_qps":         task.Options.RateLimit.SourceQPS,
				"target_qps":         task.Options.RateLimit.TargetQPS,
				"source_connections": task.Options.RateLimit.SourceConnections,
				"target_connections": task.Options.RateLimit.TargetConnections,
			}
		}
		// æ·»åŠ  key_filter ä¿¡æ¯
		if task.Options.KeyFilter != nil {
			configData["key_filter"] = map[string]interface{}{
				"mode":             task.Options.KeyFilter.Mode,
				"prefixes":         task.Options.KeyFilter.Prefixes,
				"exclude_prefixes": task.Options.KeyFilter.ExcludePrefixes,
				"patterns":         task.Options.KeyFilter.Patterns,
			}
		}
	}

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data": map[string]interface{}{
			"id":             task.ID,
			"name":           task.Name,
			"status":         task.Status,
			"migration_mode": task.MigrationMode,
			"source_cluster": map[string]interface{}{"addrs": strings.Split(task.SourceCluster, ",")},
			"target_cluster": map[string]interface{}{"addrs": strings.Split(task.TargetCluster, ",")},
			"created_at":     task.CreatedAt,
			"updated_at":     task.UpdatedAt,
			"started_at":     task.StartedAt,
			"full_start_at":  task.FullStartAt,
			"incr_start_at":  task.IncrStartAt,
			"keys_filtered":  task.KeysFiltered,
			"config":         configData,
			"progress": map[string]interface{}{
				"percentage":     task.Progress,
				"total_keys":     task.KeysTotal,
				"migrated_keys":  task.KeysMigrated,
				"total_bytes":    task.BytesTotal,
				"migrated_bytes": task.BytesMigrated,
				"current_speed":  task.Speed,
				"phase":          phase,
				"estimated_eta":  calculateETA(task),
				"elapsed_time":   calculateElapsedTime(task),
			},
			"stats": map[string]interface{}{
				"total_keys":     task.KeysTotal,
				"migrated_keys":  task.KeysMigrated,
				"failed_keys":    task.KeysFailed,
				"skipped_keys":   task.KeysSkipped,
				"filtered_keys":  task.KeysFiltered,
				"bytes_sent":     task.BytesMigrated,
			},
		},
	})
}

func calculateETA(task *Task) string {
	if task.Speed <= 0 || task.KeysTotal <= task.KeysMigrated {
		return "-"
	}
	remaining := task.KeysTotal - task.KeysMigrated
	seconds := remaining / task.Speed
	if seconds < 60 {
		return fmt.Sprintf("%ds", seconds)
	} else if seconds < 3600 {
		return fmt.Sprintf("%dm %ds", seconds/60, seconds%60)
	}
	return fmt.Sprintf("%dh %dm", seconds/3600, (seconds%3600)/60)
}

// calculateElapsedTime è®¡ç®—å·²è€—æ—¶é—´
func calculateElapsedTime(task *Task) string {
	if task.StartedAt == "" {
		return "-"
	}
	// ä½¿ç”¨æœ¬åœ°æ—¶åŒºè§£ææ—¶é—´
	loc := time.Local
	startTime, err := time.ParseInLocation("2006-01-02 15:04:05", task.StartedAt, loc)
	if err != nil {
		return "-"
	}
	elapsed := time.Since(startTime)
	seconds := int64(elapsed.Seconds())
	if seconds < 0 {
		return "-"
	}
	if seconds < 60 {
		return fmt.Sprintf("%ds", seconds)
	} else if seconds < 3600 {
		return fmt.Sprintf("%dm %ds", seconds/60, seconds%60)
	} else if seconds < 86400 {
		return fmt.Sprintf("%dh %dm", seconds/3600, (seconds%3600)/60)
	}
	return fmt.Sprintf("%dd %dh", seconds/86400, (seconds%86400)/3600)
}

func deleteTaskHandler(w http.ResponseWriter, r *http.Request, id string, log *logger.RequestLogger, taskLog *logger.TaskLogger) {
	tasksMu.Lock()
	delete(tasks, id)
	tasksMu.Unlock()
	
	log.Info("Task deleted", map[string]interface{}{"task_id": id})
	taskLog.Info("Task deleted")
	
	jsonResponse(w, map[string]interface{}{"code": 0, "message": "success"})
}

// updateTaskConfigHandler åŠ¨æ€è°ƒæ•´ä»»åŠ¡é…ç½®ï¼ˆä¼˜é›…è°ƒæ•´ï¼‰
func updateTaskConfigHandler(w http.ResponseWriter, r *http.Request, id string, log *logger.RequestLogger, taskLog *logger.TaskLogger) {
	tasksMu.Lock()
	task, ok := tasks[id]
	tasksMu.Unlock()

	if !ok {
		jsonResponse(w, map[string]interface{}{"code": 404, "message": "Task not found"})
		return
	}

	// è§£æè¯·æ±‚ä½“
	var req struct {
		WorkerCount    int `json:"worker_count"`
		ScanBatchSize  int `json:"scan_batch_size"`
		RateLimit      *RateLimit `json:"rate_limit"`
	}
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		jsonResponse(w, map[string]interface{}{"code": 400, "message": "Invalid request body"})
		return
	}

	// æ›´æ–°é…ç½®
	tasksMu.Lock()
	if task.Options == nil {
		task.Options = &TaskOptions{}
	}
	
	oldWorkerCount := task.Options.WorkerCount
	oldScanBatchSize := task.Options.ScanBatchSize
	
	if req.WorkerCount > 0 {
		task.Options.WorkerCount = req.WorkerCount
	}
	if req.ScanBatchSize > 0 {
		task.Options.ScanBatchSize = req.ScanBatchSize
	}
	if req.RateLimit != nil {
		if task.Options.RateLimit == nil {
			task.Options.RateLimit = &RateLimit{}
		}
		task.Options.RateLimit.SourceQPS = req.RateLimit.SourceQPS
		task.Options.RateLimit.TargetQPS = req.RateLimit.TargetQPS
	}
	task.UpdatedAt = time.Now().Format(time.RFC3339)
	tasksMu.Unlock()

	log.Info("Task config updated", map[string]interface{}{
		"task_id":            id,
		"old_worker_count":   oldWorkerCount,
		"new_worker_count":   task.Options.WorkerCount,
		"old_scan_batch":     oldScanBatchSize,
		"new_scan_batch":     task.Options.ScanBatchSize,
	})
	
	// è®°å½•WorkeråŠ¨æ€è°ƒæ•´ä¿¡æ¯
	adjustMsg := "will take effect dynamically"
	if task.workerPool != nil {
		currentActive := task.workerPool.GetActiveWorkerCount()
		if oldWorkerCount != task.Options.WorkerCount {
			if task.Options.WorkerCount > oldWorkerCount {
				adjustMsg = fmt.Sprintf("increasing workers from %d to %d (current active: %d)", oldWorkerCount, task.Options.WorkerCount, currentActive)
			} else {
				adjustMsg = fmt.Sprintf("decreasing workers from %d to %d gracefully (current active: %d)", oldWorkerCount, task.Options.WorkerCount, currentActive)
			}
		}
	}
	
	// è·å–QPSå€¼ç”¨äºæ—¥å¿—ï¼ˆå¤„ç†ç©ºæŒ‡é’ˆï¼‰
	sourceQPS := 0
	targetQPS := 0
	if task.Options.RateLimit != nil {
		sourceQPS = task.Options.RateLimit.SourceQPS
		targetQPS = task.Options.RateLimit.TargetQPS
	}
	
	taskLog.Info("Config updated (dynamic adjustment)", map[string]interface{}{
		"worker_count":    task.Options.WorkerCount,
		"scan_batch_size": task.Options.ScanBatchSize,
		"source_qps":      sourceQPS,
		"target_qps":      targetQPS,
		"adjustment":      adjustMsg,
	})

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "Config updated dynamically, worker adjustment in progress",
		"data": map[string]interface{}{
			"worker_count":    task.Options.WorkerCount,
			"scan_batch_size": task.Options.ScanBatchSize,
			"rate_limit":      task.Options.RateLimit,
			"adjustment":      adjustMsg,
		},
	})
}

func startTaskHandler(w http.ResponseWriter, r *http.Request, id string, log *logger.RequestLogger, taskLog *logger.TaskLogger) {
	tasksMu.Lock()
	task, ok := tasks[id]
	if ok {
		task.Status = "running"
		task.UpdatedAt = time.Now().Format(time.RFC3339)
		task.StartedAt = time.Now().Format("2006-01-02 15:04:05")
		go simulateProgress(task)
	}
	tasksMu.Unlock()
	
	if ok {
		log.Info("Task started", map[string]interface{}{"task_id": id})
		taskLog.Info("Task started", map[string]interface{}{
			"keys_total": task.KeysTotal,
		})
	} else {
		log.Warn("Task not found for start", map[string]interface{}{"task_id": id})
	}
	
	jsonResponse(w, map[string]interface{}{"code": 0, "message": "success"})
}

func pauseTaskHandler(w http.ResponseWriter, r *http.Request, id string, log *logger.RequestLogger, taskLog *logger.TaskLogger) {
	tasksMu.Lock()
	task, ok := tasks[id]
	if ok {
		task.Status = "paused"
		task.Speed = 0
		task.UpdatedAt = time.Now().Format(time.RFC3339)
	}
	tasksMu.Unlock()
	
	if ok {
		log.Info("Task paused", map[string]interface{}{"task_id": id})
		taskLog.Info("Task paused", map[string]interface{}{
			"progress": task.Progress,
		})
	}
	
	jsonResponse(w, map[string]interface{}{"code": 0, "message": "success"})
}

func resumeTaskHandler(w http.ResponseWriter, r *http.Request, id string, log *logger.RequestLogger, taskLog *logger.TaskLogger) {
	tasksMu.Lock()
	task, ok := tasks[id]
	if ok {
		task.Status = "running"
		task.UpdatedAt = time.Now().Format(time.RFC3339)
		go simulateProgress(task)
	}
	tasksMu.Unlock()
	
	if ok {
		log.Info("Task resumed", map[string]interface{}{"task_id": id})
		taskLog.Info("Task resumed")
	}
	
	jsonResponse(w, map[string]interface{}{"code": 0, "message": "success"})
}

func progressHandler(w http.ResponseWriter, r *http.Request, id string, log *logger.RequestLogger) {
	tasksMu.RLock()
	task, ok := tasks[id]
	tasksMu.RUnlock()

	if !ok {
		log.Warn("Task not found for progress", map[string]interface{}{"task_id": id})
		jsonResponse(w, map[string]interface{}{"code": 404, "message": "Task not found"})
		return
	}

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data": map[string]interface{}{
			"percentage":    task.Progress,
			"keys_total":    task.KeysTotal,
			"keys_migrated": task.KeysMigrated,
			"speed":         task.Speed,
			"phase":         "full",
			"eta":           int64((100 - task.Progress) / 0.5 * 2),
		},
	})
}

func taskLogsHandler(w http.ResponseWriter, r *http.Request, id string, log *logger.RequestLogger) {
	q := r.URL.Query()
	limit, _ := strconv.Atoi(q.Get("limit"))
	offset, _ := strconv.Atoi(q.Get("offset"))
	if limit == 0 {
		limit = 100
	}

	filter := logger.LogFilter{
		TaskID:  id,
		Level:   q.Get("level"),
		Keyword: q.Get("keyword"),
		Offset:  offset,
		Limit:   limit,
	}

	entries := logger.Default().GetEntries(filter)
	total := logger.Default().GetTotalCount(filter)

	log.Debug("Task logs queried", map[string]interface{}{
		"task_id": id,
		"count":   len(entries),
	})

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data": map[string]interface{}{
			"items":  entries,
			"total":  total,
			"offset": offset,
			"limit":  limit,
		},
	})
}

func triggerVerifyHandler(w http.ResponseWriter, r *http.Request, id string, log *logger.RequestLogger) {
	log.Info("Verify triggered", map[string]interface{}{"task_id": id})
	
	// æ¨¡æ‹Ÿè§¦å‘æ ¡éªŒ
	batchID := uuid.New().String()
	
	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data":    map[string]string{"batch_id": batchID},
	})
}

func verifyResultsHandler(w http.ResponseWriter, r *http.Request, id string, log *logger.RequestLogger) {
	log.Debug("Verify results queried", map[string]interface{}{"task_id": id})
	
	// è¿”å›ç©ºæ•°ç»„ï¼Œæš‚æ— æ ¡éªŒç»“æœ
	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data":    []interface{}{},
	})
}

func simulateProgress(task *Task) {
	taskLog := logger.WithTask(task.ID)
	taskLog.Info("Migration started - connecting to clusters")

	ctx := context.Background()

	// è§£ææºç«¯å’Œç›®æ ‡ç«¯åœ°å€
	sourceAddrs := strings.Split(task.SourceCluster, ",")
	targetAddrs := strings.Split(task.TargetCluster, ",")

	for i := range sourceAddrs {
		sourceAddrs[i] = strings.TrimSpace(sourceAddrs[i])
	}
	for i := range targetAddrs {
		targetAddrs[i] = strings.TrimSpace(targetAddrs[i])
	}

	// è·å–è¿æ¥æ•°é…ç½®
	sourcePoolSize := 50 // é»˜è®¤50è¿æ¥
	targetPoolSize := 50 // é»˜è®¤50è¿æ¥
	if task.Options != nil && task.Options.RateLimit != nil {
		if task.Options.RateLimit.SourceConnections > 0 {
			sourcePoolSize = task.Options.RateLimit.SourceConnections
		}
		if task.Options.RateLimit.TargetConnections > 0 {
			targetPoolSize = task.Options.RateLimit.TargetConnections
		}
	}

	taskLog.Info("Connection pool config", map[string]interface{}{
		"source_pool_size": sourcePoolSize,
		"target_pool_size": targetPoolSize,
	})

	// å°è¯•è¿æ¥æºç«¯ï¼ˆä½¿ç”¨é…ç½®çš„è¿æ¥æ± å¤§å°ï¼‰
	sourceClient, sourceIsCluster, err := connectRedisWithPoolSize(ctx, sourceAddrs, task.SourcePassword, sourcePoolSize)
	if err != nil {
		taskLog.Error("Failed to connect source cluster", map[string]interface{}{"error": err.Error()})
		tasksMu.Lock()
		task.Status = "failed"
		task.UpdatedAt = time.Now().Format(time.RFC3339)
		tasksMu.Unlock()
		return
	}
	defer sourceClient.Close()

	// å°è¯•è¿æ¥ç›®æ ‡ç«¯ï¼ˆä½¿ç”¨é…ç½®çš„è¿æ¥æ± å¤§å°ï¼‰
	targetClient, targetIsCluster, err := connectRedisWithPoolSize(ctx, targetAddrs, task.TargetPassword, targetPoolSize)
	if err != nil {
		taskLog.Error("Failed to connect target cluster", map[string]interface{}{"error": err.Error()})
		tasksMu.Lock()
		task.Status = "failed"
		task.UpdatedAt = time.Now().Format(time.RFC3339)
		tasksMu.Unlock()
		return
	}
	defer targetClient.Close()

	taskLog.Info("Connected to clusters", map[string]interface{}{
		"source_mode": map[bool]string{true: "cluster", false: "standalone"}[sourceIsCluster],
		"target_mode": map[bool]string{true: "cluster", false: "standalone"}[targetIsCluster],
	})

	// è®°å½•Keyè¿‡æ»¤é…ç½®
	if task.Options != nil && task.Options.KeyFilter != nil {
		taskLog.Info("Key filter config", map[string]interface{}{
			"mode":             task.Options.KeyFilter.Mode,
			"prefixes":         task.Options.KeyFilter.Prefixes,
			"exclude_prefixes": task.Options.KeyFilter.ExcludePrefixes,
			"patterns":         task.Options.KeyFilter.Patterns,
		})
	}

	// è·å–æºç«¯Keyæ€»æ•°
	totalKeys, err := getDBSize(ctx, sourceClient, sourceIsCluster)
	if err != nil {
		taskLog.Warn("Failed to get source DB size, using estimate", map[string]interface{}{"error": err.Error()})
		totalKeys = 10000 // é»˜è®¤ä¼°ç®—å€¼
	}

	tasksMu.Lock()
	task.KeysTotal = totalKeys
	task.BytesTotal = totalKeys * 256 // ä¼°ç®—å¹³å‡æ¯ä¸ªkey 256 bytes
	task.FullStartAt = time.Now().Format("2006-01-02 15:04:05")
	tasksMu.Unlock()

	taskLog.Info("Starting full migration", map[string]interface{}{
		"total_keys": totalKeys,
	})

	// æ‰§è¡Œå…¨é‡è¿ç§»
	doFullMigration(ctx, task, sourceClient, targetClient, sourceIsCluster, targetIsCluster, taskLog)

	// æ£€æŸ¥æ˜¯å¦éœ€è¦å¢é‡è¿ç§»
	tasksMu.RLock()
	status := task.Status
	mode := task.MigrationMode
	tasksMu.RUnlock()

	if status == "running" && mode == "full_and_incremental" {
		taskLog.Info("Starting incremental sync")
		tasksMu.Lock()
		task.Phase = "incremental"
		task.IncrStartAt = time.Now().Format("2006-01-02 15:04:05")
		tasksMu.Unlock()
		// å¢é‡åŒæ­¥é€»è¾‘ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼šæŒç»­ç›‘å¬ï¼‰
		doIncrementalSync(ctx, task, sourceClient, targetClient, sourceIsCluster, targetIsCluster, taskLog)
	}
}

// connectRedis è¿æ¥Redisï¼Œè¿”å›é€šç”¨å®¢æˆ·ç«¯æ¥å£
func connectRedis(ctx context.Context, addrs []string, password string) (redis.UniversalClient, bool, error) {
	return connectRedisWithPoolSize(ctx, addrs, password, 0)
}

// connectRedisWithPoolSize è¿æ¥Redisï¼Œæ”¯æŒè‡ªå®šä¹‰è¿æ¥æ± å¤§å°
func connectRedisWithPoolSize(ctx context.Context, addrs []string, password string, poolSize int) (redis.UniversalClient, bool, error) {
	// è®¾ç½®é»˜è®¤è¿æ¥æ± å¤§å°
	if poolSize <= 0 {
		poolSize = 10 // é»˜è®¤10ä¸ªè¿æ¥
	}
	
	// å…ˆå°è¯•é›†ç¾¤æ¨¡å¼
	clusterClient := redis.NewClusterClient(&redis.ClusterOptions{
		Addrs:        addrs,
		Password:     password,
		PoolSize:     poolSize,           // æ¯ä¸ªèŠ‚ç‚¹çš„è¿æ¥æ± å¤§å°
		MinIdleConns: poolSize / 4,       // æœ€å°ç©ºé—²è¿æ¥æ•°
		PoolTimeout:  30 * time.Second,   // ç­‰å¾…è¿æ¥çš„è¶…æ—¶æ—¶é—´
	})
	if err := clusterClient.Ping(ctx).Err(); err == nil {
		return clusterClient, true, nil
	}
	clusterClient.Close()

	// å°è¯•å•æœºæ¨¡å¼
	standaloneClient := redis.NewClient(&redis.Options{
		Addr:         addrs[0],
		Password:     password,
		PoolSize:     poolSize,           // è¿æ¥æ± å¤§å°
		MinIdleConns: poolSize / 4,       // æœ€å°ç©ºé—²è¿æ¥æ•°
		PoolTimeout:  30 * time.Second,   // ç­‰å¾…è¿æ¥çš„è¶…æ—¶æ—¶é—´
	})
	if err := standaloneClient.Ping(ctx).Err(); err != nil {
		standaloneClient.Close()
		return nil, false, err
	}
	return standaloneClient, false, nil
}

// getDBSize è·å–æ•°æ®åº“Keyæ•°é‡
func getDBSize(ctx context.Context, client redis.UniversalClient, isCluster bool) (int64, error) {
	if !isCluster {
		return client.DBSize(ctx).Result()
	}

	// é›†ç¾¤æ¨¡å¼éœ€è¦éå†æ‰€æœ‰èŠ‚ç‚¹
	clusterClient := client.(*redis.ClusterClient)
	var total int64
	var mu sync.Mutex

	err := clusterClient.ForEachMaster(ctx, func(ctx context.Context, node *redis.Client) error {
		size, err := node.DBSize(ctx).Result()
		if err != nil {
			return err
		}
		mu.Lock()
		total += size
		mu.Unlock()
		return nil
	})
	return total, err
}

// WorkerInfo Workerä¿¡æ¯
type WorkerInfo struct {
	ID            int
	StopChan      chan struct{}       // åœæ­¢ä¿¡å·
	StoppedChan   chan struct{}       // å·²åœæ­¢ç¡®è®¤
	ProcessingKey string              // å½“å‰æ­£åœ¨å¤„ç†çš„Keyï¼ˆç”¨äºæ—¥å¿—ï¼‰
	IsProcessing  int32               // æ˜¯å¦æ­£åœ¨å¤„ç†Keyï¼ˆåŸå­æ“ä½œï¼‰
	CreatedAt     time.Time           // åˆ›å»ºæ—¶é—´ï¼ˆç”¨äºLIFOï¼‰
}

// DynamicWorkerPool åŠ¨æ€Workeræ± ï¼Œæ”¯æŒè¿è¡Œæ—¶è°ƒæ•´Workeræ•°é‡
type DynamicWorkerPool struct {
	task           *Task
	ctx            context.Context
	keyChan        chan string
	wg             sync.WaitGroup
	mu             sync.RWMutex
	activeWorkers  int32                    // å½“å‰æ´»è·ƒWorkeræ•°é‡
	targetWorkers  int32                    // ç›®æ ‡Workeræ•°é‡
	workers        []*WorkerInfo            // Workeråˆ—è¡¨ï¼ˆæœ‰åºï¼Œç”¨äºLIFOï¼‰
	nextWorkerID   int                      // ä¸‹ä¸€ä¸ªWorker ID
	taskLog        *logger.TaskLogger
	
	// è¿ç§»ç›¸å…³å‚æ•°
	sourceClient   redis.UniversalClient
	targetClient   redis.UniversalClient
	conflictPolicy string
	rateLimiter    *RateLimiter             // æºç«¯é™é€Ÿå™¨
	targetLimiter  *RateLimiter             // ç›®æ ‡ç«¯é™é€Ÿå™¨
	rateLimiterMu  sync.RWMutex             // é™é€Ÿå™¨é”ï¼ˆæ”¯æŒåŠ¨æ€æ›´æ–°ï¼‰
	processedKeys  *sync.Map
	
	// ç»Ÿè®¡è®¡æ•°å™¨
	migratedCount  *int64
	migratedBytes  *int64
	failedCount    *int64
	skippedCount   *int64
	filteredCount  *int64
}

// NewDynamicWorkerPool åˆ›å»ºåŠ¨æ€Workeræ± 
func NewDynamicWorkerPool(ctx context.Context, task *Task, keyChan chan string, taskLog *logger.TaskLogger,
	sourceClient, targetClient redis.UniversalClient, conflictPolicy string, 
	sourceLimiter, targetLimiter *RateLimiter,
	processedKeys *sync.Map, migratedCount, migratedBytes, failedCount, skippedCount, filteredCount *int64) *DynamicWorkerPool {
	
	return &DynamicWorkerPool{
		task:           task,
		ctx:            ctx,
		keyChan:        keyChan,
		workers:        make([]*WorkerInfo, 0),
		nextWorkerID:   0,
		taskLog:        taskLog,
		sourceClient:   sourceClient,
		targetClient:   targetClient,
		conflictPolicy: conflictPolicy,
		rateLimiter:    sourceLimiter,
		targetLimiter:  targetLimiter,
		processedKeys:  processedKeys,
		migratedCount:  migratedCount,
		migratedBytes:  migratedBytes,
		failedCount:    failedCount,
		skippedCount:   skippedCount,
		filteredCount:  filteredCount,
	}
}

// SetWorkerCount åŠ¨æ€è°ƒæ•´Workeræ•°é‡
func (p *DynamicWorkerPool) SetWorkerCount(count int) {
	atomic.StoreInt32(&p.targetWorkers, int32(count))
}

// GetActiveWorkerCount è·å–å½“å‰æ´»è·ƒWorkeræ•°é‡
func (p *DynamicWorkerPool) GetActiveWorkerCount() int {
	return int(atomic.LoadInt32(&p.activeWorkers))
}

// UpdateRateLimiter åŠ¨æ€æ›´æ–°æºç«¯é™é€Ÿå™¨
func (p *DynamicWorkerPool) UpdateRateLimiter(newQPS int) {
	p.rateLimiterMu.Lock()
	defer p.rateLimiterMu.Unlock()
	
	oldQPS := 0
	if p.rateLimiter != nil {
		oldQPS = p.rateLimiter.qps
	}
	
	// QPS æ²¡æœ‰å˜åŒ–ï¼Œæ— éœ€æ›´æ–°
	if oldQPS == newQPS {
		return
	}
	
	// åœæ­¢æ—§çš„é™é€Ÿå™¨
	if p.rateLimiter != nil {
		p.rateLimiter.Stop()
		p.rateLimiter = nil
	}
	
	// åˆ›å»ºæ–°çš„é™é€Ÿå™¨ï¼ˆå¦‚æœ QPS > 0ï¼‰
	if newQPS > 0 {
		p.rateLimiter = NewRateLimiter(newQPS)
	}
	
	p.taskLog.Info("Source rate limiter updated dynamically", map[string]interface{}{
		"old_qps": oldQPS,
		"new_qps": newQPS,
	})
}

// UpdateTargetRateLimiter åŠ¨æ€æ›´æ–°ç›®æ ‡ç«¯é™é€Ÿå™¨
func (p *DynamicWorkerPool) UpdateTargetRateLimiter(newQPS int) {
	p.rateLimiterMu.Lock()
	defer p.rateLimiterMu.Unlock()
	
	oldQPS := 0
	if p.targetLimiter != nil {
		oldQPS = p.targetLimiter.qps
	}
	
	// QPS æ²¡æœ‰å˜åŒ–ï¼Œæ— éœ€æ›´æ–°
	if oldQPS == newQPS {
		return
	}
	
	// åœæ­¢æ—§çš„é™é€Ÿå™¨
	if p.targetLimiter != nil {
		p.targetLimiter.Stop()
		p.targetLimiter = nil
	}
	
	// åˆ›å»ºæ–°çš„é™é€Ÿå™¨ï¼ˆå¦‚æœ QPS > 0ï¼‰
	if newQPS > 0 {
		p.targetLimiter = NewRateLimiter(newQPS)
	}
	
	p.taskLog.Info("Target rate limiter updated dynamically", map[string]interface{}{
		"old_qps": oldQPS,
		"new_qps": newQPS,
	})
}

// GetTargetRateLimiter è·å–ç›®æ ‡ç«¯é™é€Ÿå™¨ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
func (p *DynamicWorkerPool) GetTargetRateLimiter() *RateLimiter {
	p.rateLimiterMu.RLock()
	defer p.rateLimiterMu.RUnlock()
	return p.targetLimiter
}

// GetRateLimiter è·å–å½“å‰é™é€Ÿå™¨ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
func (p *DynamicWorkerPool) GetRateLimiter() *RateLimiter {
	p.rateLimiterMu.RLock()
	defer p.rateLimiterMu.RUnlock()
	return p.rateLimiter
}

// Start å¯åŠ¨æŒ‡å®šæ•°é‡çš„Worker
func (p *DynamicWorkerPool) Start(initialCount int) {
	atomic.StoreInt32(&p.targetWorkers, int32(initialCount))
	for i := 0; i < initialCount; i++ {
		p.addWorker()
	}
}

// addWorker æ·»åŠ ä¸€ä¸ªæ–°Worker
func (p *DynamicWorkerPool) addWorker() {
	p.mu.Lock()
	workerID := p.nextWorkerID
	p.nextWorkerID++
	
	workerInfo := &WorkerInfo{
		ID:          workerID,
		StopChan:    make(chan struct{}),
		StoppedChan: make(chan struct{}),
		CreatedAt:   time.Now(),
	}
	p.workers = append(p.workers, workerInfo)
	p.mu.Unlock()
	
	atomic.AddInt32(&p.activeWorkers, 1)
	p.wg.Add(1)
	
	go p.workerLoop(workerInfo)
	
	p.taskLog.Info("Worker started", map[string]interface{}{
		"worker_id":      workerID,
		"active_workers": atomic.LoadInt32(&p.activeWorkers),
	})
}

// removeWorkerSmart æ™ºèƒ½é€‰æ‹©Workeråœæ­¢ï¼šä¼˜å…ˆç©ºé—²Workerï¼Œå…¶æ¬¡LIFOï¼ˆä¼˜é›…åœæ­¢ï¼‰
func (p *DynamicWorkerPool) removeWorkerSmart() bool {
	p.mu.Lock()
	
	if len(p.workers) == 0 {
		p.mu.Unlock()
		return false
	}
	
	// ç­–ç•¥ï¼šä¼˜å…ˆæ‰¾ç©ºé—²çš„Workerï¼ˆä»åå¾€å‰æ‰¾ï¼Œä¿æŒLIFOå€¾å‘ï¼‰
	var targetIdx = -1
	var selectionReason string
	
	for i := len(p.workers) - 1; i >= 0; i-- {
		if atomic.LoadInt32(&p.workers[i].IsProcessing) == 0 {
			targetIdx = i
			selectionReason = "idle"
			break
		}
	}
	
	// å¦‚æœæ²¡æœ‰ç©ºé—²çš„ï¼Œé€‰æœ€åä¸€ä¸ªï¼ˆLIFOå…œåº•ï¼‰
	if targetIdx == -1 {
		targetIdx = len(p.workers) - 1
		selectionReason = "LIFO (all busy)"
	}
	
	// ç§»é™¤é€‰ä¸­çš„Worker
	workerInfo := p.workers[targetIdx]
	p.workers = append(p.workers[:targetIdx], p.workers[targetIdx+1:]...)
	p.mu.Unlock()
	
	// è®°å½•Workerå½“å‰çŠ¶æ€
	isProcessing := atomic.LoadInt32(&workerInfo.IsProcessing) == 1
	processingKey := workerInfo.ProcessingKey
	
	p.taskLog.Info("Stopping worker (smart selection)", map[string]interface{}{
		"worker_id":        workerInfo.ID,
		"selection_reason": selectionReason,
		"is_processing":    isProcessing,
		"processing_key":   processingKey,
		"created_at":       workerInfo.CreatedAt.Format("15:04:05"),
	})
	
	// å‘é€åœæ­¢ä¿¡å·
	close(workerInfo.StopChan)
	
	// ç­‰å¾…Workerç¡®è®¤åœæ­¢ï¼ˆç©ºé—²Workeråº”è¯¥ç«‹å³åœæ­¢ï¼Œå¿™ç¢ŒWorkeréœ€è¦ç­‰å¾…ï¼‰
	timeout := 5 * time.Second
	if isProcessing {
		timeout = 30 * time.Second // å¿™ç¢ŒWorkerç»™æ›´é•¿æ—¶é—´
	}
	
	select {
	case <-workerInfo.StoppedChan:
		p.taskLog.Info("Worker stopped confirmed", map[string]interface{}{
			"worker_id":        workerInfo.ID,
			"selection_reason": selectionReason,
		})
	case <-time.After(timeout):
		p.taskLog.Warn("Worker stop timeout, force continue", map[string]interface{}{
			"worker_id": workerInfo.ID,
			"timeout":   timeout.String(),
		})
	}
	
	return true
}

// workerLoop Workerçš„ä¸»å¾ªç¯ï¼ˆæ”¹è¿›ç‰ˆï¼šå®Œå…¨ä¼˜é›…åœæ­¢ï¼‰
func (p *DynamicWorkerPool) workerLoop(info *WorkerInfo) {
	defer func() {
		atomic.AddInt32(&p.activeWorkers, -1)
		p.wg.Done()
		// å‘é€å·²åœæ­¢ç¡®è®¤
		close(info.StoppedChan)
		p.taskLog.Debug("Worker exited", map[string]interface{}{
			"worker_id":      info.ID,
			"active_workers": atomic.LoadInt32(&p.activeWorkers),
		})
	}()
	
	shouldStop := false
	
	for {
		// å…ˆæ£€æŸ¥æ˜¯å¦æ”¶åˆ°åœæ­¢ä¿¡å·ï¼ˆéé˜»å¡ï¼‰
		select {
		case <-info.StopChan:
			shouldStop = true
		default:
		}
		
		// å¦‚æœå·²æ”¶åˆ°åœæ­¢ä¿¡å·ä¸”å½“å‰æ²¡æœ‰åœ¨å¤„ç†Keyï¼Œåˆ™é€€å‡º
		if shouldStop && atomic.LoadInt32(&info.IsProcessing) == 0 {
			p.taskLog.Debug("Worker stopping gracefully (no pending work)", map[string]interface{}{
				"worker_id": info.ID,
			})
			return
		}
		
		// å°è¯•ä»é€šé“è·å–Keyï¼ˆå¸¦è¶…æ—¶ï¼Œä¾¿äºå®šæœŸæ£€æŸ¥åœæ­¢ä¿¡å·ï¼‰
		select {
		case <-info.StopChan:
			// æ”¶åˆ°åœæ­¢ä¿¡å·
			if atomic.LoadInt32(&info.IsProcessing) == 0 {
				p.taskLog.Debug("Worker stopping gracefully (stop signal received)", map[string]interface{}{
					"worker_id": info.ID,
				})
				return
			}
			// æ­£åœ¨å¤„ç†ä¸­ï¼Œæ ‡è®°éœ€è¦åœæ­¢ï¼Œä½†ç»§ç»­å®Œæˆå½“å‰å·¥ä½œ
			shouldStop = true
			continue
			
		case key, ok := <-p.keyChan:
			if !ok {
				// é€šé“å…³é—­ï¼Œé€€å‡º
				return
			}
			
			// å¦‚æœå·²æ ‡è®°åœæ­¢ï¼Œå°†keyæ”¾å›é€šé“è®©å…¶ä»–workerå¤„ç†ï¼Œç„¶åé€€å‡º
			if shouldStop {
				// å°è¯•æ”¾å›ï¼ˆéé˜»å¡ï¼‰ï¼Œå¦‚æœé€šé“æ»¡äº†å°±ä¸¢å¼ƒï¼ˆä¼šåœ¨scanæ—¶é‡æ–°æ‰«åˆ°ï¼‰
				select {
				case p.keyChan <- key:
					p.taskLog.Debug("Key returned to channel", map[string]interface{}{
						"worker_id": info.ID,
						"key":       key,
					})
				default:
					// é€šé“æ»¡äº†ï¼Œè¿™ä¸ªkeyä¼šä¸¢å¤±ï¼Œä½†scanä¼šé‡æ–°æ‰«æåˆ°
					p.taskLog.Warn("Could not return key to channel (full)", map[string]interface{}{
						"worker_id": info.ID,
						"key":       key,
					})
				}
				return
			}
			
			// æ ‡è®°æ­£åœ¨å¤„ç†
			atomic.StoreInt32(&info.IsProcessing, 1)
			info.ProcessingKey = key
			
			// å¤„ç†Key
			p.processKey(info.ID, key)
			
			// æ ‡è®°å¤„ç†å®Œæˆ
			info.ProcessingKey = ""
			atomic.StoreInt32(&info.IsProcessing, 0)
			
			// å¤„ç†å®Œæˆåå†æ¬¡æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢
			if shouldStop {
				p.taskLog.Info("Worker completed current key and stopping", map[string]interface{}{
					"worker_id":    info.ID,
					"completed_key": key,
				})
				return
			}
			
		case <-time.After(100 * time.Millisecond):
			// è¶…æ—¶ï¼Œç»§ç»­å¾ªç¯æ£€æŸ¥åœæ­¢ä¿¡å·
			continue
		}
	}
}

// processKey å¤„ç†å•ä¸ªKeyçš„è¿ç§»
func (p *DynamicWorkerPool) processKey(workerID int, key string) {
	// æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
	tasksMu.RLock()
	status := p.task.Status
	tasksMu.RUnlock()
	if status != "running" {
		return
	}

	// æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
	if _, loaded := p.processedKeys.LoadOrStore(key, true); loaded {
		return
	}

	// æºç«¯é™é€Ÿï¼ˆè¯»å–æ“ä½œï¼‰
	if rl := p.GetRateLimiter(); rl != nil {
		rl.Wait()
	}

	// æ£€æŸ¥Keyæ˜¯å¦åŒ¹é…è¿‡æ»¤è§„åˆ™
	if !matchKeyFilter(key, p.task.Options) {
		atomic.AddInt64(p.filteredCount, 1)
		return
	}

	// è¿ç§»Keyï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
	var migrated bool
	var bytes int64
	var reason string
	maxRetries, fullIntervalMs, _ := getRetryConfig(p.task.Options)
	
	for retry := 0; retry < maxRetries; retry++ {
		// ç›®æ ‡ç«¯é™é€Ÿï¼ˆå†™å…¥æ“ä½œï¼‰- åœ¨æ¯æ¬¡é‡è¯•å‰éƒ½è¿›è¡Œé™é€Ÿ
		if tl := p.GetTargetRateLimiter(); tl != nil {
			tl.Wait()
		}
		
		migrated, bytes, reason = migrateKeyWithPolicy(p.ctx, p.sourceClient, p.targetClient, key, p.conflictPolicy)
		if migrated || reason == "skipped" || reason == "filtered" || reason == "" {
			break
		}
		if retry < maxRetries-1 {
			time.Sleep(time.Duration((retry+1)*fullIntervalMs) * time.Millisecond)
		}
	}

	if migrated {
		atomic.AddInt64(p.migratedCount, 1)
		atomic.AddInt64(p.migratedBytes, bytes)
	} else if reason == "skipped" {
		atomic.AddInt64(p.skippedCount, 1)
	} else if reason == "filtered" {
		atomic.AddInt64(p.filteredCount, 1)
	} else {
		atomic.AddInt64(p.failedCount, 1)
		addErrorKey(p.task.ID, key, "string", "failed", reason+" (after "+fmt.Sprintf("%d", maxRetries)+" retries)")
	}
}

// AdjustWorkers è°ƒæ•´Workeræ•°é‡åˆ°ç›®æ ‡å€¼
func (p *DynamicWorkerPool) AdjustWorkers() {
	target := int(atomic.LoadInt32(&p.targetWorkers))
	current := int(atomic.LoadInt32(&p.activeWorkers))
	
	if target > current {
		// éœ€è¦å¢åŠ Worker
		toAdd := target - current
		for i := 0; i < toAdd; i++ {
			p.addWorker()
		}
		p.taskLog.Info("Workers increased", map[string]interface{}{
			"from":  current,
			"to":    target,
			"added": toAdd,
		})
	} else if target < current {
		// éœ€è¦å‡å°‘Workerï¼ˆæ™ºèƒ½é€‰æ‹©ï¼šä¼˜å…ˆç©ºé—²ï¼Œå…¶æ¬¡LIFOï¼‰
		toRemove := current - target
		for i := 0; i < toRemove; i++ {
			p.removeWorkerSmart()
		}
		p.taskLog.Info("Workers decreased (smart selection)", map[string]interface{}{
			"from":    current,
			"to":      target,
			"removed": toRemove,
		})
	}
}

// Wait ç­‰å¾…æ‰€æœ‰Workerå®Œæˆ
func (p *DynamicWorkerPool) Wait() {
	p.wg.Wait()
}

// StopAll åœæ­¢æ‰€æœ‰Worker
func (p *DynamicWorkerPool) StopAll() {
	p.mu.Lock()
	workers := make([]*WorkerInfo, len(p.workers))
	copy(workers, p.workers)
	p.workers = p.workers[:0]
	p.mu.Unlock()
	
	// å¹¶è¡Œå‘é€åœæ­¢ä¿¡å·
	for _, w := range workers {
		close(w.StopChan)
	}
	
	// ç­‰å¾…æ‰€æœ‰Workerç¡®è®¤åœæ­¢
	for _, w := range workers {
		select {
		case <-w.StoppedChan:
		case <-time.After(30 * time.Second):
			p.taskLog.Warn("Worker stop timeout during StopAll", map[string]interface{}{
				"worker_id": w.ID,
			})
		}
	}
}

// GetWorkerStatus è·å–æ‰€æœ‰Workerçš„çŠ¶æ€ï¼ˆç”¨äºç›‘æ§ï¼‰
func (p *DynamicWorkerPool) GetWorkerStatus() []map[string]interface{} {
	p.mu.RLock()
	defer p.mu.RUnlock()
	
	status := make([]map[string]interface{}, len(p.workers))
	for i, w := range p.workers {
		status[i] = map[string]interface{}{
			"id":            w.ID,
			"created_at":    w.CreatedAt.Format("2006-01-02 15:04:05"),
			"is_processing": atomic.LoadInt32(&w.IsProcessing) == 1,
			"current_key":   w.ProcessingKey,
		}
	}
	return status
}

// doFullMigration æ‰§è¡Œå…¨é‡è¿ç§»ï¼ˆå¹¶è¡ŒWorkeræ¨¡å¼ - æ”¯æŒåŠ¨æ€è°ƒæ•´ï¼‰
func doFullMigration(ctx context.Context, task *Task, sourceClient, targetClient redis.UniversalClient, sourceIsCluster, targetIsCluster bool, taskLog *logger.TaskLogger) {
	// è·å–é…ç½®å‚æ•°
	batchSize := int64(1000)
	workerCount := 4
	var sourceLimiter *RateLimiter
	var targetLimiter *RateLimiter

	if task.Options != nil {
		if task.Options.ScanBatchSize > 0 {
			batchSize = int64(task.Options.ScanBatchSize)
		}
		if task.Options.WorkerCount > 0 {
			workerCount = task.Options.WorkerCount
		}
		// åˆå§‹åŒ–æºç«¯é™é€Ÿå™¨
		if task.Options.RateLimit != nil && task.Options.RateLimit.SourceQPS > 0 {
			sourceLimiter = NewRateLimiter(task.Options.RateLimit.SourceQPS)
		}
		// åˆå§‹åŒ–ç›®æ ‡ç«¯é™é€Ÿå™¨
		if task.Options.RateLimit != nil && task.Options.RateLimit.TargetQPS > 0 {
			targetLimiter = NewRateLimiter(task.Options.RateLimit.TargetQPS)
		}
	}

	// è·å–å†²çªç­–ç•¥
	conflictPolicy := "skip_full_only"
	if task.Options != nil && task.Options.ConflictPolicy != "" {
		conflictPolicy = task.Options.ConflictPolicy
	}

	// è·å–QPSé…ç½®ç”¨äºæ—¥å¿—
	sourceQPS := 0
	targetQPS := 0
	if task.Options != nil && task.Options.RateLimit != nil {
		sourceQPS = task.Options.RateLimit.SourceQPS
		targetQPS = task.Options.RateLimit.TargetQPS
	}

	taskLog.Info("Starting parallel migration with dynamic worker pool", map[string]interface{}{
		"worker_count": workerCount,
		"batch_size":   batchSize,
		"policy":       conflictPolicy,
		"source_qps":   sourceQPS,
		"target_qps":   targetQPS,
	})

	// ç»Ÿè®¡è®¡æ•°å™¨ï¼ˆä½¿ç”¨åŸå­æ“ä½œï¼‰
	var migratedCount int64
	var migratedBytes int64
	var failedCount int64
	var skippedCount int64
	var filteredCount int64
	startTime := time.Now()
	lastLogTime := time.Now()
	var lastLogMu sync.Mutex

	// ç”¨äºè¿½è¸ªå·²å¤„ç†çš„keyï¼ˆé¿å…é‡å¤å¤„ç†ï¼‰
	processedKeys := sync.Map{}

	// åˆ›å»ºKeyé€šé“ï¼ˆç¼“å†²åŒºå¤§å°åŠ¨æ€è°ƒæ•´ï¼‰
	keyChan := make(chan string, workerCount*100)

	// åˆ›å»ºåŠ¨æ€Workeræ± 
	workerPool := NewDynamicWorkerPool(ctx, task, keyChan, taskLog,
		sourceClient, targetClient, conflictPolicy, sourceLimiter, targetLimiter,
		&processedKeys, &migratedCount, &migratedBytes, &failedCount, &skippedCount, &filteredCount)
	
	// å¯åŠ¨åˆå§‹Worker
	workerPool.Start(workerCount)
	
	// æ³¨å†ŒWorkeræ± åˆ°ä»»åŠ¡ï¼ˆç”¨äºåŠ¨æ€è°ƒæ•´ï¼‰
	tasksMu.Lock()
	task.workerPool = workerPool
	tasksMu.Unlock()

	// è¿›åº¦æ›´æ–°åç¨‹ï¼ˆåŒ…å«WorkeråŠ¨æ€è°ƒæ•´æ£€æŸ¥ï¼‰
	stopProgress := make(chan struct{})
	go func() {
		ticker := time.NewTicker(500 * time.Millisecond)
		defer ticker.Stop()
		for {
			select {
			case <-stopProgress:
				return
			case <-ticker.C:
				mc := atomic.LoadInt64(&migratedCount)
				mb := atomic.LoadInt64(&migratedBytes)
				fc := atomic.LoadInt64(&failedCount)
				sc := atomic.LoadInt64(&skippedCount)
				ftc := atomic.LoadInt64(&filteredCount)
				
				// æ£€æŸ¥æ˜¯å¦éœ€è¦åŠ¨æ€è°ƒæ•´é…ç½®
				tasksMu.RLock()
				targetWorkerCount := 4
				targetSourceQPS := 0
				targetTargetQPS := 0
				if task.Options != nil {
					if task.Options.WorkerCount > 0 {
						targetWorkerCount = task.Options.WorkerCount
					}
					if task.Options.RateLimit != nil {
						targetSourceQPS = task.Options.RateLimit.SourceQPS
						targetTargetQPS = task.Options.RateLimit.TargetQPS
					}
				}
				tasksMu.RUnlock()
				
				// åŠ¨æ€è°ƒæ•´Worker
				workerPool.SetWorkerCount(targetWorkerCount)
				workerPool.AdjustWorkers()
				
				// åŠ¨æ€è°ƒæ•´æºç«¯QPSé™é€Ÿå™¨
				workerPool.UpdateRateLimiter(targetSourceQPS)
				
				// åŠ¨æ€è°ƒæ•´ç›®æ ‡ç«¯QPSé™é€Ÿå™¨
				workerPool.UpdateTargetRateLimiter(targetTargetQPS)
				
				currentWorkers := workerPool.GetActiveWorkerCount()

				tasksMu.Lock()
				task.KeysMigrated = mc
				task.BytesMigrated = mb
				task.KeysFailed = fc
				task.KeysSkipped = sc
				task.KeysFiltered = ftc
				task.ActiveWorkers = currentWorkers  // æ›´æ–°å½“å‰æ´»è·ƒWorkeræ•°
				if task.KeysTotal > 0 {
					task.Progress = float64(mc+sc+ftc) / float64(task.KeysTotal) * 100
					if task.Progress > 100 {
						task.Progress = 100
					}
				}
				elapsed := time.Since(startTime).Seconds()
				if elapsed > 0 {
					task.Speed = int64(float64(mc) / elapsed)
				}
				task.UpdatedAt = time.Now().Format(time.RFC3339)
				tasksMu.Unlock()

				// æ¯10ç§’è®°å½•ä¸€æ¬¡æ—¥å¿—
				lastLogMu.Lock()
				if time.Since(lastLogTime) > 10*time.Second {
					taskLog.Info("Migration progress", map[string]interface{}{
						"progress":       fmt.Sprintf("%.1f%%", task.Progress),
						"migrated_keys":  mc,
						"failed_keys":    fc,
						"skipped_keys":   sc,
						"filtered_keys":  ftc,
						"speed":          task.Speed,
						"active_workers": currentWorkers,
						"target_workers": targetWorkerCount,
					})
					lastLogTime = time.Now()
				}
				lastLogMu.Unlock()
			}
		}
	}()

	// SCANå¹¶åˆ†å‘Keyåˆ°Worker
	// è¾…åŠ©å‡½æ•°ï¼šåŠ¨æ€è·å–æ‰¹æ¬¡å¤§å°
	getBatchSize := func() int64 {
		tasksMu.RLock()
		defer tasksMu.RUnlock()
		if task.Options != nil && task.Options.ScanBatchSize > 0 {
			return int64(task.Options.ScanBatchSize)
		}
		return 1000
	}
	
	if sourceIsCluster {
		// é›†ç¾¤æ¨¡å¼ï¼šå¹¶è¡Œéå†æ‰€æœ‰masterèŠ‚ç‚¹
		clusterClient := sourceClient.(*redis.ClusterClient)
		var scanWg sync.WaitGroup

		clusterClient.ForEachMaster(ctx, func(ctx context.Context, node *redis.Client) error {
			scanWg.Add(1)
			go func(nodeClient *redis.Client) {
				defer scanWg.Done()
				var cursor uint64
				for {
					tasksMu.RLock()
					status := task.Status
					tasksMu.RUnlock()
					if status != "running" {
						return
					}

					// åŠ¨æ€è·å–æ‰¹æ¬¡å¤§å°
					currentBatchSize := getBatchSize()
					keys, newCursor, err := nodeClient.Scan(ctx, cursor, "*", currentBatchSize).Result()
					if err != nil {
						taskLog.Warn("SCAN failed on node", map[string]interface{}{"error": err.Error()})
						time.Sleep(time.Second)
						continue
					}

					for _, key := range keys {
						tasksMu.RLock()
						status := task.Status
						tasksMu.RUnlock()
						if status != "running" {
							return
						}
						keyChan <- key
					}

					cursor = newCursor
					if cursor == 0 {
						break
					}
				}
			}(node)
			return nil
		})

		scanWg.Wait()
	} else {
		// å•æœºæ¨¡å¼
		var cursor uint64
		for {
			tasksMu.RLock()
			status := task.Status
			tasksMu.RUnlock()
			if status != "running" {
				break
			}

			// åŠ¨æ€è·å–æ‰¹æ¬¡å¤§å°
			currentBatchSize := getBatchSize()
			keys, newCursor, err := sourceClient.Scan(ctx, cursor, "*", currentBatchSize).Result()
			if err != nil {
				taskLog.Error("SCAN failed", map[string]interface{}{"error": err.Error()})
				time.Sleep(time.Second)
				continue
			}

			for _, key := range keys {
				tasksMu.RLock()
				status := task.Status
				tasksMu.RUnlock()
				if status != "running" {
					break
				}
				keyChan <- key
			}

			cursor = newCursor
			if cursor == 0 {
				break
			}
		}
	}

	// å…³é—­é€šé“ï¼Œç­‰å¾…æ‰€æœ‰Workerå®Œæˆ
	close(keyChan)
	workerPool.Wait()
	close(stopProgress)
	
	// æ¸…ç†Workeræ± å¼•ç”¨
	tasksMu.Lock()
	task.workerPool = nil
	tasksMu.Unlock()

	// æœ€ç»ˆæ›´æ–°ç»Ÿè®¡
	mc := atomic.LoadInt64(&migratedCount)
	fc := atomic.LoadInt64(&failedCount)
	sc := atomic.LoadInt64(&skippedCount)
	ftc := atomic.LoadInt64(&filteredCount)

	tasksMu.Lock()
	task.KeysMigrated = mc
	task.KeysFailed = fc
	task.KeysSkipped = sc
	task.KeysFiltered = ftc
	if task.Status == "running" {
		if task.MigrationMode == "full_only" {
			task.Status = "completed"
			task.Progress = 100
			task.Phase = "completed"
		} else {
			// å…¨é‡è¿ç§»å®Œæˆï¼Œå‡†å¤‡è¿›å…¥å¢é‡åŒæ­¥
			task.Progress = 100
			task.Phase = "incremental"
		}
	}
	task.UpdatedAt = time.Now().Format(time.RFC3339)
	tasksMu.Unlock()

	taskLog.Info("Full migration completed", map[string]interface{}{
		"migrated_keys": mc,
		"failed_keys":   fc,
		"skipped_keys":  sc,
		"filtered_keys": ftc,
		"duration":      time.Since(startTime).String(),
		"avg_speed":     int64(float64(mc) / time.Since(startTime).Seconds()),
	})
}

// RateLimiter ç®€å•çš„é™é€Ÿå™¨
type RateLimiter struct {
	qps      int                // QPSå€¼ï¼ˆç”¨äºæ¯”è¾ƒæ˜¯å¦éœ€è¦æ›´æ–°ï¼‰
	ticker   *time.Ticker
	tokens   chan struct{}
	stopChan chan struct{}
}

// NewRateLimiter åˆ›å»ºé™é€Ÿå™¨
func NewRateLimiter(qps int) *RateLimiter {
	if qps <= 0 {
		return nil
	}
	interval := time.Second / time.Duration(qps)
	if interval < time.Microsecond {
		interval = time.Microsecond
	}

	rl := &RateLimiter{
		qps:      qps,
		ticker:   time.NewTicker(interval),
		tokens:   make(chan struct{}, qps),
		stopChan: make(chan struct{}),
	}

	// é¢„å¡«å……tokens
	for i := 0; i < qps/10+1; i++ {
		select {
		case rl.tokens <- struct{}{}:
		default:
		}
	}

	// æŒç»­å¡«å……tokens
	go func() {
		for {
			select {
			case <-rl.stopChan:
				return
			case <-rl.ticker.C:
				select {
				case rl.tokens <- struct{}{}:
				default:
				}
			}
		}
	}()

	return rl
}

// Wait ç­‰å¾…è·å–ä»¤ç‰Œ
func (rl *RateLimiter) Wait() {
	if rl == nil {
		return
	}
	<-rl.tokens
}

// Stop åœæ­¢é™é€Ÿå™¨
func (rl *RateLimiter) Stop() {
	if rl == nil {
		return
	}
	close(rl.stopChan)
	rl.ticker.Stop()
}

// matchKeyFilter æ£€æŸ¥Keyæ˜¯å¦åŒ¹é…è¿‡æ»¤è§„åˆ™
func matchKeyFilter(key string, options *TaskOptions) bool {
	if options == nil || options.KeyFilter == nil {
		return true
	}

	filter := options.KeyFilter
	switch filter.Mode {
	case "prefix":
		// æ£€æŸ¥æ’é™¤å‰ç¼€
		for _, prefix := range filter.ExcludePrefixes {
			if strings.HasPrefix(key, prefix) {
				return false
			}
		}
		// å¦‚æœè®¾ç½®äº†åŒ…å«å‰ç¼€ï¼Œåªè¿ç§»åŒ¹é…çš„
		if len(filter.Prefixes) > 0 {
			for _, prefix := range filter.Prefixes {
				if strings.HasPrefix(key, prefix) {
					return true
				}
			}
			return false
		}
		return true
	case "pattern":
		// æ­£åˆ™åŒ¹é…ï¼ˆç®€åŒ–å®ç°ï¼Œä½¿ç”¨ strings.Containsï¼‰
		if len(filter.Patterns) > 0 {
			for _, pattern := range filter.Patterns {
				if strings.Contains(key, pattern) {
					return true
				}
			}
			return false
		}
		return true
	default:
		return true
	}
}

// migrateKeyWithPolicy æ ¹æ®å†²çªç­–ç•¥è¿ç§»Key
func migrateKeyWithPolicy(ctx context.Context, sourceClient, targetClient redis.UniversalClient, key string, policy string) (bool, int64, string) {
	// è·å–Keyçš„TTL
	ttl, err := sourceClient.TTL(ctx, key).Result()
	if err != nil {
		return false, 0, "get TTL failed: " + err.Error()
	}

	// ä½¿ç”¨DUMP+RESTOREè¿ç§»
	dump, err := sourceClient.Dump(ctx, key).Result()
	if err != nil {
		if err == redis.Nil {
			return false, 0, "skipped" // Keyä¸å­˜åœ¨ï¼Œè·³è¿‡
		}
		return false, 0, "dump failed: " + err.Error()
	}

	bytes := int64(len(dump))

	// æ£€æŸ¥ç›®æ ‡æ˜¯å¦å­˜åœ¨
	exists, err := targetClient.Exists(ctx, key).Result()
	if err != nil {
		return false, 0, "check exists failed: " + err.Error()
	}

	if exists > 0 {
		switch policy {
		case "skip", "skip_full_only":
			return false, 0, "skipped"
		case "replace":
			// å…ˆåˆ é™¤ç›®æ ‡Key
			if err := targetClient.Del(ctx, key).Err(); err != nil {
				return false, 0, "delete failed: " + err.Error()
			}
		case "error":
			return false, 0, "key conflict"
		default:
			return false, 0, "skipped"
		}
	}

	// RESTOREåˆ°ç›®æ ‡
	if ttl < 0 {
		ttl = 0 // æ— è¿‡æœŸæ—¶é—´
	}
	err = targetClient.Restore(ctx, key, ttl, dump).Err()
	if err != nil {
		return false, 0, "restore failed: " + err.Error()
	}

	return true, bytes, ""
}

// migrateKey è¿ç§»å•ä¸ªKeyï¼ˆä¿ç•™æ—§å‡½æ•°ä¾›å…¼å®¹ï¼‰
func migrateKey(ctx context.Context, sourceClient, targetClient redis.UniversalClient, key string) (bool, int64, string) {
	return migrateKeyWithPolicy(ctx, sourceClient, targetClient, key, "skip_full_only")
}

// scanAllKeys æ‰«ææ‰€æœ‰keyï¼ˆæ”¯æŒé›†ç¾¤æ¨¡å¼ï¼‰
func scanAllKeys(ctx context.Context, client redis.UniversalClient, isCluster bool) (map[string]bool, error) {
	knownKeys := make(map[string]bool)

	if !isCluster {
		// å•æœºæ¨¡å¼ï¼šç›´æ¥SCAN
		var cursor uint64
		for {
			keys, newCursor, err := client.Scan(ctx, cursor, "*", 1000).Result()
			if err != nil {
				return knownKeys, err
			}
			for _, key := range keys {
				knownKeys[key] = true
			}
			cursor = newCursor
			if cursor == 0 {
				break
			}
		}
		return knownKeys, nil
	}

	// é›†ç¾¤æ¨¡å¼ï¼šéå†æ‰€æœ‰masterèŠ‚ç‚¹æ‰«æ
	clusterClient := client.(*redis.ClusterClient)
	var mu sync.Mutex

	err := clusterClient.ForEachMaster(ctx, func(ctx context.Context, node *redis.Client) error {
		var cursor uint64
		for {
			keys, newCursor, err := node.Scan(ctx, cursor, "*", 1000).Result()
			if err != nil {
				return err
			}
			mu.Lock()
			for _, key := range keys {
				knownKeys[key] = true
			}
			mu.Unlock()
			cursor = newCursor
			if cursor == 0 {
				break
			}
		}
		return nil
	})

	return knownKeys, err
}

// doIncrementalSync å¢é‡åŒæ­¥
func doIncrementalSync(ctx context.Context, task *Task, sourceClient, targetClient redis.UniversalClient, sourceIsCluster, targetIsCluster bool, taskLog *logger.TaskLogger) {
	taskLog.Info("Incremental sync mode - monitoring for changes")

	// è®°å½•å·²çŸ¥çš„keyé›†åˆï¼ˆç”¨äºæ£€æµ‹æ–°keyï¼‰
	// ä½¿ç”¨æ–°çš„ scanAllKeys ç¡®ä¿é›†ç¾¤æ¨¡å¼ä¸‹æ‰«ææ‰€æœ‰èŠ‚ç‚¹
	knownKeys, err := scanAllKeys(ctx, sourceClient, sourceIsCluster)
	if err != nil {
		taskLog.Warn("Failed to take initial key snapshot", map[string]interface{}{"error": err.Error()})
	}
	taskLog.Info("Initial key snapshot taken", map[string]interface{}{"known_keys": len(knownKeys)})

	// å®šæœŸæ‰«ææ£€æµ‹æ–°key
	ticker := time.NewTicker(3 * time.Second)
	defer ticker.Stop()

	syncedInIncr := int64(0)
	skippedInIncr := int64(0)

	for {
		select {
		case <-ticker.C:
			tasksMu.RLock()
			status := task.Status
			tasksMu.RUnlock()
			if status != "running" {
				taskLog.Info("Incremental sync stopped", map[string]interface{}{
					"synced_in_incremental":  syncedInIncr,
					"skipped_in_incremental": skippedInIncr,
				})
				return
			}

			// æ‰«ææ‰€æœ‰keyï¼ŒæŸ¥æ‰¾æ–°keyï¼ˆé›†ç¾¤æ¨¡å¼ä¸‹æ‰«ææ‰€æœ‰èŠ‚ç‚¹ï¼‰
			currentKeys, scanErr := scanAllKeys(ctx, sourceClient, sourceIsCluster)
			if scanErr != nil {
				taskLog.Warn("Incremental scan failed", map[string]interface{}{"error": scanErr.Error()})
				continue
			}

			newKeysFound := 0
			newKeysSkipped := 0
			for key := range currentKeys {
				if !knownKeys[key] {
					// æ£€æŸ¥æ˜¯å¦åŒ¹é…è¿‡æ»¤è§„åˆ™
					if !matchKeyFilter(key, task.Options) {
						knownKeys[key] = true
						tasksMu.Lock()
						task.KeysFiltered++
						tasksMu.Unlock()
						continue
					}

					// å‘ç°æ–°keyï¼ŒåŒæ­¥åˆ°ç›®æ ‡ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
					var migrated bool
					var bytes int64
					var reason string
					maxRetries, _, incrIntervalMs := getRetryConfig(task.Options)
					
					for retry := 0; retry < maxRetries; retry++ {
						migrated, bytes, reason = migrateKeyWithPolicy(ctx, sourceClient, targetClient, key, "replace")
						if migrated || reason == "skipped" || reason == "" {
							// æˆåŠŸã€è¢«è·³è¿‡ã€æˆ–æ— é”™è¯¯ï¼Œé€€å‡ºé‡è¯•
							break
						}
						// ç½‘ç»œé”™è¯¯ç­‰ä¸´æ—¶æ€§é”™è¯¯ï¼Œç­‰å¾…åé‡è¯•
						if retry < maxRetries-1 {
							taskLog.Debug("Retrying incremental key sync", map[string]interface{}{
								"key":     key,
								"retry":   retry + 1,
								"reason":  reason,
							})
							time.Sleep(time.Duration((retry+1)*incrIntervalMs) * time.Millisecond) // é€€é¿
						}
					}
					knownKeys[key] = true

					if migrated {
						syncedInIncr++
						newKeysFound++
						tasksMu.Lock()
						task.KeysMigrated++
						task.BytesMigrated += bytes
						task.UpdatedAt = time.Now().Format(time.RFC3339)
						tasksMu.Unlock()

						taskLog.Debug("Incremental key synced", map[string]interface{}{
							"key":   key,
							"bytes": bytes,
						})
					} else if reason == "skipped" {
						// å¢é‡é˜¶æ®µçš„å†²çªè·³è¿‡ä¹Ÿéœ€è¦ç»Ÿè®¡
						skippedInIncr++
						newKeysSkipped++
						tasksMu.Lock()
						task.KeysSkipped++
						task.UpdatedAt = time.Now().Format(time.RFC3339)
						tasksMu.Unlock()
						addErrorKey(task.ID, key, "string", "skipped", "Key already exists in target (incremental)")
					} else if reason != "" {
						tasksMu.Lock()
						task.KeysFailed++
						task.UpdatedAt = time.Now().Format(time.RFC3339)
						tasksMu.Unlock()
						taskLog.Warn("Failed to sync incremental key after retries", map[string]interface{}{
							"key":      key,
							"reason":   reason,
							"retries":  maxRetries,
						})
						addErrorKey(task.ID, key, "string", "failed", reason+" (incremental, after "+fmt.Sprintf("%d", maxRetries)+" retries)")
					}
				}
			}

			// æ›´æ–°æ€»keyæ•°ï¼ˆåªåœ¨æºç«¯keyæ•°å¢åŠ æ—¶æ›´æ–°ï¼‰
			newTotal, _ := getDBSize(ctx, sourceClient, sourceIsCluster)
			tasksMu.Lock()
			if newTotal > task.KeysTotal {
				task.KeysTotal = newTotal
			}
			tasksMu.Unlock()

			if newKeysFound > 0 || newKeysSkipped > 0 {
				taskLog.Info("Incremental sync progress", map[string]interface{}{
					"new_keys_synced":        newKeysFound,
					"new_keys_skipped":       newKeysSkipped,
					"total_synced_in_incr":   syncedInIncr,
					"total_skipped_in_incr":  skippedInIncr,
				})
			}
		}
	}
}

// addErrorKey æ·»åŠ é”™è¯¯Keyè®°å½•
func addErrorKey(taskID, key, keyType, reason, detail string) {
	errorKeyMu.Lock()
	defer errorKeyMu.Unlock()

	if errorKeys[taskID] == nil {
		errorKeys[taskID] = []ErrorKey{}
	}

	// é™åˆ¶æœ€å¤§è®°å½•æ•°
	if len(errorKeys[taskID]) < 10000 {
		errorKeys[taskID] = append(errorKeys[taskID], ErrorKey{
			Key:       key,
			Type:      keyType,
			Reason:    reason,
			Detail:    detail,
			Timestamp: time.Now().Format(time.RFC3339),
		})
	}
}

// errorKeysHandler è·å–é”™è¯¯Keyåˆ—è¡¨
func errorKeysHandler(w http.ResponseWriter, r *http.Request, id string, log *logger.RequestLogger) {
	errorKeyMu.RLock()
	keys := errorKeys[id]
	errorKeyMu.RUnlock()

	tasksMu.RLock()
	task, ok := tasks[id]
	tasksMu.RUnlock()

	if !ok {
		jsonResponse(w, map[string]interface{}{"code": 404, "message": "Task not found"})
		return
	}

	// ç»Ÿè®¡
	stats := map[string]int64{
		"total":      int64(len(keys)),
		"failed":     task.KeysFailed,
		"skipped":    task.KeysSkipped,
		"large_keys": 0,
	}

	// åªè¿”å›å‰100æ¡
	items := keys
	if len(items) > 100 {
		items = items[:100]
	}

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data": map[string]interface{}{
			"stats": stats,
			"items": items,
		},
	})
}

// downloadErrorKeysHandler ä¸‹è½½é”™è¯¯Key CSV
func downloadErrorKeysHandler(w http.ResponseWriter, r *http.Request, id string, log *logger.RequestLogger) {
	errorKeyMu.RLock()
	keys := errorKeys[id]
	errorKeyMu.RUnlock()

	// ç”ŸæˆCSV
	var sb strings.Builder
	sb.WriteString("Key,Type,Reason,Detail,Timestamp\n")
	for _, k := range keys {
		sb.WriteString(fmt.Sprintf("\"%s\",\"%s\",\"%s\",\"%s\",\"%s\"\n",
			strings.ReplaceAll(k.Key, "\"", "\"\""),
			k.Type,
			k.Reason,
			strings.ReplaceAll(k.Detail, "\"", "\"\""),
			k.Timestamp,
		))
	}

	w.Header().Set("Content-Type", "text/csv; charset=utf-8")
	w.Header().Set("Content-Disposition", fmt.Sprintf("attachment; filename=\"error-keys-%s.csv\"", id[:8]))
	w.Write([]byte(sb.String()))

	log.Info("Error keys downloaded", map[string]interface{}{"task_id": id, "count": len(keys)})
}

func systemHandler(w http.ResponseWriter, r *http.Request, log *logger.RequestLogger) {
	running := 0
	tasksMu.RLock()
	for _, t := range tasks {
		if t.Status == "running" {
			running++
		}
	}
	tasksMu.RUnlock()

	log.Debug("System status queried")

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data": map[string]interface{}{
			"status":        "running",
			"worker_count":  4,
			"running_tasks": running,
			"total_tasks":   len(tasks),
			"uptime":        time.Since(startTime).String(),
			"memory_mb":     getMemoryUsage(),
		},
	})
}

// ==================== æµ‹è¯•è¿æ¥ API ====================

func testConnectionHandler(w http.ResponseWriter, r *http.Request, log *logger.RequestLogger) {
	if r.Method != "POST" {
		http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
		return
	}

	var req struct {
		Addrs    []string `json:"addrs"`
		Password string   `json:"password"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		log.Error("Failed to decode request", map[string]interface{}{"error": err.Error()})
		jsonResponse(w, map[string]interface{}{"code": 400, "message": "Invalid request: " + err.Error()})
		return
	}

	// è¿‡æ»¤ç©ºåœ°å€
	var addrs []string
	for _, addr := range req.Addrs {
		if addr != "" {
			addrs = append(addrs, addr)
		}
	}
	if len(addrs) == 0 {
		jsonResponse(w, map[string]interface{}{"code": 400, "message": "è‡³å°‘éœ€è¦ä¸€ä¸ªé›†ç¾¤åœ°å€"})
		return
	}

	log.Info("Testing connection", map[string]interface{}{
		"addrs": addrs,
	})

	startTime := time.Now()
	ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
	defer cancel()

	// å…ˆå°è¯•é›†ç¾¤æ¨¡å¼è¿æ¥
	clusterClient := redis.NewClusterClient(&redis.ClusterOptions{
		Addrs:    addrs,
		Password: req.Password,
	})
	defer clusterClient.Close()

	if err := clusterClient.Ping(ctx).Err(); err == nil {
		// é›†ç¾¤æ¨¡å¼è¿æ¥æˆåŠŸ
		info := getClusterInfo(ctx, clusterClient)
		info["mode"] = "cluster"

		log.Info("Cluster connection successful", map[string]interface{}{
			"mode":       "cluster",
			"node_count": info["node_count"],
			"latency_ms": time.Since(startTime).Milliseconds(),
		})

		jsonResponse(w, map[string]interface{}{
			"code":    0,
			"message": "success",
			"data": map[string]interface{}{
				"success":      true,
				"message":      "é›†ç¾¤è¿æ¥æˆåŠŸ",
				"cluster_info": info,
				"latency_ms":   time.Since(startTime).Milliseconds(),
			},
		})
		return
	}

	// å°è¯•å•æœºæ¨¡å¼è¿æ¥
	standaloneClient := redis.NewClient(&redis.Options{
		Addr:     addrs[0],
		Password: req.Password,
	})
	defer standaloneClient.Close()

	if err := standaloneClient.Ping(ctx).Err(); err != nil {
		log.Error("Connection failed", map[string]interface{}{
			"error":      err.Error(),
			"latency_ms": time.Since(startTime).Milliseconds(),
		})

		jsonResponse(w, map[string]interface{}{
			"code":    0,
			"message": "success",
			"data": map[string]interface{}{
				"success":    false,
				"message":    "è¿æ¥å¤±è´¥: " + err.Error(),
				"latency_ms": time.Since(startTime).Milliseconds(),
			},
		})
		return
	}

	// å•æœºæ¨¡å¼è¿æ¥æˆåŠŸ
	info := getStandaloneInfo(ctx, standaloneClient, addrs[0])
	info["mode"] = "standalone"

	log.Info("Standalone connection successful", map[string]interface{}{
		"mode":       "standalone",
		"latency_ms": time.Since(startTime).Milliseconds(),
	})

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data": map[string]interface{}{
			"success":      true,
			"message":      "å•æœºæ¨¡å¼è¿æ¥æˆåŠŸ",
			"cluster_info": info,
			"latency_ms":   time.Since(startTime).Milliseconds(),
		},
	})
}

func getClusterInfo(ctx context.Context, client *redis.ClusterClient) map[string]interface{} {
	info := map[string]interface{}{
		"nodes":        []map[string]interface{}{},
		"node_count":   0,
		"total_keys":   int64(0),
		"total_memory": int64(0),
		"version":      "unknown",
	}

	var nodes []map[string]interface{}
	var totalKeys int64
	var totalMemory int64
	var version string
	var mu sync.Mutex

	// è·å–é›†ç¾¤èŠ‚ç‚¹ä¿¡æ¯
	client.ForEachMaster(ctx, func(ctx context.Context, node *redis.Client) error {
		nodeInfo := map[string]interface{}{
			"role": "master",
		}

		// è·å–èŠ‚ç‚¹åœ°å€
		opts := node.Options()
		nodeInfo["addr"] = opts.Addr

		// è·å–DBSize
		if dbsize, err := node.DBSize(ctx).Result(); err == nil {
			nodeInfo["keys"] = dbsize
			mu.Lock()
			totalKeys += dbsize
			mu.Unlock()
		}

		// è·å–å†…å­˜ä½¿ç”¨
		if memInfo, err := node.Info(ctx, "memory").Result(); err == nil {
			mem := parseMemoryFromInfo(memInfo)
			nodeInfo["memory"] = mem
			mu.Lock()
			totalMemory += mem
			mu.Unlock()
		}

		// è·å–Redisç‰ˆæœ¬ï¼ˆåªéœ€è·å–ä¸€æ¬¡ï¼‰
		mu.Lock()
		if version == "" {
			if serverInfo, err := node.Info(ctx, "server").Result(); err == nil {
				version = parseVersionFromInfo(serverInfo)
			}
		}
		mu.Unlock()

		mu.Lock()
		nodes = append(nodes, nodeInfo)
		mu.Unlock()
		return nil
	})

	info["nodes"] = nodes
	info["node_count"] = len(nodes)
	info["total_keys"] = totalKeys
	info["total_memory"] = totalMemory
	info["version"] = version

	return info
}

func getStandaloneInfo(ctx context.Context, client *redis.Client, addr string) map[string]interface{} {
	info := map[string]interface{}{
		"node_count":   1,
		"total_keys":   int64(0),
		"total_memory": int64(0),
		"version":      "unknown",
	}

	nodeInfo := map[string]interface{}{
		"addr": addr,
		"role": "master",
	}

	// è·å–DBSize
	if dbsize, err := client.DBSize(ctx).Result(); err == nil {
		nodeInfo["keys"] = dbsize
		info["total_keys"] = dbsize
	}

	// è·å–å†…å­˜å’Œç‰ˆæœ¬ä¿¡æ¯
	if serverInfo, err := client.Info(ctx, "server").Result(); err == nil {
		info["version"] = parseVersionFromInfo(serverInfo)
	}
	if memInfo, err := client.Info(ctx, "memory").Result(); err == nil {
		mem := parseMemoryFromInfo(memInfo)
		nodeInfo["memory"] = mem
		info["total_memory"] = mem
	}

	info["nodes"] = []map[string]interface{}{nodeInfo}
	return info
}

func parseVersionFromInfo(info string) string {
	for _, line := range strings.Split(info, "\n") {
		line = strings.TrimSpace(line)
		if strings.HasPrefix(line, "redis_version:") {
			return strings.TrimPrefix(line, "redis_version:")
		}
	}
	return "unknown"
}

func parseMemoryFromInfo(info string) int64 {
	for _, line := range strings.Split(info, "\n") {
		line = strings.TrimSpace(line)
		if strings.HasPrefix(line, "used_memory:") {
			memStr := strings.TrimPrefix(line, "used_memory:")
			mem, _ := strconv.ParseInt(memStr, 10, 64)
			return mem
		}
	}
	return 0
}

// analyzeClusterHandler åˆ†æé›†ç¾¤è¯¦ç»†ä¿¡æ¯
func analyzeClusterHandler(w http.ResponseWriter, r *http.Request, log *logger.RequestLogger) {
	if r.Method != "POST" {
		http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
		return
	}

	var req struct {
		Addrs    []string `json:"addrs"`
		Password string   `json:"password"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		jsonResponse(w, map[string]interface{}{"code": 400, "message": "Invalid request"})
		return
	}

	ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
	defer cancel()

	info, err := analyzeCluster(ctx, req.Addrs, req.Password)
	if err != nil {
		log.Error("Failed to analyze cluster", map[string]interface{}{"error": err.Error()})
		jsonResponse(w, map[string]interface{}{"code": 500, "message": err.Error()})
		return
	}

	log.Info("Cluster analyzed", map[string]interface{}{
		"total_keys": info.TotalKeys,
		"is_cluster": info.IsCluster,
	})

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data":    info,
	})
}

// analyzeCluster åˆ†æé›†ç¾¤è¯¦ç»†ä¿¡æ¯
func analyzeCluster(ctx context.Context, addrs []string, password string) (*ClusterInfo, error) {
	info := &ClusterInfo{
		Addrs: addrs,
	}

	// å°è¯•é›†ç¾¤æ¨¡å¼
	clusterClient := redis.NewClusterClient(&redis.ClusterOptions{
		Addrs:    addrs,
		Password: password,
	})

	if err := clusterClient.Ping(ctx).Err(); err == nil {
		info.IsCluster = true
		defer clusterClient.Close()

		var mu sync.Mutex
		var totalKeys int64
		var totalMemory int64
		var maxMemory int64
		var masterCount int
		var connectedClients int
		var instantaneousOPS int64
		var maxClients int

		clusterClient.ForEachMaster(ctx, func(ctx context.Context, node *redis.Client) error {
			masterCount++

			// DBSize
			if dbsize, err := node.DBSize(ctx).Result(); err == nil {
				mu.Lock()
				totalKeys += dbsize
				mu.Unlock()
			}

			// Memory info
			if memInfo, err := node.Info(ctx, "memory").Result(); err == nil {
				mem := parseMemoryFromInfo(memInfo)
				maxMem := parseMaxMemoryFromInfo(memInfo)
				mu.Lock()
				totalMemory += mem
				if maxMem > maxMemory {
					maxMemory = maxMem
				}
				mu.Unlock()
			}

			// Stats info
			if statsInfo, err := node.Info(ctx, "stats").Result(); err == nil {
				ops := parseOPSFromInfo(statsInfo)
				mu.Lock()
				instantaneousOPS += ops
				mu.Unlock()
			}

			// Clients info
			if clientInfo, err := node.Info(ctx, "clients").Result(); err == nil {
				clients := parseConnectedClientsFromInfo(clientInfo)
				mu.Lock()
				connectedClients += clients
				mu.Unlock()
			}

			// Server info (version, maxclients)
			if serverInfo, err := node.Info(ctx, "server").Result(); err == nil {
				mu.Lock()
				if info.Version == "" {
					info.Version = parseVersionFromInfo(serverInfo)
				}
				mu.Unlock()
			}

			// Config maxclients
			if result, err := node.ConfigGet(ctx, "maxclients").Result(); err == nil && len(result) >= 2 {
				if mcStr, ok := result[1].(string); ok {
					mc, _ := strconv.Atoi(mcStr)
					mu.Lock()
					if mc > maxClients {
						maxClients = mc
					}
					mu.Unlock()
				}
			}

			return nil
		})

		info.MasterCount = masterCount
		info.TotalKeys = totalKeys
		info.UsedMemory = totalMemory
		info.UsedMemoryHuman = formatBytes(totalMemory)
		info.MaxMemory = maxMemory
		info.MaxClients = maxClients
		info.ConnectedClients = connectedClients
		info.InstantaneousOPS = instantaneousOPS

		// ä¼°ç®—å¹³å‡keyå¤§å°
		if totalKeys > 0 {
			info.AvgKeySize = totalMemory / totalKeys
		}

		return info, nil
	}
	clusterClient.Close()

	// å°è¯•å•æœºæ¨¡å¼
	standaloneClient := redis.NewClient(&redis.Options{
		Addr:     addrs[0],
		Password: password,
	})
	defer standaloneClient.Close()

	if err := standaloneClient.Ping(ctx).Err(); err != nil {
		return nil, err
	}

	info.IsCluster = false
	info.MasterCount = 1

	if dbsize, err := standaloneClient.DBSize(ctx).Result(); err == nil {
		info.TotalKeys = dbsize
	}

	if memInfo, err := standaloneClient.Info(ctx, "memory").Result(); err == nil {
		info.UsedMemory = parseMemoryFromInfo(memInfo)
		info.UsedMemoryHuman = formatBytes(info.UsedMemory)
		info.MaxMemory = parseMaxMemoryFromInfo(memInfo)
	}

	if statsInfo, err := standaloneClient.Info(ctx, "stats").Result(); err == nil {
		info.InstantaneousOPS = parseOPSFromInfo(statsInfo)
	}

	if clientInfo, err := standaloneClient.Info(ctx, "clients").Result(); err == nil {
		info.ConnectedClients = parseConnectedClientsFromInfo(clientInfo)
	}

	if serverInfo, err := standaloneClient.Info(ctx, "server").Result(); err == nil {
		info.Version = parseVersionFromInfo(serverInfo)
	}

	if result, err := standaloneClient.ConfigGet(ctx, "maxclients").Result(); err == nil && len(result) >= 2 {
		if mcStr, ok := result[1].(string); ok {
			info.MaxClients, _ = strconv.Atoi(mcStr)
		}
	}

	if info.TotalKeys > 0 {
		info.AvgKeySize = info.UsedMemory / info.TotalKeys
	}

	return info, nil
}

// recommendConfigHandler æ¨èé…ç½®
func recommendConfigHandler(w http.ResponseWriter, r *http.Request, log *logger.RequestLogger) {
	if r.Method != "POST" {
		http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
		return
	}

	var req struct {
		SourceCluster struct {
			Addrs    []string `json:"addrs"`
			Password string   `json:"password"`
		} `json:"source_cluster"`
		TargetCluster struct {
			Addrs    []string `json:"addrs"`
			Password string   `json:"password"`
		} `json:"target_cluster"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		jsonResponse(w, map[string]interface{}{"code": 400, "message": "Invalid request"})
		return
	}

	ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
	defer cancel()

	// åˆ†ææºç«¯é›†ç¾¤
	sourceInfo, err := analyzeCluster(ctx, req.SourceCluster.Addrs, req.SourceCluster.Password)
	if err != nil {
		jsonResponse(w, map[string]interface{}{"code": 500, "message": "åˆ†ææºç«¯é›†ç¾¤å¤±è´¥: " + err.Error()})
		return
	}

	// åˆ†æç›®æ ‡ç«¯é›†ç¾¤
	targetInfo, err := analyzeCluster(ctx, req.TargetCluster.Addrs, req.TargetCluster.Password)
	if err != nil {
		jsonResponse(w, map[string]interface{}{"code": 500, "message": "åˆ†æç›®æ ‡ç«¯é›†ç¾¤å¤±è´¥: " + err.Error()})
		return
	}

	// ç”Ÿæˆæ¨èé…ç½®
	config := generateRecommendedConfig(sourceInfo, targetInfo)

	log.Info("Config recommended", map[string]interface{}{
		"worker_count":    config.WorkerCount,
		"estimated_speed": config.EstimatedSpeed,
	})

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data": map[string]interface{}{
			"source_info": sourceInfo,
			"target_info": targetInfo,
			"recommended": config,
		},
	})
}

// generateRecommendedConfig ç”Ÿæˆæ¨èé…ç½®
func generateRecommendedConfig(source, target *ClusterInfo) *RecommendedConfig {
	config := &RecommendedConfig{
		ScanBatchSize:     1000,
		LargeKeyThreshold: 10 * 1024 * 1024, // 10MB
	}

	var reasons []string

	// 1. è®¡ç®—Workeræ•°é‡
	// åŸºäºå¤šä¸ªå› ç´ ï¼šæºç«¯è´Ÿè½½ã€è¿æ¥æ•°é™åˆ¶ã€CPUæ ¸å¿ƒæ•°
	cpuCores := runtime.NumCPU()
	maxWorkersByCPU := cpuCores * 4 // æ¯æ ¸4ä¸ªworker

	// åŸºäºè¿æ¥æ•°é™åˆ¶
	sourceMaxConns := source.MaxClients - source.ConnectedClients
	targetMaxConns := target.MaxClients - target.ConnectedClients
	if sourceMaxConns < 100 {
		sourceMaxConns = 100
	}
	if targetMaxConns < 100 {
		targetMaxConns = 100
	}
	maxWorkersByConn := min(sourceMaxConns/3, targetMaxConns/3) // æ¯workeréœ€è¦çº¦3ä¸ªè¿æ¥

	// åŸºäºæºç«¯å½“å‰è´Ÿè½½
	var maxWorkersByLoad int
	if source.InstantaneousOPS < 1000 {
		// ä½è´Ÿè½½ï¼Œå¯ä»¥æ¿€è¿›é…ç½®
		maxWorkersByLoad = 100
		reasons = append(reasons, "æºç«¯è´Ÿè½½è¾ƒä½(OPS<1000)ï¼Œå¯ä½¿ç”¨è¾ƒå¤šWorker")
	} else if source.InstantaneousOPS < 10000 {
		// ä¸­ç­‰è´Ÿè½½
		maxWorkersByLoad = 50
		reasons = append(reasons, "æºç«¯è´Ÿè½½ä¸­ç­‰ï¼ŒWorkeræ•°é‡é€‚ä¸­")
	} else {
		// é«˜è´Ÿè½½ï¼Œä¿å®ˆé…ç½®
		maxWorkersByLoad = 20
		reasons = append(reasons, "æºç«¯è´Ÿè½½è¾ƒé«˜ï¼ŒWorkeræ•°é‡ä¿å®ˆè®¾ç½®")
	}

	// å–æœ€å°å€¼
	config.WorkerCount = min(maxWorkersByCPU, min(maxWorkersByConn, maxWorkersByLoad))
	if config.WorkerCount < 4 {
		config.WorkerCount = 4
	}
	if config.WorkerCount > 100 {
		config.WorkerCount = 100
	}

	// 2. è®¡ç®—QPSé™åˆ¶
	// æºç«¯ï¼šé¢„ç•™70%ç»™ä¸šåŠ¡ï¼Œè¿ç§»ä½¿ç”¨30%
	if source.InstantaneousOPS < 100 {
		// å‡ ä¹æ— ä¸šåŠ¡ï¼Œä¸é™åˆ¶
		config.SourceQPS = 0
		reasons = append(reasons, "æºç«¯å‡ ä¹æ— ä¸šåŠ¡è´Ÿè½½ï¼Œä¸é™åˆ¶QPS")
	} else {
		// ä¼°ç®—æœ€å¤§å®¹é‡ï¼ˆå‡è®¾å½“å‰æ˜¯ä¸šåŠ¡è´Ÿè½½çš„50%ï¼‰
		estimatedMaxOPS := source.InstantaneousOPS * 2
		if estimatedMaxOPS < 50000 {
			estimatedMaxOPS = 50000 // æœ€ä½å‡è®¾5ä¸‡
		}
		config.SourceQPS = int(estimatedMaxOPS * 30 / 100) // ä½¿ç”¨30%
		reasons = append(reasons, fmt.Sprintf("æºç«¯QPSé™åˆ¶ä¸ºé¢„ä¼°å®¹é‡çš„30%%(%d)", config.SourceQPS))
	}

	// ç›®æ ‡ç«¯ï¼šé€šå¸¸å¯ä»¥æ›´æ¿€è¿›
	if target.InstantaneousOPS < 100 {
		config.TargetQPS = 0
		reasons = append(reasons, "ç›®æ ‡ç«¯å‡ ä¹æ— è´Ÿè½½ï¼Œä¸é™åˆ¶QPS")
	} else {
		estimatedMaxOPS := target.InstantaneousOPS * 2
		if estimatedMaxOPS < 50000 {
			estimatedMaxOPS = 50000
		}
		config.TargetQPS = int(estimatedMaxOPS * 50 / 100) // ä½¿ç”¨50%
	}

	// 3. è®¡ç®—è¿æ¥æ•°
	// æ¯ä¸ªWorkeréœ€è¦çº¦2-3ä¸ªè¿æ¥
	config.SourceConnections = config.WorkerCount * 3
	config.TargetConnections = config.WorkerCount * 3

	// ç¡®ä¿ä¸è¶…è¿‡å¯ç”¨è¿æ¥æ•°çš„50%
	if config.SourceConnections > sourceMaxConns/2 {
		config.SourceConnections = sourceMaxConns / 2
	}
	if config.TargetConnections > targetMaxConns/2 {
		config.TargetConnections = targetMaxConns / 2
	}

	// æœ€å°è¿æ¥æ•°
	if config.SourceConnections < 10 {
		config.SourceConnections = 10
	}
	if config.TargetConnections < 10 {
		config.TargetConnections = 10
	}

	// 4. ä¼°ç®—è¿ç§»é€Ÿåº¦å’Œæ—¶é—´
	// åŸºäºkeyå¤§å°ä¼°ç®—å•workeråå
	var singleWorkerSpeed int64
	if source.AvgKeySize < 1024 { // < 1KB
		singleWorkerSpeed = 500
	} else if source.AvgKeySize < 10*1024 { // < 10KB
		singleWorkerSpeed = 200
	} else if source.AvgKeySize < 100*1024 { // < 100KB
		singleWorkerSpeed = 50
	} else { // >= 100KB
		singleWorkerSpeed = 10
	}

	config.EstimatedSpeed = singleWorkerSpeed * int64(config.WorkerCount)

	// QPSé™åˆ¶å¯èƒ½æˆä¸ºç“¶é¢ˆ
	if config.SourceQPS > 0 && int64(config.SourceQPS) < config.EstimatedSpeed*4 {
		// æ¯ä¸ªkeyéœ€è¦çº¦4æ¬¡æ“ä½œï¼ŒQPSé™åˆ¶å¯èƒ½å½±å“é€Ÿåº¦
		config.EstimatedSpeed = int64(config.SourceQPS) / 4
	}

	// ä¼°ç®—æ—¶é—´
	if config.EstimatedSpeed > 0 && source.TotalKeys > 0 {
		seconds := source.TotalKeys / config.EstimatedSpeed
		config.EstimatedTime = formatDuration(seconds)
	} else {
		config.EstimatedTime = "æ— æ³•ä¼°ç®—"
	}

	// 5. å¤§Keyé˜ˆå€¼
	if source.AvgKeySize > 1024*1024 { // å¹³å‡å¤§äº1MB
		config.LargeKeyThreshold = 5 * 1024 * 1024 // 5MB
		reasons = append(reasons, "æ£€æµ‹åˆ°è¾ƒå¤§çš„å¹³å‡Keyå¤§å°ï¼Œè°ƒä½å¤§Keyé˜ˆå€¼")
	}

	config.Reason = strings.Join(reasons, "ï¼›")

	return config
}

// è¾…åŠ©å‡½æ•°
func parseMaxMemoryFromInfo(info string) int64 {
	for _, line := range strings.Split(info, "\n") {
		line = strings.TrimSpace(line)
		if strings.HasPrefix(line, "maxmemory:") {
			memStr := strings.TrimPrefix(line, "maxmemory:")
			mem, _ := strconv.ParseInt(memStr, 10, 64)
			return mem
		}
	}
	return 0
}

func parseOPSFromInfo(info string) int64 {
	for _, line := range strings.Split(info, "\n") {
		line = strings.TrimSpace(line)
		if strings.HasPrefix(line, "instantaneous_ops_per_sec:") {
			opsStr := strings.TrimPrefix(line, "instantaneous_ops_per_sec:")
			ops, _ := strconv.ParseInt(opsStr, 10, 64)
			return ops
		}
	}
	return 0
}

func parseConnectedClientsFromInfo(info string) int {
	for _, line := range strings.Split(info, "\n") {
		line = strings.TrimSpace(line)
		if strings.HasPrefix(line, "connected_clients:") {
			clientsStr := strings.TrimPrefix(line, "connected_clients:")
			clients, _ := strconv.Atoi(clientsStr)
			return clients
		}
	}
	return 0
}

func formatBytes(bytes int64) string {
	if bytes < 1024 {
		return fmt.Sprintf("%d B", bytes)
	} else if bytes < 1024*1024 {
		return fmt.Sprintf("%.2f KB", float64(bytes)/1024)
	} else if bytes < 1024*1024*1024 {
		return fmt.Sprintf("%.2f MB", float64(bytes)/(1024*1024))
	}
	return fmt.Sprintf("%.2f GB", float64(bytes)/(1024*1024*1024))
}

func formatDuration(seconds int64) string {
	if seconds < 60 {
		return fmt.Sprintf("%dç§’", seconds)
	} else if seconds < 3600 {
		return fmt.Sprintf("%dåˆ†%dç§’", seconds/60, seconds%60)
	} else if seconds < 86400 {
		return fmt.Sprintf("%då°æ—¶%dåˆ†", seconds/3600, (seconds%3600)/60)
	}
	return fmt.Sprintf("%då¤©%då°æ—¶", seconds/86400, (seconds%86400)/3600)
}

func min(a, b int) int {
	if a < b {
		return a
	}
	return b
}
func templatesHandler(w http.ResponseWriter, r *http.Request, log *logger.RequestLogger) {
	switch r.Method {
	case "GET":
		listTemplates(w, r, log)
	case "POST":
		createTemplate(w, r, log)
	default:
		http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
	}
}

// listTemplates è·å–æ¨¡æ¿åˆ—è¡¨
func listTemplates(w http.ResponseWriter, r *http.Request, log *logger.RequestLogger) {
	templateMu.RLock()
	defer templateMu.RUnlock()

	items := make([]*TaskTemplate, 0, len(templates))
	for _, t := range templates {
		items = append(items, t)
	}

	// æŒ‰åˆ›å»ºæ—¶é—´æ’åº
	sort.Slice(items, func(i, j int) bool {
		return items[i].CreatedAt > items[j].CreatedAt
	})

	log.Debug("Templates listed", map[string]interface{}{"count": len(items)})

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data": map[string]interface{}{
			"items": items,
			"total": len(items),
		},
	})
}

// createTemplate åˆ›å»ºæ¨¡æ¿
func createTemplate(w http.ResponseWriter, r *http.Request, log *logger.RequestLogger) {
	var req struct {
		Name          string `json:"name"`
		Description   string `json:"description"`
		SourceCluster struct {
			Addrs    []string `json:"addrs"`
			Password string   `json:"password"`
		} `json:"source_cluster"`
		TargetCluster struct {
			Addrs    []string `json:"addrs"`
			Password string   `json:"password"`
		} `json:"target_cluster"`
		MigrationMode string       `json:"migration_mode"`
		Options       *TaskOptions `json:"options"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		log.Error("Failed to decode request", map[string]interface{}{"error": err.Error()})
		jsonResponse(w, map[string]interface{}{"code": 400, "message": err.Error()})
		return
	}

	if req.Name == "" {
		jsonResponse(w, map[string]interface{}{"code": 400, "message": "æ¨¡æ¿åç§°ä¸èƒ½ä¸ºç©º"})
		return
	}

	template := &TaskTemplate{
		ID:             uuid.New().String(),
		Name:           req.Name,
		Description:    req.Description,
		SourceCluster:  strings.Join(req.SourceCluster.Addrs, ","),
		TargetCluster:  strings.Join(req.TargetCluster.Addrs, ","),
		SourcePassword: req.SourceCluster.Password,
		TargetPassword: req.TargetCluster.Password,
		MigrationMode:  req.MigrationMode,
		Options:        req.Options,
		CreatedAt:      time.Now().Format(time.RFC3339),
		UpdatedAt:      time.Now().Format(time.RFC3339),
	}

	templateMu.Lock()
	templates[template.ID] = template
	templateMu.Unlock()

	log.Info("Template created", map[string]interface{}{
		"template_id":   template.ID,
		"template_name": template.Name,
	})

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data":    map[string]string{"template_id": template.ID},
	})
}

// templateHandler å¤„ç†å•ä¸ªæ¨¡æ¿è¯·æ±‚
func templateHandler(w http.ResponseWriter, r *http.Request, log *logger.RequestLogger) {
	path := strings.TrimPrefix(r.URL.Path, "/api/v1/templates/")
	parts := strings.Split(path, "/")

	if len(parts) == 0 || parts[0] == "" {
		http.NotFound(w, r)
		return
	}

	id := parts[0]
	action := ""
	if len(parts) > 1 {
		action = parts[1]
	}

	switch r.Method {
	case "GET":
		getTemplate(w, r, id, log)
	case "PUT":
		updateTemplate(w, r, id, log)
	case "DELETE":
		deleteTemplate(w, r, id, log)
	case "POST":
		if action == "create-task" {
			createTaskFromTemplate(w, r, id, log)
		} else {
			http.NotFound(w, r)
		}
	default:
		http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
	}
}

// getTemplate è·å–æ¨¡æ¿è¯¦æƒ…
func getTemplate(w http.ResponseWriter, r *http.Request, id string, log *logger.RequestLogger) {
	templateMu.RLock()
	template, ok := templates[id]
	templateMu.RUnlock()

	if !ok {
		jsonResponse(w, map[string]interface{}{"code": 404, "message": "Template not found"})
		return
	}

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data":    template,
	})
}

// updateTemplate æ›´æ–°æ¨¡æ¿
func updateTemplate(w http.ResponseWriter, r *http.Request, id string, log *logger.RequestLogger) {
	templateMu.Lock()
	template, ok := templates[id]
	if !ok {
		templateMu.Unlock()
		jsonResponse(w, map[string]interface{}{"code": 404, "message": "Template not found"})
		return
	}

	var req struct {
		Name          string `json:"name"`
		Description   string `json:"description"`
		SourceCluster struct {
			Addrs    []string `json:"addrs"`
			Password string   `json:"password"`
		} `json:"source_cluster"`
		TargetCluster struct {
			Addrs    []string `json:"addrs"`
			Password string   `json:"password"`
		} `json:"target_cluster"`
		MigrationMode string       `json:"migration_mode"`
		Options       *TaskOptions `json:"options"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		templateMu.Unlock()
		jsonResponse(w, map[string]interface{}{"code": 400, "message": err.Error()})
		return
	}

	if req.Name != "" {
		template.Name = req.Name
	}
	if req.Description != "" {
		template.Description = req.Description
	}
	if len(req.SourceCluster.Addrs) > 0 {
		template.SourceCluster = strings.Join(req.SourceCluster.Addrs, ",")
		template.SourcePassword = req.SourceCluster.Password
	}
	if len(req.TargetCluster.Addrs) > 0 {
		template.TargetCluster = strings.Join(req.TargetCluster.Addrs, ",")
		template.TargetPassword = req.TargetCluster.Password
	}
	if req.MigrationMode != "" {
		template.MigrationMode = req.MigrationMode
	}
	if req.Options != nil {
		template.Options = req.Options
	}
	template.UpdatedAt = time.Now().Format(time.RFC3339)
	templateMu.Unlock()

	log.Info("Template updated", map[string]interface{}{"template_id": id})

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
	})
}

// deleteTemplate åˆ é™¤æ¨¡æ¿
func deleteTemplate(w http.ResponseWriter, r *http.Request, id string, log *logger.RequestLogger) {
	templateMu.Lock()
	_, ok := templates[id]
	if !ok {
		templateMu.Unlock()
		jsonResponse(w, map[string]interface{}{"code": 404, "message": "Template not found"})
		return
	}
	delete(templates, id)
	templateMu.Unlock()

	log.Info("Template deleted", map[string]interface{}{"template_id": id})

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
	})
}

// createTaskFromTemplate ä»æ¨¡æ¿åˆ›å»ºä»»åŠ¡
func createTaskFromTemplate(w http.ResponseWriter, r *http.Request, id string, log *logger.RequestLogger) {
	templateMu.RLock()
	template, ok := templates[id]
	templateMu.RUnlock()

	if !ok {
		jsonResponse(w, map[string]interface{}{"code": 404, "message": "Template not found"})
		return
	}

	// å¯é€‰ï¼šå…è®¸è¦†ç›–éƒ¨åˆ†å‚æ•°
	var req struct {
		Name string `json:"name"`
	}
	json.NewDecoder(r.Body).Decode(&req)

	taskName := req.Name
	if taskName == "" {
		taskName = template.Name + "-" + time.Now().Format("0102-1504")
	}

	mode := template.MigrationMode
	if mode == "" {
		mode = "full_and_incremental"
	}

	options := template.Options
	if options == nil {
		options = &TaskOptions{
			WorkerCount:       4,
			ScanBatchSize:     1000,
			ConflictPolicy:    "skip_full_only",
			LargeKeyThreshold: 10485760,
			EnableCompression: true,
			KeyFilter:         &KeyFilter{Mode: "all"},
		}
	} else if options.KeyFilter == nil || options.KeyFilter.Mode == "" {
		if options.KeyFilter == nil {
			options.KeyFilter = &KeyFilter{Mode: "all"}
		} else {
			options.KeyFilter.Mode = "all"
		}
	}

	task := &Task{
		ID:             uuid.New().String(),
		Name:           taskName,
		Status:         "pending",
		Progress:       0,
		SourceCluster:  template.SourceCluster,
		TargetCluster:  template.TargetCluster,
		SourcePassword: template.SourcePassword,
		TargetPassword: template.TargetPassword,
		MigrationMode:  mode,
		CreatedAt:      time.Now().Format(time.RFC3339),
		UpdatedAt:      time.Now().Format(time.RFC3339),
		Phase:          "full",
		Options:        options,
	}

	tasksMu.Lock()
	tasks[task.ID] = task
	tasksMu.Unlock()

	log.Info("Task created from template", map[string]interface{}{
		"task_id":     task.ID,
		"template_id": id,
		"task_name":   taskName,
	})

	logger.WithTask(task.ID).Info("Task created from template", map[string]interface{}{
		"name":        task.Name,
		"template_id": id,
		"source":      task.SourceCluster,
		"target":      task.TargetCluster,
	})

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data":    map[string]string{"task_id": task.ID},
	})
}
