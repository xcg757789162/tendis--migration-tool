package main

import (
	"context"
	"encoding/json"
	"fmt"
	"net/http"
	"runtime"
	"sort"
	"strconv"
	"strings"
	"sync"
	"sync/atomic"
	"time"

	"github.com/go-redis/redis/v8"
	"github.com/google/uuid"
	"tendis-migrate/pkg/logger"
)

type Task struct {
	ID             string  `json:"id"`
	Name           string  `json:"name"`
	Status         string  `json:"status"`
	Progress       float64 `json:"progress"`
	SourceCluster  string  `json:"source_cluster"`
	TargetCluster  string  `json:"target_cluster"`
	SourcePassword string  `json:"-"`
	TargetPassword string  `json:"-"`
	MigrationMode  string  `json:"migration_mode"` // full_only, full_and_incremental
	CreatedAt      string  `json:"created_at"`
	UpdatedAt      string  `json:"updated_at"`
	FullStartAt    string  `json:"full_start_at,omitempty"`    // å…¨é‡è¿ç§»å¼€å§‹æ—¶é—´
	IncrStartAt    string  `json:"incr_start_at,omitempty"`    // å¢é‡è¿ç§»å¼€å§‹æ—¶é—´
	StartedAt      string  `json:"started_at,omitempty"`       // ä»»åŠ¡å¼€å§‹æ—¶é—´ï¼ˆç”¨äºè®¡ç®—å·²è€—æ—¶é—´ï¼‰
	KeysTotal      int64   `json:"keys_total"`
	KeysMigrated   int64   `json:"keys_migrated"`
	KeysFailed     int64   `json:"keys_failed"`
	KeysSkipped    int64   `json:"keys_skipped"`
	KeysFiltered   int64   `json:"keys_filtered"`
	BytesMigrated  int64   `json:"bytes_migrated"`
	BytesTotal     int64   `json:"bytes_total"`
	Speed          int64   `json:"speed"`
	Phase          string  `json:"phase"` // full, incremental, completed
	// é…ç½®é€‰é¡¹
	Options *TaskOptions `json:"options,omitempty"`
}

// TaskTemplate ä»»åŠ¡æ¨¡æ¿
type TaskTemplate struct {
	ID            string       `json:"id"`
	Name          string       `json:"name"`
	Description   string       `json:"description"`
	SourceCluster string       `json:"source_cluster"`
	TargetCluster string       `json:"target_cluster"`
	SourcePassword string      `json:"source_password,omitempty"`
	TargetPassword string      `json:"target_password,omitempty"`
	MigrationMode string       `json:"migration_mode"`
	Options       *TaskOptions `json:"options,omitempty"`
	CreatedAt     string       `json:"created_at"`
	UpdatedAt     string       `json:"updated_at"`
}

// TaskOptions ä»»åŠ¡é…ç½®é€‰é¡¹
type TaskOptions struct {
	WorkerCount       int        `json:"worker_count"`
	ScanBatchSize     int        `json:"scan_batch_size"`
	ConflictPolicy    string     `json:"conflict_policy"`     // skip, replace, error, skip_full_only
	LargeKeyThreshold int64      `json:"large_key_threshold"`
	EnableCompression bool       `json:"enable_compression"`
	SkipFullSync      bool       `json:"skip_full_sync"`
	SkipIncremental   bool       `json:"skip_incremental"`
	KeyFilter         *KeyFilter `json:"key_filter,omitempty"`
	RateLimit         *RateLimit `json:"rate_limit,omitempty"`
}

// RateLimit é™é€Ÿé…ç½®
type RateLimit struct {
	SourceQPS         int `json:"source_qps"`          // æºç«¯QPSé™åˆ¶ï¼Œ0è¡¨ç¤ºä¸é™åˆ¶
	TargetQPS         int `json:"target_qps"`          // ç›®æ ‡ç«¯QPSé™åˆ¶ï¼Œ0è¡¨ç¤ºä¸é™åˆ¶
	SourceConnections int `json:"source_connections"`  // æºç«¯è¿æ¥æ•°
	TargetConnections int `json:"target_connections"`  // ç›®æ ‡ç«¯è¿æ¥æ•°
}

// ClusterInfo é›†ç¾¤ä¿¡æ¯ï¼ˆç”¨äºæ¨èé…ç½®ï¼‰
type ClusterInfo struct {
	Addrs            []string `json:"addrs"`
	IsCluster        bool     `json:"is_cluster"`
	MasterCount      int      `json:"master_count"`
	TotalKeys        int64    `json:"total_keys"`
	UsedMemory       int64    `json:"used_memory"`
	UsedMemoryHuman  string   `json:"used_memory_human"`
	MaxMemory        int64    `json:"max_memory"`
	MaxClients       int      `json:"max_clients"`
	ConnectedClients int      `json:"connected_clients"`
	InstantaneousOPS int64    `json:"instantaneous_ops"`
	Version          string   `json:"version"`
	AvgKeySize       int64    `json:"avg_key_size"`       // ä¼°ç®—çš„å¹³å‡keyå¤§å°
	LargeKeyCount    int64    `json:"large_key_count"`    // å¤§keyæ•°é‡ä¼°ç®—
}

// RecommendedConfig æ¨èé…ç½®
type RecommendedConfig struct {
	WorkerCount       int    `json:"worker_count"`
	ScanBatchSize     int    `json:"scan_batch_size"`
	SourceQPS         int    `json:"source_qps"`
	TargetQPS         int    `json:"target_qps"`
	SourceConnections int    `json:"source_connections"`
	TargetConnections int    `json:"target_connections"`
	LargeKeyThreshold int64  `json:"large_key_threshold"`
	EstimatedTime     string `json:"estimated_time"`      // é¢„è®¡è€—æ—¶
	EstimatedSpeed    int64  `json:"estimated_speed"`     // é¢„è®¡é€Ÿåº¦ keys/s
	Reason            string `json:"reason"`              // æ¨èç†ç”±
}

// KeyFilter Keyè¿‡æ»¤é…ç½®
type KeyFilter struct {
	Mode            string   `json:"mode"` // all, prefix, pattern
	Prefixes        []string `json:"prefixes"`
	ExcludePrefixes []string `json:"exclude_prefixes"`
	Patterns        []string `json:"patterns"`
}

// ErrorKey è®°å½•è¿ç§»å¤±è´¥æˆ–è·³è¿‡çš„Key
type ErrorKey struct {
	Key       string `json:"key"`
	Type      string `json:"type"`
	Reason    string `json:"reason"`
	Detail    string `json:"detail"`
	Timestamp string `json:"timestamp"`
}

var (
	tasks      = make(map[string]*Task)
	tasksMu    sync.RWMutex
	templates  = make(map[string]*TaskTemplate)
	templateMu sync.RWMutex
	errorKeys  = make(map[string][]ErrorKey) // taskID -> error keys
	errorKeyMu sync.RWMutex
	startTime  time.Time
)

func main() {
	startTime = time.Now()
	
	// åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
	if err := logger.Init("./logs", logger.DEBUG); err != nil {
		fmt.Printf("Failed to init logger: %v\n", err)
	}
	
	logger.Info("ğŸš€ Tendis Migration Tool starting", map[string]interface{}{
		"port":    8088,
		"version": "1.0.0",
		"pid":     fmt.Sprintf("%d", getPID()),
	})

	initDemoData()

	// ä½¿ç”¨è‡ªå®šä¹‰ handler ç»Ÿä¸€å¤„ç†
	server := &http.Server{
		Addr:         ":8088",
		Handler:      http.HandlerFunc(mainHandler),
		ReadTimeout:  30 * time.Second,
		WriteTimeout: 30 * time.Second,
	}

	logger.Info("Server listening on http://localhost:8088")
	if err := server.ListenAndServe(); err != nil {
		logger.Fatal("Server failed to start", map[string]interface{}{"error": err.Error()})
	}
}

func mainHandler(w http.ResponseWriter, r *http.Request) {
	// ç”Ÿæˆè¯·æ±‚ID
	requestID := uuid.New().String()
	startTime := time.Now()
	
	// CORS
	w.Header().Set("Access-Control-Allow-Origin", "*")
	w.Header().Set("Access-Control-Allow-Methods", "GET, POST, PUT, DELETE, OPTIONS")
	w.Header().Set("Access-Control-Allow-Headers", "*")
	w.Header().Set("X-Request-ID", requestID)
	
	if r.Method == "OPTIONS" {
		w.WriteHeader(http.StatusOK)
		return
	}

	path := r.URL.Path
	log := logger.WithRequest(requestID)
	
	// è®°å½•è¯·æ±‚æ—¥å¿—
	log.Info("Request started", map[string]interface{}{
		"method": r.Method,
		"path":   path,
		"query":  r.URL.RawQuery,
		"remote": r.RemoteAddr,
		"ua":     r.UserAgent(),
	})

	// åŒ…è£… ResponseWriter ä»¥æ•è·çŠ¶æ€ç 
	rw := &responseWriter{ResponseWriter: w, statusCode: 200}

	// è·¯ç”±å¤„ç†
	switch {
	// æ—¥å¿—ç›¸å…³ API
	case path == "/api/v1/logs":
		logsHandler(rw, r, log)
	case path == "/api/v1/logs/export":
		logsExportHandler(rw, r, log)
	case path == "/api/v1/logs/clear":
		logsClearHandler(rw, r, log)
	case path == "/api/v1/logs/stats":
		logsStatsHandler(rw, r, log)
		
	// ä¸šåŠ¡ API
	case path == "/api/v1/health":
		healthHandler(rw, r, log)
	case path == "/api/v1/tasks":
		tasksHandler(rw, r, log)
	case strings.HasPrefix(path, "/api/v1/tasks/"):
		taskHandler(rw, r, log)
	case path == "/api/v1/system/status":
		systemHandler(rw, r, log)
	case path == "/api/v1/test-connection":
		testConnectionHandler(rw, r, log)
	case path == "/api/v1/analyze-cluster":
		analyzeClusterHandler(rw, r, log)
	case path == "/api/v1/recommend-config":
		recommendConfigHandler(rw, r, log)
	case path == "/api/v1/templates":
		templatesHandler(rw, r, log)
	case strings.HasPrefix(path, "/api/v1/templates/"):
		templateHandler(rw, r, log)
		
	// é™æ€èµ„æº
	case strings.HasPrefix(path, "/assets/"):
		http.FileServer(http.Dir("./web/dist")).ServeHTTP(rw, r)
		
	// SPA å…¥å£
	default:
		http.ServeFile(rw, r, "./web/dist/index.html")
	}

	// è®°å½•å“åº”æ—¥å¿—
	duration := time.Since(startTime)
	log.Info("Request completed", map[string]interface{}{
		"status":      rw.statusCode,
		"duration_ms": duration.Milliseconds(),
	})
}

// responseWriter åŒ…è£…å™¨
type responseWriter struct {
	http.ResponseWriter
	statusCode int
}

func (rw *responseWriter) WriteHeader(code int) {
	rw.statusCode = code
	rw.ResponseWriter.WriteHeader(code)
}

func getPID() int {
	return 1 // ç®€åŒ–å¤„ç†
}

func jsonResponse(w http.ResponseWriter, data interface{}) {
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(data)
}

func initDemoData() {
	// ä¸å†åˆå§‹åŒ–demoæ•°æ®ï¼Œè®©ç”¨æˆ·åˆ›å»ºçœŸå®ä»»åŠ¡
	logger.Info("System initialized", map[string]interface{}{"mode": "production"})
}

// ==================== æ—¥å¿— API ====================

func logsHandler(w http.ResponseWriter, r *http.Request, log *logger.RequestLogger) {
	if r.Method != "GET" {
		http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
		return
	}

	q := r.URL.Query()
	limit, _ := strconv.Atoi(q.Get("limit"))
	offset, _ := strconv.Atoi(q.Get("offset"))
	if limit == 0 {
		limit = 100
	}

	filter := logger.LogFilter{
		Level:     q.Get("level"),
		RequestID: q.Get("request_id"),
		TaskID:    q.Get("task_id"),
		Keyword:   q.Get("keyword"),
		StartTime: q.Get("start_time"),
		EndTime:   q.Get("end_time"),
		Offset:    offset,
		Limit:     limit,
	}

	entries := logger.Default().GetEntries(filter)
	total := logger.Default().GetTotalCount(filter)

	log.Debug("Logs queried", map[string]interface{}{
		"filter": filter,
		"count":  len(entries),
		"total":  total,
	})

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data": map[string]interface{}{
			"items":  entries,
			"total":  total,
			"offset": offset,
			"limit":  limit,
		},
	})
}

func logsExportHandler(w http.ResponseWriter, r *http.Request, log *logger.RequestLogger) {
	if r.Method != "GET" {
		http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
		return
	}

	q := r.URL.Query()
	format := q.Get("format")
	if format == "" {
		format = "text"
	}

	taskID := q.Get("task_id")
	filter := logger.LogFilter{
		Level:     q.Get("level"),
		RequestID: q.Get("request_id"),
		TaskID:    taskID,
		Keyword:   q.Get("keyword"),
		StartTime: q.Get("start_time"),
		EndTime:   q.Get("end_time"),
	}

	// å¦‚æœæŒ‡å®šäº†ä»»åŠ¡IDï¼Œè·å–ä»»åŠ¡åç§°ç”¨äºæ–‡ä»¶å
	taskName := ""
	if taskID != "" {
		tasksMu.RLock()
		for _, t := range tasks {
			if t.ID == taskID || strings.HasPrefix(t.ID, taskID) {
				taskName = t.Name
				filter.TaskID = t.ID // ä½¿ç”¨å®Œæ•´ID
				break
			}
		}
		tasksMu.RUnlock()
	}

	data, err := logger.Default().Export(filter, format)
	if err != nil {
		log.Error("Failed to export logs", map[string]interface{}{"error": err.Error()})
		jsonResponse(w, map[string]interface{}{"code": 500, "message": err.Error()})
		return
	}

	// ç”Ÿæˆæ–‡ä»¶å
	var filename string
	timestamp := time.Now().Format("20060102-150405")
	ext := "txt"
	if format == "json" {
		ext = "json"
	}
	if taskID != "" {
		shortID := taskID
		if len(shortID) > 8 {
			shortID = shortID[:8]
		}
		if taskName != "" {
			// æ¸…ç†ä»»åŠ¡åä¸­çš„ç‰¹æ®Šå­—ç¬¦
			safeName := strings.Map(func(r rune) rune {
				if r == '/' || r == '\\' || r == ':' || r == '*' || r == '?' || r == '"' || r == '<' || r == '>' || r == '|' {
					return '-'
				}
				return r
			}, taskName)
			filename = fmt.Sprintf("task-%s-%s-%s.%s", shortID, safeName, timestamp, ext)
		} else {
			filename = fmt.Sprintf("task-%s-logs-%s.%s", shortID, timestamp, ext)
		}
	} else {
		filename = fmt.Sprintf("tendis-migrate-logs-%s.%s", timestamp, ext)
	}
	
	if format == "json" {
		w.Header().Set("Content-Type", "application/json")
	} else {
		w.Header().Set("Content-Type", "text/plain; charset=utf-8")
	}
	w.Header().Set("Content-Disposition", fmt.Sprintf("attachment; filename=\"%s\"", filename))
	w.Header().Set("Content-Length", strconv.Itoa(len(data)))
	w.Write(data)

	log.Info("Logs exported", map[string]interface{}{
		"format":   format,
		"size":     len(data),
		"filename": filename,
		"task_id":  taskID,
	})
}

func logsClearHandler(w http.ResponseWriter, r *http.Request, log *logger.RequestLogger) {
	if r.Method != "POST" {
		http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
		return
	}

	logger.Default().Clear()
	log.Info("Logs cleared")

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
	})
}

func logsStatsHandler(w http.ResponseWriter, r *http.Request, log *logger.RequestLogger) {
	if r.Method != "GET" {
		http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
		return
	}

	l := logger.Default()
	
	// ç»Ÿè®¡å„çº§åˆ«æ—¥å¿—æ•°é‡
	stats := map[string]int{
		"DEBUG": 0,
		"INFO":  0,
		"WARN":  0,
		"ERROR": 0,
		"FATAL": 0,
	}
	
	allEntries := l.GetEntries(logger.LogFilter{Limit: 0})
	for _, e := range allEntries {
		stats[e.Level]++
	}

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data": map[string]interface{}{
			"total":     len(allEntries),
			"by_level":  stats,
			"uptime":    time.Since(startTime).String(),
			"memory_mb": getMemoryUsage(),
		},
	})
}

func getMemoryUsage() float64 {
	var m runtime.MemStats
	runtime.ReadMemStats(&m)
	return float64(m.Alloc) / 1024 / 1024
}

// ==================== ä¸šåŠ¡ API ====================

func healthHandler(w http.ResponseWriter, r *http.Request, log *logger.RequestLogger) {
	log.Debug("Health check")
	jsonResponse(w, map[string]interface{}{
		"status": "healthy",
		"time":   time.Now().Format(time.RFC3339),
	})
}

func tasksHandler(w http.ResponseWriter, r *http.Request, log *logger.RequestLogger) {
	switch r.Method {
	case "GET":
		listTasksHandler(w, r, log)
	case "POST":
		createTaskHandler(w, r, log)
	default:
		http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
	}
}

func listTasksHandler(w http.ResponseWriter, r *http.Request, log *logger.RequestLogger) {
	tasksMu.RLock()
	defer tasksMu.RUnlock()

	var items []map[string]interface{}
	for _, t := range tasks {
		items = append(items, map[string]interface{}{
			"id":         t.ID,
			"name":       t.Name,
			"status":     t.Status,
			"created_at": t.CreatedAt,
			"updated_at": t.UpdatedAt,
			"progress": map[string]interface{}{
				"percentage":    t.Progress,
				"keys_total":    t.KeysTotal,
				"keys_migrated": t.KeysMigrated,
				"speed":         t.Speed,
			},
		})
	}

	log.Debug("Tasks listed", map[string]interface{}{"count": len(items)})

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data": map[string]interface{}{
			"items": items,
			"total": len(items),
		},
	})
}

func createTaskHandler(w http.ResponseWriter, r *http.Request, log *logger.RequestLogger) {
	var req struct {
		Name          string `json:"name"`
		MigrationMode string `json:"migration_mode"`
		SourceCluster struct {
			Addrs    []string `json:"addrs"`
			Password string   `json:"password"`
		} `json:"source_cluster"`
		TargetCluster struct {
			Addrs    []string `json:"addrs"`
			Password string   `json:"password"`
		} `json:"target_cluster"`
		Options *TaskOptions `json:"options"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		log.Error("Failed to decode request", map[string]interface{}{"error": err.Error()})
		jsonResponse(w, map[string]interface{}{"code": 400, "message": err.Error()})
		return
	}

	mode := req.MigrationMode
	if mode == "" {
		mode = "full_and_incremental"
	}

	// è®¾ç½®é»˜è®¤é€‰é¡¹
	options := req.Options
	if options == nil {
		options = &TaskOptions{
			WorkerCount:       4,
			ScanBatchSize:     1000,
			ConflictPolicy:    "skip_full_only",
			LargeKeyThreshold: 10485760,
			EnableCompression: true,
			KeyFilter: &KeyFilter{
				Mode: "all",
			},
		}
	} else {
		// ç¡®ä¿ KeyFilter æœ‰é»˜è®¤å€¼
		if options.KeyFilter == nil {
			options.KeyFilter = &KeyFilter{Mode: "all"}
		} else if options.KeyFilter.Mode == "" {
			options.KeyFilter.Mode = "all"
		}
		// ç¡®ä¿å…¶ä»–é€‰é¡¹æœ‰é»˜è®¤å€¼
		if options.WorkerCount == 0 {
			options.WorkerCount = 4
		}
		if options.ScanBatchSize == 0 {
			options.ScanBatchSize = 1000
		}
		if options.ConflictPolicy == "" {
			options.ConflictPolicy = "skip_full_only"
		}
		if options.LargeKeyThreshold == 0 {
			options.LargeKeyThreshold = 10485760
		}
	}

	task := &Task{
		ID:             uuid.New().String(),
		Name:           req.Name,
		Status:         "pending",
		Progress:       0,
		SourceCluster:  strings.Join(req.SourceCluster.Addrs, ","),
		TargetCluster:  strings.Join(req.TargetCluster.Addrs, ","),
		SourcePassword: req.SourceCluster.Password,
		TargetPassword: req.TargetCluster.Password,
		MigrationMode:  mode,
		CreatedAt:      time.Now().Format(time.RFC3339),
		UpdatedAt:      time.Now().Format(time.RFC3339),
		Phase:          "full",
		Options:        options,
	}

	tasksMu.Lock()
	tasks[task.ID] = task
	
	// ä¿ç•™æœ€è¿‘3ä¸ªä»»åŠ¡ï¼Œæ¸…ç†æ—§ä»»åŠ¡
	cleanupOldTasks()
	tasksMu.Unlock()

	log.Info("Task created", map[string]interface{}{
		"task_id":        task.ID,
		"task_name":      task.Name,
		"migration_mode": mode,
	})

	// åŒæ—¶è®°å½•ä»»åŠ¡æ—¥å¿—
	logger.WithTask(task.ID).Info("Task created", map[string]interface{}{
		"name":           task.Name,
		"source":         task.SourceCluster,
		"target":         task.TargetCluster,
		"migration_mode": mode,
	})

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data":    map[string]string{"task_id": task.ID},
	})
}

// cleanupOldTasks æ¸…ç†æ—§ä»»åŠ¡ï¼Œä¿ç•™æœ€è¿‘3ä¸ªï¼ˆå¿…é¡»åœ¨tasksMué”å†…è°ƒç”¨ï¼‰
func cleanupOldTasks() {
	const maxTasks = 3
	if len(tasks) <= maxTasks {
		return
	}
	
	// è·å–æ‰€æœ‰ä»»åŠ¡å¹¶æŒ‰åˆ›å»ºæ—¶é—´æ’åº
	taskList := make([]*Task, 0, len(tasks))
	for _, t := range tasks {
		taskList = append(taskList, t)
	}
	sort.Slice(taskList, func(i, j int) bool {
		return taskList[i].CreatedAt > taskList[j].CreatedAt // é™åºï¼Œæœ€æ–°åœ¨å‰
	})
	
	// åˆ é™¤è¶…å‡ºé™åˆ¶çš„æ—§ä»»åŠ¡
	for i := maxTasks; i < len(taskList); i++ {
		oldTask := taskList[i]
		// å¦‚æœä»»åŠ¡æ­£åœ¨è¿è¡Œï¼Œè·³è¿‡
		if oldTask.Status == "running" {
			continue
		}
		// æ¸…ç†è¯¥ä»»åŠ¡çš„æ—¥å¿—
		logger.Default().ClearTaskLogs(oldTask.ID)
		// åˆ é™¤ä»»åŠ¡
		delete(tasks, oldTask.ID)
		logger.Default().Info("Old task cleaned up", map[string]interface{}{
			"task_id":   oldTask.ID,
			"task_name": oldTask.Name,
		})
	}
}

func taskHandler(w http.ResponseWriter, r *http.Request, log *logger.RequestLogger) {
	path := strings.TrimPrefix(r.URL.Path, "/api/v1/tasks/")
	parts := strings.Split(path, "/")

	if len(parts) == 0 || parts[0] == "" {
		http.NotFound(w, r)
		return
	}

	id := parts[0]
	action := ""
	if len(parts) > 1 {
		action = parts[1]
	}

	taskLog := logger.WithTask(id)

	switch {
	case action == "" && r.Method == "GET":
		getTaskHandler(w, r, id, log)
	case action == "" && r.Method == "DELETE":
		deleteTaskHandler(w, r, id, log, taskLog)
	case action == "start" && r.Method == "POST":
		startTaskHandler(w, r, id, log, taskLog)
	case action == "pause" && r.Method == "POST":
		pauseTaskHandler(w, r, id, log, taskLog)
	case action == "resume" && r.Method == "POST":
		resumeTaskHandler(w, r, id, log, taskLog)
	case action == "progress" && r.Method == "GET":
		progressHandler(w, r, id, log)
	case action == "logs" && r.Method == "GET":
		taskLogsHandler(w, r, id, log)
	case action == "verify" && r.Method == "POST":
		triggerVerifyHandler(w, r, id, log)
	case strings.HasPrefix(action, "verify") && r.Method == "GET":
		verifyResultsHandler(w, r, id, log)
	case action == "error-keys" && r.Method == "GET":
		errorKeysHandler(w, r, id, log)
	case strings.HasPrefix(action, "error-keys/download") && r.Method == "GET":
		downloadErrorKeysHandler(w, r, id, log)
	default:
		http.NotFound(w, r)
	}
}

func getTaskHandler(w http.ResponseWriter, r *http.Request, id string, log *logger.RequestLogger) {
	tasksMu.RLock()
	task, ok := tasks[id]
	tasksMu.RUnlock()

	if !ok {
		log.Warn("Task not found", map[string]interface{}{"task_id": id})
		jsonResponse(w, map[string]interface{}{"code": 404, "message": "Task not found"})
		return
	}

	log.Debug("Task retrieved", map[string]interface{}{"task_id": id})

	phase := task.Phase
	if phase == "" {
		phase = "full"
	}

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data": map[string]interface{}{
			"id":             task.ID,
			"name":           task.Name,
			"status":         task.Status,
			"migration_mode": task.MigrationMode,
			"source_cluster": map[string]interface{}{"addrs": strings.Split(task.SourceCluster, ",")},
			"target_cluster": map[string]interface{}{"addrs": strings.Split(task.TargetCluster, ",")},
			"created_at":     task.CreatedAt,
			"updated_at":     task.UpdatedAt,
			"started_at":     task.StartedAt,
			"full_start_at":  task.FullStartAt,
			"incr_start_at":  task.IncrStartAt,
			"progress": map[string]interface{}{
				"percentage":     task.Progress,
				"total_keys":     task.KeysTotal,
				"migrated_keys":  task.KeysMigrated,
				"total_bytes":    task.BytesTotal,
				"migrated_bytes": task.BytesMigrated,
				"current_speed":  task.Speed,
				"phase":          phase,
				"estimated_eta":  calculateETA(task),
				"elapsed_time":   calculateElapsedTime(task),
			},
			"stats": map[string]interface{}{
				"total_keys":     task.KeysTotal,
				"migrated_keys":  task.KeysMigrated,
				"failed_keys":    task.KeysFailed,
				"skipped_keys":   task.KeysSkipped,
				"filtered_keys":  task.KeysFiltered,
				"bytes_sent":     task.BytesMigrated,
			},
		},
	})
}

func calculateETA(task *Task) string {
	if task.Speed <= 0 || task.KeysTotal <= task.KeysMigrated {
		return "-"
	}
	remaining := task.KeysTotal - task.KeysMigrated
	seconds := remaining / task.Speed
	if seconds < 60 {
		return fmt.Sprintf("%ds", seconds)
	} else if seconds < 3600 {
		return fmt.Sprintf("%dm %ds", seconds/60, seconds%60)
	}
	return fmt.Sprintf("%dh %dm", seconds/3600, (seconds%3600)/60)
}

// calculateElapsedTime è®¡ç®—å·²è€—æ—¶é—´
func calculateElapsedTime(task *Task) string {
	if task.StartedAt == "" {
		return "-"
	}
	// ä½¿ç”¨æœ¬åœ°æ—¶åŒºè§£ææ—¶é—´
	loc := time.Local
	startTime, err := time.ParseInLocation("2006-01-02 15:04:05", task.StartedAt, loc)
	if err != nil {
		return "-"
	}
	elapsed := time.Since(startTime)
	seconds := int64(elapsed.Seconds())
	if seconds < 0 {
		return "-"
	}
	if seconds < 60 {
		return fmt.Sprintf("%ds", seconds)
	} else if seconds < 3600 {
		return fmt.Sprintf("%dm %ds", seconds/60, seconds%60)
	} else if seconds < 86400 {
		return fmt.Sprintf("%dh %dm", seconds/3600, (seconds%3600)/60)
	}
	return fmt.Sprintf("%dd %dh", seconds/86400, (seconds%86400)/3600)
}

func deleteTaskHandler(w http.ResponseWriter, r *http.Request, id string, log *logger.RequestLogger, taskLog *logger.TaskLogger) {
	tasksMu.Lock()
	delete(tasks, id)
	tasksMu.Unlock()
	
	log.Info("Task deleted", map[string]interface{}{"task_id": id})
	taskLog.Info("Task deleted")
	
	jsonResponse(w, map[string]interface{}{"code": 0, "message": "success"})
}

func startTaskHandler(w http.ResponseWriter, r *http.Request, id string, log *logger.RequestLogger, taskLog *logger.TaskLogger) {
	tasksMu.Lock()
	task, ok := tasks[id]
	if ok {
		task.Status = "running"
		task.UpdatedAt = time.Now().Format(time.RFC3339)
		task.StartedAt = time.Now().Format("2006-01-02 15:04:05")
		go simulateProgress(task)
	}
	tasksMu.Unlock()
	
	if ok {
		log.Info("Task started", map[string]interface{}{"task_id": id})
		taskLog.Info("Task started", map[string]interface{}{
			"keys_total": task.KeysTotal,
		})
	} else {
		log.Warn("Task not found for start", map[string]interface{}{"task_id": id})
	}
	
	jsonResponse(w, map[string]interface{}{"code": 0, "message": "success"})
}

func pauseTaskHandler(w http.ResponseWriter, r *http.Request, id string, log *logger.RequestLogger, taskLog *logger.TaskLogger) {
	tasksMu.Lock()
	task, ok := tasks[id]
	if ok {
		task.Status = "paused"
		task.Speed = 0
		task.UpdatedAt = time.Now().Format(time.RFC3339)
	}
	tasksMu.Unlock()
	
	if ok {
		log.Info("Task paused", map[string]interface{}{"task_id": id})
		taskLog.Info("Task paused", map[string]interface{}{
			"progress": task.Progress,
		})
	}
	
	jsonResponse(w, map[string]interface{}{"code": 0, "message": "success"})
}

func resumeTaskHandler(w http.ResponseWriter, r *http.Request, id string, log *logger.RequestLogger, taskLog *logger.TaskLogger) {
	tasksMu.Lock()
	task, ok := tasks[id]
	if ok {
		task.Status = "running"
		task.UpdatedAt = time.Now().Format(time.RFC3339)
		go simulateProgress(task)
	}
	tasksMu.Unlock()
	
	if ok {
		log.Info("Task resumed", map[string]interface{}{"task_id": id})
		taskLog.Info("Task resumed")
	}
	
	jsonResponse(w, map[string]interface{}{"code": 0, "message": "success"})
}

func progressHandler(w http.ResponseWriter, r *http.Request, id string, log *logger.RequestLogger) {
	tasksMu.RLock()
	task, ok := tasks[id]
	tasksMu.RUnlock()

	if !ok {
		log.Warn("Task not found for progress", map[string]interface{}{"task_id": id})
		jsonResponse(w, map[string]interface{}{"code": 404, "message": "Task not found"})
		return
	}

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data": map[string]interface{}{
			"percentage":    task.Progress,
			"keys_total":    task.KeysTotal,
			"keys_migrated": task.KeysMigrated,
			"speed":         task.Speed,
			"phase":         "full",
			"eta":           int64((100 - task.Progress) / 0.5 * 2),
		},
	})
}

func taskLogsHandler(w http.ResponseWriter, r *http.Request, id string, log *logger.RequestLogger) {
	q := r.URL.Query()
	limit, _ := strconv.Atoi(q.Get("limit"))
	offset, _ := strconv.Atoi(q.Get("offset"))
	if limit == 0 {
		limit = 100
	}

	filter := logger.LogFilter{
		TaskID:  id,
		Level:   q.Get("level"),
		Keyword: q.Get("keyword"),
		Offset:  offset,
		Limit:   limit,
	}

	entries := logger.Default().GetEntries(filter)
	total := logger.Default().GetTotalCount(filter)

	log.Debug("Task logs queried", map[string]interface{}{
		"task_id": id,
		"count":   len(entries),
	})

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data": map[string]interface{}{
			"items":  entries,
			"total":  total,
			"offset": offset,
			"limit":  limit,
		},
	})
}

func triggerVerifyHandler(w http.ResponseWriter, r *http.Request, id string, log *logger.RequestLogger) {
	log.Info("Verify triggered", map[string]interface{}{"task_id": id})
	
	// æ¨¡æ‹Ÿè§¦å‘æ ¡éªŒ
	batchID := uuid.New().String()
	
	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data":    map[string]string{"batch_id": batchID},
	})
}

func verifyResultsHandler(w http.ResponseWriter, r *http.Request, id string, log *logger.RequestLogger) {
	log.Debug("Verify results queried", map[string]interface{}{"task_id": id})
	
	// è¿”å›ç©ºæ•°ç»„ï¼Œæš‚æ— æ ¡éªŒç»“æœ
	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data":    []interface{}{},
	})
}

func simulateProgress(task *Task) {
	taskLog := logger.WithTask(task.ID)
	taskLog.Info("Migration started - connecting to clusters")

	ctx := context.Background()

	// è§£ææºç«¯å’Œç›®æ ‡ç«¯åœ°å€
	sourceAddrs := strings.Split(task.SourceCluster, ",")
	targetAddrs := strings.Split(task.TargetCluster, ",")

	for i := range sourceAddrs {
		sourceAddrs[i] = strings.TrimSpace(sourceAddrs[i])
	}
	for i := range targetAddrs {
		targetAddrs[i] = strings.TrimSpace(targetAddrs[i])
	}

	// å°è¯•è¿æ¥æºç«¯
	sourceClient, sourceIsCluster, err := connectRedis(ctx, sourceAddrs, task.SourcePassword)
	if err != nil {
		taskLog.Error("Failed to connect source cluster", map[string]interface{}{"error": err.Error()})
		tasksMu.Lock()
		task.Status = "failed"
		task.UpdatedAt = time.Now().Format(time.RFC3339)
		tasksMu.Unlock()
		return
	}
	defer sourceClient.Close()

	// å°è¯•è¿æ¥ç›®æ ‡ç«¯
	targetClient, targetIsCluster, err := connectRedis(ctx, targetAddrs, task.TargetPassword)
	if err != nil {
		taskLog.Error("Failed to connect target cluster", map[string]interface{}{"error": err.Error()})
		tasksMu.Lock()
		task.Status = "failed"
		task.UpdatedAt = time.Now().Format(time.RFC3339)
		tasksMu.Unlock()
		return
	}
	defer targetClient.Close()

	taskLog.Info("Connected to clusters", map[string]interface{}{
		"source_mode": map[bool]string{true: "cluster", false: "standalone"}[sourceIsCluster],
		"target_mode": map[bool]string{true: "cluster", false: "standalone"}[targetIsCluster],
	})

	// è®°å½•Keyè¿‡æ»¤é…ç½®
	if task.Options != nil && task.Options.KeyFilter != nil {
		taskLog.Info("Key filter config", map[string]interface{}{
			"mode":             task.Options.KeyFilter.Mode,
			"prefixes":         task.Options.KeyFilter.Prefixes,
			"exclude_prefixes": task.Options.KeyFilter.ExcludePrefixes,
			"patterns":         task.Options.KeyFilter.Patterns,
		})
	}

	// è·å–æºç«¯Keyæ€»æ•°
	totalKeys, err := getDBSize(ctx, sourceClient, sourceIsCluster)
	if err != nil {
		taskLog.Warn("Failed to get source DB size, using estimate", map[string]interface{}{"error": err.Error()})
		totalKeys = 10000 // é»˜è®¤ä¼°ç®—å€¼
	}

	tasksMu.Lock()
	task.KeysTotal = totalKeys
	task.BytesTotal = totalKeys * 256 // ä¼°ç®—å¹³å‡æ¯ä¸ªkey 256 bytes
	task.FullStartAt = time.Now().Format("2006-01-02 15:04:05")
	tasksMu.Unlock()

	taskLog.Info("Starting full migration", map[string]interface{}{
		"total_keys": totalKeys,
	})

	// æ‰§è¡Œå…¨é‡è¿ç§»
	doFullMigration(ctx, task, sourceClient, targetClient, sourceIsCluster, targetIsCluster, taskLog)

	// æ£€æŸ¥æ˜¯å¦éœ€è¦å¢é‡è¿ç§»
	tasksMu.RLock()
	status := task.Status
	mode := task.MigrationMode
	tasksMu.RUnlock()

	if status == "running" && mode == "full_and_incremental" {
		taskLog.Info("Starting incremental sync")
		tasksMu.Lock()
		task.Phase = "incremental"
		task.IncrStartAt = time.Now().Format("2006-01-02 15:04:05")
		tasksMu.Unlock()
		// å¢é‡åŒæ­¥é€»è¾‘ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼šæŒç»­ç›‘å¬ï¼‰
		doIncrementalSync(ctx, task, sourceClient, targetClient, sourceIsCluster, targetIsCluster, taskLog)
	}
}

// connectRedis è¿æ¥Redisï¼Œè¿”å›é€šç”¨å®¢æˆ·ç«¯æ¥å£
func connectRedis(ctx context.Context, addrs []string, password string) (redis.UniversalClient, bool, error) {
	// å…ˆå°è¯•é›†ç¾¤æ¨¡å¼
	clusterClient := redis.NewClusterClient(&redis.ClusterOptions{
		Addrs:    addrs,
		Password: password,
	})
	if err := clusterClient.Ping(ctx).Err(); err == nil {
		return clusterClient, true, nil
	}
	clusterClient.Close()

	// å°è¯•å•æœºæ¨¡å¼
	standaloneClient := redis.NewClient(&redis.Options{
		Addr:     addrs[0],
		Password: password,
	})
	if err := standaloneClient.Ping(ctx).Err(); err != nil {
		standaloneClient.Close()
		return nil, false, err
	}
	return standaloneClient, false, nil
}

// getDBSize è·å–æ•°æ®åº“Keyæ•°é‡
func getDBSize(ctx context.Context, client redis.UniversalClient, isCluster bool) (int64, error) {
	if !isCluster {
		return client.DBSize(ctx).Result()
	}

	// é›†ç¾¤æ¨¡å¼éœ€è¦éå†æ‰€æœ‰èŠ‚ç‚¹
	clusterClient := client.(*redis.ClusterClient)
	var total int64
	var mu sync.Mutex

	err := clusterClient.ForEachMaster(ctx, func(ctx context.Context, node *redis.Client) error {
		size, err := node.DBSize(ctx).Result()
		if err != nil {
			return err
		}
		mu.Lock()
		total += size
		mu.Unlock()
		return nil
	})
	return total, err
}

// doFullMigration æ‰§è¡Œå…¨é‡è¿ç§»ï¼ˆå¹¶è¡ŒWorkeræ¨¡å¼ï¼‰
func doFullMigration(ctx context.Context, task *Task, sourceClient, targetClient redis.UniversalClient, sourceIsCluster, targetIsCluster bool, taskLog *logger.TaskLogger) {
	// è·å–é…ç½®å‚æ•°
	batchSize := int64(1000)
	workerCount := 4
	var rateLimiter *RateLimiter

	if task.Options != nil {
		if task.Options.ScanBatchSize > 0 {
			batchSize = int64(task.Options.ScanBatchSize)
		}
		if task.Options.WorkerCount > 0 {
			workerCount = task.Options.WorkerCount
		}
		// åˆå§‹åŒ–é™é€Ÿå™¨
		if task.Options.RateLimit != nil && task.Options.RateLimit.SourceQPS > 0 {
			rateLimiter = NewRateLimiter(task.Options.RateLimit.SourceQPS)
		}
	}

	// è·å–å†²çªç­–ç•¥
	conflictPolicy := "skip_full_only"
	if task.Options != nil && task.Options.ConflictPolicy != "" {
		conflictPolicy = task.Options.ConflictPolicy
	}

	taskLog.Info("Starting parallel migration", map[string]interface{}{
		"worker_count": workerCount,
		"batch_size":   batchSize,
		"policy":       conflictPolicy,
	})

	// ç»Ÿè®¡è®¡æ•°å™¨ï¼ˆä½¿ç”¨åŸå­æ“ä½œï¼‰
	var migratedCount int64
	var migratedBytes int64
	var failedCount int64
	var skippedCount int64
	var filteredCount int64
	startTime := time.Now()
	lastLogTime := time.Now()
	var lastLogMu sync.Mutex

	// ç”¨äºè¿½è¸ªå·²å¤„ç†çš„keyï¼ˆé¿å…é‡å¤å¤„ç†ï¼‰
	processedKeys := sync.Map{}

	// åˆ›å»ºKeyé€šé“
	keyChan := make(chan string, workerCount*100)
	var wg sync.WaitGroup

	// å¯åŠ¨Workeråç¨‹æ± 
	for i := 0; i < workerCount; i++ {
		wg.Add(1)
		go func(workerID int) {
			defer wg.Done()
			for key := range keyChan {
				// æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
				tasksMu.RLock()
				status := task.Status
				tasksMu.RUnlock()
				if status != "running" {
					continue
				}

				// æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
				if _, loaded := processedKeys.LoadOrStore(key, true); loaded {
					continue
				}

				// é™é€Ÿ
				if rateLimiter != nil {
					rateLimiter.Wait()
				}

				// æ£€æŸ¥Keyæ˜¯å¦åŒ¹é…è¿‡æ»¤è§„åˆ™
				if !matchKeyFilter(key, task.Options) {
					atomic.AddInt64(&filteredCount, 1)
					continue
				}

				// è¿ç§»Key
				migrated, bytes, reason := migrateKeyWithPolicy(ctx, sourceClient, targetClient, key, conflictPolicy)

				if migrated {
					atomic.AddInt64(&migratedCount, 1)
					atomic.AddInt64(&migratedBytes, bytes)
				} else if reason == "skipped" {
					atomic.AddInt64(&skippedCount, 1)
				} else if reason == "filtered" {
					atomic.AddInt64(&filteredCount, 1)
				} else {
					atomic.AddInt64(&failedCount, 1)
					addErrorKey(task.ID, key, "string", "failed", reason)
				}
			}
		}(i)
	}

	// è¿›åº¦æ›´æ–°åç¨‹
	stopProgress := make(chan struct{})
	go func() {
		ticker := time.NewTicker(500 * time.Millisecond)
		defer ticker.Stop()
		for {
			select {
			case <-stopProgress:
				return
			case <-ticker.C:
				mc := atomic.LoadInt64(&migratedCount)
				mb := atomic.LoadInt64(&migratedBytes)
				fc := atomic.LoadInt64(&failedCount)
				sc := atomic.LoadInt64(&skippedCount)
				ftc := atomic.LoadInt64(&filteredCount)

				tasksMu.Lock()
				task.KeysMigrated = mc
				task.BytesMigrated = mb
				task.KeysFailed = fc
				task.KeysSkipped = sc
				task.KeysFiltered = ftc
				if task.KeysTotal > 0 {
					task.Progress = float64(mc+sc+ftc) / float64(task.KeysTotal) * 100
					if task.Progress > 100 {
						task.Progress = 100
					}
				}
				elapsed := time.Since(startTime).Seconds()
				if elapsed > 0 {
					task.Speed = int64(float64(mc) / elapsed)
				}
				task.UpdatedAt = time.Now().Format(time.RFC3339)
				tasksMu.Unlock()

				// æ¯10ç§’è®°å½•ä¸€æ¬¡æ—¥å¿—
				lastLogMu.Lock()
				if time.Since(lastLogTime) > 10*time.Second {
					taskLog.Info("Migration progress", map[string]interface{}{
						"progress":      fmt.Sprintf("%.1f%%", task.Progress),
						"migrated_keys": mc,
						"failed_keys":   fc,
						"skipped_keys":  sc,
						"filtered_keys": ftc,
						"speed":         task.Speed,
						"workers":       workerCount,
					})
					lastLogTime = time.Now()
				}
				lastLogMu.Unlock()
			}
		}
	}()

	// SCANå¹¶åˆ†å‘Keyåˆ°Worker
	if sourceIsCluster {
		// é›†ç¾¤æ¨¡å¼ï¼šå¹¶è¡Œéå†æ‰€æœ‰masterèŠ‚ç‚¹
		clusterClient := sourceClient.(*redis.ClusterClient)
		var scanWg sync.WaitGroup

		clusterClient.ForEachMaster(ctx, func(ctx context.Context, node *redis.Client) error {
			scanWg.Add(1)
			go func(nodeClient *redis.Client) {
				defer scanWg.Done()
				var cursor uint64
				for {
					tasksMu.RLock()
					status := task.Status
					tasksMu.RUnlock()
					if status != "running" {
						return
					}

					keys, newCursor, err := nodeClient.Scan(ctx, cursor, "*", batchSize).Result()
					if err != nil {
						taskLog.Warn("SCAN failed on node", map[string]interface{}{"error": err.Error()})
						time.Sleep(time.Second)
						continue
					}

					for _, key := range keys {
						tasksMu.RLock()
						status := task.Status
						tasksMu.RUnlock()
						if status != "running" {
							return
						}
						keyChan <- key
					}

					cursor = newCursor
					if cursor == 0 {
						break
					}
				}
			}(node)
			return nil
		})

		scanWg.Wait()
	} else {
		// å•æœºæ¨¡å¼
		var cursor uint64
		for {
			tasksMu.RLock()
			status := task.Status
			tasksMu.RUnlock()
			if status != "running" {
				break
			}

			keys, newCursor, err := sourceClient.Scan(ctx, cursor, "*", batchSize).Result()
			if err != nil {
				taskLog.Error("SCAN failed", map[string]interface{}{"error": err.Error()})
				time.Sleep(time.Second)
				continue
			}

			for _, key := range keys {
				tasksMu.RLock()
				status := task.Status
				tasksMu.RUnlock()
				if status != "running" {
					break
				}
				keyChan <- key
			}

			cursor = newCursor
			if cursor == 0 {
				break
			}
		}
	}

	// å…³é—­é€šé“ï¼Œç­‰å¾…æ‰€æœ‰Workerå®Œæˆ
	close(keyChan)
	wg.Wait()
	close(stopProgress)

	// æœ€ç»ˆæ›´æ–°ç»Ÿè®¡
	mc := atomic.LoadInt64(&migratedCount)
	fc := atomic.LoadInt64(&failedCount)
	sc := atomic.LoadInt64(&skippedCount)
	ftc := atomic.LoadInt64(&filteredCount)

	tasksMu.Lock()
	task.KeysMigrated = mc
	task.KeysFailed = fc
	task.KeysSkipped = sc
	task.KeysFiltered = ftc
	if task.Status == "running" {
		if task.MigrationMode == "full_only" {
			task.Status = "completed"
			task.Progress = 100
			task.Phase = "completed"
		} else {
			// å…¨é‡è¿ç§»å®Œæˆï¼Œå‡†å¤‡è¿›å…¥å¢é‡åŒæ­¥
			task.Progress = 100
			task.Phase = "incremental"
		}
	}
	task.UpdatedAt = time.Now().Format(time.RFC3339)
	tasksMu.Unlock()

	taskLog.Info("Full migration completed", map[string]interface{}{
		"migrated_keys": mc,
		"failed_keys":   fc,
		"skipped_keys":  sc,
		"filtered_keys": ftc,
		"duration":      time.Since(startTime).String(),
		"avg_speed":     int64(float64(mc) / time.Since(startTime).Seconds()),
	})
}

// RateLimiter ç®€å•çš„é™é€Ÿå™¨
type RateLimiter struct {
	ticker   *time.Ticker
	tokens   chan struct{}
	stopChan chan struct{}
}

// NewRateLimiter åˆ›å»ºé™é€Ÿå™¨
func NewRateLimiter(qps int) *RateLimiter {
	if qps <= 0 {
		return nil
	}
	interval := time.Second / time.Duration(qps)
	if interval < time.Microsecond {
		interval = time.Microsecond
	}

	rl := &RateLimiter{
		ticker:   time.NewTicker(interval),
		tokens:   make(chan struct{}, qps),
		stopChan: make(chan struct{}),
	}

	// é¢„å¡«å……tokens
	for i := 0; i < qps/10+1; i++ {
		select {
		case rl.tokens <- struct{}{}:
		default:
		}
	}

	// æŒç»­å¡«å……tokens
	go func() {
		for {
			select {
			case <-rl.stopChan:
				return
			case <-rl.ticker.C:
				select {
				case rl.tokens <- struct{}{}:
				default:
				}
			}
		}
	}()

	return rl
}

// Wait ç­‰å¾…è·å–ä»¤ç‰Œ
func (rl *RateLimiter) Wait() {
	if rl == nil {
		return
	}
	<-rl.tokens
}

// Stop åœæ­¢é™é€Ÿå™¨
func (rl *RateLimiter) Stop() {
	if rl == nil {
		return
	}
	close(rl.stopChan)
	rl.ticker.Stop()
}

// matchKeyFilter æ£€æŸ¥Keyæ˜¯å¦åŒ¹é…è¿‡æ»¤è§„åˆ™
func matchKeyFilter(key string, options *TaskOptions) bool {
	if options == nil || options.KeyFilter == nil {
		return true
	}

	filter := options.KeyFilter
	switch filter.Mode {
	case "prefix":
		// æ£€æŸ¥æ’é™¤å‰ç¼€
		for _, prefix := range filter.ExcludePrefixes {
			if strings.HasPrefix(key, prefix) {
				return false
			}
		}
		// å¦‚æœè®¾ç½®äº†åŒ…å«å‰ç¼€ï¼Œåªè¿ç§»åŒ¹é…çš„
		if len(filter.Prefixes) > 0 {
			for _, prefix := range filter.Prefixes {
				if strings.HasPrefix(key, prefix) {
					return true
				}
			}
			return false
		}
		return true
	case "pattern":
		// æ­£åˆ™åŒ¹é…ï¼ˆç®€åŒ–å®ç°ï¼Œä½¿ç”¨ strings.Containsï¼‰
		if len(filter.Patterns) > 0 {
			for _, pattern := range filter.Patterns {
				if strings.Contains(key, pattern) {
					return true
				}
			}
			return false
		}
		return true
	default:
		return true
	}
}

// migrateKeyWithPolicy æ ¹æ®å†²çªç­–ç•¥è¿ç§»Key
func migrateKeyWithPolicy(ctx context.Context, sourceClient, targetClient redis.UniversalClient, key string, policy string) (bool, int64, string) {
	// è·å–Keyçš„TTL
	ttl, err := sourceClient.TTL(ctx, key).Result()
	if err != nil {
		return false, 0, "get TTL failed: " + err.Error()
	}

	// ä½¿ç”¨DUMP+RESTOREè¿ç§»
	dump, err := sourceClient.Dump(ctx, key).Result()
	if err != nil {
		if err == redis.Nil {
			return false, 0, "skipped" // Keyä¸å­˜åœ¨ï¼Œè·³è¿‡
		}
		return false, 0, "dump failed: " + err.Error()
	}

	bytes := int64(len(dump))

	// æ£€æŸ¥ç›®æ ‡æ˜¯å¦å­˜åœ¨
	exists, err := targetClient.Exists(ctx, key).Result()
	if err != nil {
		return false, 0, "check exists failed: " + err.Error()
	}

	if exists > 0 {
		switch policy {
		case "skip", "skip_full_only":
			return false, 0, "skipped"
		case "replace":
			// å…ˆåˆ é™¤ç›®æ ‡Key
			if err := targetClient.Del(ctx, key).Err(); err != nil {
				return false, 0, "delete failed: " + err.Error()
			}
		case "error":
			return false, 0, "key conflict"
		default:
			return false, 0, "skipped"
		}
	}

	// RESTOREåˆ°ç›®æ ‡
	if ttl < 0 {
		ttl = 0 // æ— è¿‡æœŸæ—¶é—´
	}
	err = targetClient.Restore(ctx, key, ttl, dump).Err()
	if err != nil {
		return false, 0, "restore failed: " + err.Error()
	}

	return true, bytes, ""
}

// migrateKey è¿ç§»å•ä¸ªKeyï¼ˆä¿ç•™æ—§å‡½æ•°ä¾›å…¼å®¹ï¼‰
func migrateKey(ctx context.Context, sourceClient, targetClient redis.UniversalClient, key string) (bool, int64, string) {
	return migrateKeyWithPolicy(ctx, sourceClient, targetClient, key, "skip_full_only")
}

// scanAllKeys æ‰«ææ‰€æœ‰keyï¼ˆæ”¯æŒé›†ç¾¤æ¨¡å¼ï¼‰
func scanAllKeys(ctx context.Context, client redis.UniversalClient, isCluster bool) (map[string]bool, error) {
	knownKeys := make(map[string]bool)

	if !isCluster {
		// å•æœºæ¨¡å¼ï¼šç›´æ¥SCAN
		var cursor uint64
		for {
			keys, newCursor, err := client.Scan(ctx, cursor, "*", 1000).Result()
			if err != nil {
				return knownKeys, err
			}
			for _, key := range keys {
				knownKeys[key] = true
			}
			cursor = newCursor
			if cursor == 0 {
				break
			}
		}
		return knownKeys, nil
	}

	// é›†ç¾¤æ¨¡å¼ï¼šéå†æ‰€æœ‰masterèŠ‚ç‚¹æ‰«æ
	clusterClient := client.(*redis.ClusterClient)
	var mu sync.Mutex

	err := clusterClient.ForEachMaster(ctx, func(ctx context.Context, node *redis.Client) error {
		var cursor uint64
		for {
			keys, newCursor, err := node.Scan(ctx, cursor, "*", 1000).Result()
			if err != nil {
				return err
			}
			mu.Lock()
			for _, key := range keys {
				knownKeys[key] = true
			}
			mu.Unlock()
			cursor = newCursor
			if cursor == 0 {
				break
			}
		}
		return nil
	})

	return knownKeys, err
}

// doIncrementalSync å¢é‡åŒæ­¥
func doIncrementalSync(ctx context.Context, task *Task, sourceClient, targetClient redis.UniversalClient, sourceIsCluster, targetIsCluster bool, taskLog *logger.TaskLogger) {
	taskLog.Info("Incremental sync mode - monitoring for changes")

	// è®°å½•å·²çŸ¥çš„keyé›†åˆï¼ˆç”¨äºæ£€æµ‹æ–°keyï¼‰
	// ä½¿ç”¨æ–°çš„ scanAllKeys ç¡®ä¿é›†ç¾¤æ¨¡å¼ä¸‹æ‰«ææ‰€æœ‰èŠ‚ç‚¹
	knownKeys, err := scanAllKeys(ctx, sourceClient, sourceIsCluster)
	if err != nil {
		taskLog.Warn("Failed to take initial key snapshot", map[string]interface{}{"error": err.Error()})
	}
	taskLog.Info("Initial key snapshot taken", map[string]interface{}{"known_keys": len(knownKeys)})

	// å®šæœŸæ‰«ææ£€æµ‹æ–°key
	ticker := time.NewTicker(3 * time.Second)
	defer ticker.Stop()

	syncedInIncr := int64(0)
	skippedInIncr := int64(0)

	for {
		select {
		case <-ticker.C:
			tasksMu.RLock()
			status := task.Status
			tasksMu.RUnlock()
			if status != "running" {
				taskLog.Info("Incremental sync stopped", map[string]interface{}{
					"synced_in_incremental":  syncedInIncr,
					"skipped_in_incremental": skippedInIncr,
				})
				return
			}

			// æ‰«ææ‰€æœ‰keyï¼ŒæŸ¥æ‰¾æ–°keyï¼ˆé›†ç¾¤æ¨¡å¼ä¸‹æ‰«ææ‰€æœ‰èŠ‚ç‚¹ï¼‰
			currentKeys, scanErr := scanAllKeys(ctx, sourceClient, sourceIsCluster)
			if scanErr != nil {
				taskLog.Warn("Incremental scan failed", map[string]interface{}{"error": scanErr.Error()})
				continue
			}

			newKeysFound := 0
			newKeysSkipped := 0
			for key := range currentKeys {
				if !knownKeys[key] {
					// æ£€æŸ¥æ˜¯å¦åŒ¹é…è¿‡æ»¤è§„åˆ™
					if !matchKeyFilter(key, task.Options) {
						knownKeys[key] = true
						tasksMu.Lock()
						task.KeysFiltered++
						tasksMu.Unlock()
						continue
					}

					// å‘ç°æ–°keyï¼ŒåŒæ­¥åˆ°ç›®æ ‡
					migrated, bytes, reason := migrateKeyWithPolicy(ctx, sourceClient, targetClient, key, "replace")
					knownKeys[key] = true

					if migrated {
						syncedInIncr++
						newKeysFound++
						tasksMu.Lock()
						task.KeysMigrated++
						task.BytesMigrated += bytes
						task.UpdatedAt = time.Now().Format(time.RFC3339)
						tasksMu.Unlock()

						taskLog.Debug("Incremental key synced", map[string]interface{}{
							"key":   key,
							"bytes": bytes,
						})
					} else if reason == "skipped" {
						// å¢é‡é˜¶æ®µçš„å†²çªè·³è¿‡ä¹Ÿéœ€è¦ç»Ÿè®¡
						skippedInIncr++
						newKeysSkipped++
						tasksMu.Lock()
						task.KeysSkipped++
						task.UpdatedAt = time.Now().Format(time.RFC3339)
						tasksMu.Unlock()
						addErrorKey(task.ID, key, "string", "skipped", "Key already exists in target (incremental)")
					} else if reason != "" {
						tasksMu.Lock()
						task.KeysFailed++
						task.UpdatedAt = time.Now().Format(time.RFC3339)
						tasksMu.Unlock()
						taskLog.Warn("Failed to sync incremental key", map[string]interface{}{
							"key":    key,
							"reason": reason,
						})
						addErrorKey(task.ID, key, "string", "failed", reason+" (incremental)")
					}
				}
			}

			// æ›´æ–°æ€»keyæ•°ï¼ˆåªåœ¨æºç«¯keyæ•°å¢åŠ æ—¶æ›´æ–°ï¼‰
			newTotal, _ := getDBSize(ctx, sourceClient, sourceIsCluster)
			tasksMu.Lock()
			if newTotal > task.KeysTotal {
				task.KeysTotal = newTotal
			}
			tasksMu.Unlock()

			if newKeysFound > 0 || newKeysSkipped > 0 {
				taskLog.Info("Incremental sync progress", map[string]interface{}{
					"new_keys_synced":        newKeysFound,
					"new_keys_skipped":       newKeysSkipped,
					"total_synced_in_incr":   syncedInIncr,
					"total_skipped_in_incr":  skippedInIncr,
				})
			}
		}
	}
}

// addErrorKey æ·»åŠ é”™è¯¯Keyè®°å½•
func addErrorKey(taskID, key, keyType, reason, detail string) {
	errorKeyMu.Lock()
	defer errorKeyMu.Unlock()

	if errorKeys[taskID] == nil {
		errorKeys[taskID] = []ErrorKey{}
	}

	// é™åˆ¶æœ€å¤§è®°å½•æ•°
	if len(errorKeys[taskID]) < 10000 {
		errorKeys[taskID] = append(errorKeys[taskID], ErrorKey{
			Key:       key,
			Type:      keyType,
			Reason:    reason,
			Detail:    detail,
			Timestamp: time.Now().Format(time.RFC3339),
		})
	}
}

// errorKeysHandler è·å–é”™è¯¯Keyåˆ—è¡¨
func errorKeysHandler(w http.ResponseWriter, r *http.Request, id string, log *logger.RequestLogger) {
	errorKeyMu.RLock()
	keys := errorKeys[id]
	errorKeyMu.RUnlock()

	tasksMu.RLock()
	task, ok := tasks[id]
	tasksMu.RUnlock()

	if !ok {
		jsonResponse(w, map[string]interface{}{"code": 404, "message": "Task not found"})
		return
	}

	// ç»Ÿè®¡
	stats := map[string]int64{
		"total":      int64(len(keys)),
		"failed":     task.KeysFailed,
		"skipped":    task.KeysSkipped,
		"large_keys": 0,
	}

	// åªè¿”å›å‰100æ¡
	items := keys
	if len(items) > 100 {
		items = items[:100]
	}

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data": map[string]interface{}{
			"stats": stats,
			"items": items,
		},
	})
}

// downloadErrorKeysHandler ä¸‹è½½é”™è¯¯Key CSV
func downloadErrorKeysHandler(w http.ResponseWriter, r *http.Request, id string, log *logger.RequestLogger) {
	errorKeyMu.RLock()
	keys := errorKeys[id]
	errorKeyMu.RUnlock()

	// ç”ŸæˆCSV
	var sb strings.Builder
	sb.WriteString("Key,Type,Reason,Detail,Timestamp\n")
	for _, k := range keys {
		sb.WriteString(fmt.Sprintf("\"%s\",\"%s\",\"%s\",\"%s\",\"%s\"\n",
			strings.ReplaceAll(k.Key, "\"", "\"\""),
			k.Type,
			k.Reason,
			strings.ReplaceAll(k.Detail, "\"", "\"\""),
			k.Timestamp,
		))
	}

	w.Header().Set("Content-Type", "text/csv; charset=utf-8")
	w.Header().Set("Content-Disposition", fmt.Sprintf("attachment; filename=\"error-keys-%s.csv\"", id[:8]))
	w.Write([]byte(sb.String()))

	log.Info("Error keys downloaded", map[string]interface{}{"task_id": id, "count": len(keys)})
}

func systemHandler(w http.ResponseWriter, r *http.Request, log *logger.RequestLogger) {
	running := 0
	tasksMu.RLock()
	for _, t := range tasks {
		if t.Status == "running" {
			running++
		}
	}
	tasksMu.RUnlock()

	log.Debug("System status queried")

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data": map[string]interface{}{
			"status":        "running",
			"worker_count":  4,
			"running_tasks": running,
			"total_tasks":   len(tasks),
			"uptime":        time.Since(startTime).String(),
			"memory_mb":     getMemoryUsage(),
		},
	})
}

// ==================== æµ‹è¯•è¿æ¥ API ====================

func testConnectionHandler(w http.ResponseWriter, r *http.Request, log *logger.RequestLogger) {
	if r.Method != "POST" {
		http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
		return
	}

	var req struct {
		Addrs    []string `json:"addrs"`
		Password string   `json:"password"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		log.Error("Failed to decode request", map[string]interface{}{"error": err.Error()})
		jsonResponse(w, map[string]interface{}{"code": 400, "message": "Invalid request: " + err.Error()})
		return
	}

	// è¿‡æ»¤ç©ºåœ°å€
	var addrs []string
	for _, addr := range req.Addrs {
		if addr != "" {
			addrs = append(addrs, addr)
		}
	}
	if len(addrs) == 0 {
		jsonResponse(w, map[string]interface{}{"code": 400, "message": "è‡³å°‘éœ€è¦ä¸€ä¸ªé›†ç¾¤åœ°å€"})
		return
	}

	log.Info("Testing connection", map[string]interface{}{
		"addrs": addrs,
	})

	startTime := time.Now()
	ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
	defer cancel()

	// å…ˆå°è¯•é›†ç¾¤æ¨¡å¼è¿æ¥
	clusterClient := redis.NewClusterClient(&redis.ClusterOptions{
		Addrs:    addrs,
		Password: req.Password,
	})
	defer clusterClient.Close()

	if err := clusterClient.Ping(ctx).Err(); err == nil {
		// é›†ç¾¤æ¨¡å¼è¿æ¥æˆåŠŸ
		info := getClusterInfo(ctx, clusterClient)
		info["mode"] = "cluster"

		log.Info("Cluster connection successful", map[string]interface{}{
			"mode":       "cluster",
			"node_count": info["node_count"],
			"latency_ms": time.Since(startTime).Milliseconds(),
		})

		jsonResponse(w, map[string]interface{}{
			"code":    0,
			"message": "success",
			"data": map[string]interface{}{
				"success":      true,
				"message":      "é›†ç¾¤è¿æ¥æˆåŠŸ",
				"cluster_info": info,
				"latency_ms":   time.Since(startTime).Milliseconds(),
			},
		})
		return
	}

	// å°è¯•å•æœºæ¨¡å¼è¿æ¥
	standaloneClient := redis.NewClient(&redis.Options{
		Addr:     addrs[0],
		Password: req.Password,
	})
	defer standaloneClient.Close()

	if err := standaloneClient.Ping(ctx).Err(); err != nil {
		log.Error("Connection failed", map[string]interface{}{
			"error":      err.Error(),
			"latency_ms": time.Since(startTime).Milliseconds(),
		})

		jsonResponse(w, map[string]interface{}{
			"code":    0,
			"message": "success",
			"data": map[string]interface{}{
				"success":    false,
				"message":    "è¿æ¥å¤±è´¥: " + err.Error(),
				"latency_ms": time.Since(startTime).Milliseconds(),
			},
		})
		return
	}

	// å•æœºæ¨¡å¼è¿æ¥æˆåŠŸ
	info := getStandaloneInfo(ctx, standaloneClient, addrs[0])
	info["mode"] = "standalone"

	log.Info("Standalone connection successful", map[string]interface{}{
		"mode":       "standalone",
		"latency_ms": time.Since(startTime).Milliseconds(),
	})

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data": map[string]interface{}{
			"success":      true,
			"message":      "å•æœºæ¨¡å¼è¿æ¥æˆåŠŸ",
			"cluster_info": info,
			"latency_ms":   time.Since(startTime).Milliseconds(),
		},
	})
}

func getClusterInfo(ctx context.Context, client *redis.ClusterClient) map[string]interface{} {
	info := map[string]interface{}{
		"nodes":        []map[string]interface{}{},
		"node_count":   0,
		"total_keys":   int64(0),
		"total_memory": int64(0),
		"version":      "unknown",
	}

	var nodes []map[string]interface{}
	var totalKeys int64
	var totalMemory int64
	var version string
	var mu sync.Mutex

	// è·å–é›†ç¾¤èŠ‚ç‚¹ä¿¡æ¯
	client.ForEachMaster(ctx, func(ctx context.Context, node *redis.Client) error {
		nodeInfo := map[string]interface{}{
			"role": "master",
		}

		// è·å–èŠ‚ç‚¹åœ°å€
		opts := node.Options()
		nodeInfo["addr"] = opts.Addr

		// è·å–DBSize
		if dbsize, err := node.DBSize(ctx).Result(); err == nil {
			nodeInfo["keys"] = dbsize
			mu.Lock()
			totalKeys += dbsize
			mu.Unlock()
		}

		// è·å–å†…å­˜ä½¿ç”¨
		if memInfo, err := node.Info(ctx, "memory").Result(); err == nil {
			mem := parseMemoryFromInfo(memInfo)
			nodeInfo["memory"] = mem
			mu.Lock()
			totalMemory += mem
			mu.Unlock()
		}

		// è·å–Redisç‰ˆæœ¬ï¼ˆåªéœ€è·å–ä¸€æ¬¡ï¼‰
		mu.Lock()
		if version == "" {
			if serverInfo, err := node.Info(ctx, "server").Result(); err == nil {
				version = parseVersionFromInfo(serverInfo)
			}
		}
		mu.Unlock()

		mu.Lock()
		nodes = append(nodes, nodeInfo)
		mu.Unlock()
		return nil
	})

	info["nodes"] = nodes
	info["node_count"] = len(nodes)
	info["total_keys"] = totalKeys
	info["total_memory"] = totalMemory
	info["version"] = version

	return info
}

func getStandaloneInfo(ctx context.Context, client *redis.Client, addr string) map[string]interface{} {
	info := map[string]interface{}{
		"node_count":   1,
		"total_keys":   int64(0),
		"total_memory": int64(0),
		"version":      "unknown",
	}

	nodeInfo := map[string]interface{}{
		"addr": addr,
		"role": "master",
	}

	// è·å–DBSize
	if dbsize, err := client.DBSize(ctx).Result(); err == nil {
		nodeInfo["keys"] = dbsize
		info["total_keys"] = dbsize
	}

	// è·å–å†…å­˜å’Œç‰ˆæœ¬ä¿¡æ¯
	if serverInfo, err := client.Info(ctx, "server").Result(); err == nil {
		info["version"] = parseVersionFromInfo(serverInfo)
	}
	if memInfo, err := client.Info(ctx, "memory").Result(); err == nil {
		mem := parseMemoryFromInfo(memInfo)
		nodeInfo["memory"] = mem
		info["total_memory"] = mem
	}

	info["nodes"] = []map[string]interface{}{nodeInfo}
	return info
}

func parseVersionFromInfo(info string) string {
	for _, line := range strings.Split(info, "\n") {
		line = strings.TrimSpace(line)
		if strings.HasPrefix(line, "redis_version:") {
			return strings.TrimPrefix(line, "redis_version:")
		}
	}
	return "unknown"
}

func parseMemoryFromInfo(info string) int64 {
	for _, line := range strings.Split(info, "\n") {
		line = strings.TrimSpace(line)
		if strings.HasPrefix(line, "used_memory:") {
			memStr := strings.TrimPrefix(line, "used_memory:")
			mem, _ := strconv.ParseInt(memStr, 10, 64)
			return mem
		}
	}
	return 0
}

// analyzeClusterHandler åˆ†æé›†ç¾¤è¯¦ç»†ä¿¡æ¯
func analyzeClusterHandler(w http.ResponseWriter, r *http.Request, log *logger.RequestLogger) {
	if r.Method != "POST" {
		http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
		return
	}

	var req struct {
		Addrs    []string `json:"addrs"`
		Password string   `json:"password"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		jsonResponse(w, map[string]interface{}{"code": 400, "message": "Invalid request"})
		return
	}

	ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
	defer cancel()

	info, err := analyzeCluster(ctx, req.Addrs, req.Password)
	if err != nil {
		log.Error("Failed to analyze cluster", map[string]interface{}{"error": err.Error()})
		jsonResponse(w, map[string]interface{}{"code": 500, "message": err.Error()})
		return
	}

	log.Info("Cluster analyzed", map[string]interface{}{
		"total_keys": info.TotalKeys,
		"is_cluster": info.IsCluster,
	})

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data":    info,
	})
}

// analyzeCluster åˆ†æé›†ç¾¤è¯¦ç»†ä¿¡æ¯
func analyzeCluster(ctx context.Context, addrs []string, password string) (*ClusterInfo, error) {
	info := &ClusterInfo{
		Addrs: addrs,
	}

	// å°è¯•é›†ç¾¤æ¨¡å¼
	clusterClient := redis.NewClusterClient(&redis.ClusterOptions{
		Addrs:    addrs,
		Password: password,
	})

	if err := clusterClient.Ping(ctx).Err(); err == nil {
		info.IsCluster = true
		defer clusterClient.Close()

		var mu sync.Mutex
		var totalKeys int64
		var totalMemory int64
		var maxMemory int64
		var masterCount int
		var connectedClients int
		var instantaneousOPS int64
		var maxClients int

		clusterClient.ForEachMaster(ctx, func(ctx context.Context, node *redis.Client) error {
			masterCount++

			// DBSize
			if dbsize, err := node.DBSize(ctx).Result(); err == nil {
				mu.Lock()
				totalKeys += dbsize
				mu.Unlock()
			}

			// Memory info
			if memInfo, err := node.Info(ctx, "memory").Result(); err == nil {
				mem := parseMemoryFromInfo(memInfo)
				maxMem := parseMaxMemoryFromInfo(memInfo)
				mu.Lock()
				totalMemory += mem
				if maxMem > maxMemory {
					maxMemory = maxMem
				}
				mu.Unlock()
			}

			// Stats info
			if statsInfo, err := node.Info(ctx, "stats").Result(); err == nil {
				ops := parseOPSFromInfo(statsInfo)
				mu.Lock()
				instantaneousOPS += ops
				mu.Unlock()
			}

			// Clients info
			if clientInfo, err := node.Info(ctx, "clients").Result(); err == nil {
				clients := parseConnectedClientsFromInfo(clientInfo)
				mu.Lock()
				connectedClients += clients
				mu.Unlock()
			}

			// Server info (version, maxclients)
			if serverInfo, err := node.Info(ctx, "server").Result(); err == nil {
				mu.Lock()
				if info.Version == "" {
					info.Version = parseVersionFromInfo(serverInfo)
				}
				mu.Unlock()
			}

			// Config maxclients
			if result, err := node.ConfigGet(ctx, "maxclients").Result(); err == nil && len(result) >= 2 {
				if mcStr, ok := result[1].(string); ok {
					mc, _ := strconv.Atoi(mcStr)
					mu.Lock()
					if mc > maxClients {
						maxClients = mc
					}
					mu.Unlock()
				}
			}

			return nil
		})

		info.MasterCount = masterCount
		info.TotalKeys = totalKeys
		info.UsedMemory = totalMemory
		info.UsedMemoryHuman = formatBytes(totalMemory)
		info.MaxMemory = maxMemory
		info.MaxClients = maxClients
		info.ConnectedClients = connectedClients
		info.InstantaneousOPS = instantaneousOPS

		// ä¼°ç®—å¹³å‡keyå¤§å°
		if totalKeys > 0 {
			info.AvgKeySize = totalMemory / totalKeys
		}

		return info, nil
	}
	clusterClient.Close()

	// å°è¯•å•æœºæ¨¡å¼
	standaloneClient := redis.NewClient(&redis.Options{
		Addr:     addrs[0],
		Password: password,
	})
	defer standaloneClient.Close()

	if err := standaloneClient.Ping(ctx).Err(); err != nil {
		return nil, err
	}

	info.IsCluster = false
	info.MasterCount = 1

	if dbsize, err := standaloneClient.DBSize(ctx).Result(); err == nil {
		info.TotalKeys = dbsize
	}

	if memInfo, err := standaloneClient.Info(ctx, "memory").Result(); err == nil {
		info.UsedMemory = parseMemoryFromInfo(memInfo)
		info.UsedMemoryHuman = formatBytes(info.UsedMemory)
		info.MaxMemory = parseMaxMemoryFromInfo(memInfo)
	}

	if statsInfo, err := standaloneClient.Info(ctx, "stats").Result(); err == nil {
		info.InstantaneousOPS = parseOPSFromInfo(statsInfo)
	}

	if clientInfo, err := standaloneClient.Info(ctx, "clients").Result(); err == nil {
		info.ConnectedClients = parseConnectedClientsFromInfo(clientInfo)
	}

	if serverInfo, err := standaloneClient.Info(ctx, "server").Result(); err == nil {
		info.Version = parseVersionFromInfo(serverInfo)
	}

	if result, err := standaloneClient.ConfigGet(ctx, "maxclients").Result(); err == nil && len(result) >= 2 {
		if mcStr, ok := result[1].(string); ok {
			info.MaxClients, _ = strconv.Atoi(mcStr)
		}
	}

	if info.TotalKeys > 0 {
		info.AvgKeySize = info.UsedMemory / info.TotalKeys
	}

	return info, nil
}

// recommendConfigHandler æ¨èé…ç½®
func recommendConfigHandler(w http.ResponseWriter, r *http.Request, log *logger.RequestLogger) {
	if r.Method != "POST" {
		http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
		return
	}

	var req struct {
		SourceCluster struct {
			Addrs    []string `json:"addrs"`
			Password string   `json:"password"`
		} `json:"source_cluster"`
		TargetCluster struct {
			Addrs    []string `json:"addrs"`
			Password string   `json:"password"`
		} `json:"target_cluster"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		jsonResponse(w, map[string]interface{}{"code": 400, "message": "Invalid request"})
		return
	}

	ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
	defer cancel()

	// åˆ†ææºç«¯é›†ç¾¤
	sourceInfo, err := analyzeCluster(ctx, req.SourceCluster.Addrs, req.SourceCluster.Password)
	if err != nil {
		jsonResponse(w, map[string]interface{}{"code": 500, "message": "åˆ†ææºç«¯é›†ç¾¤å¤±è´¥: " + err.Error()})
		return
	}

	// åˆ†æç›®æ ‡ç«¯é›†ç¾¤
	targetInfo, err := analyzeCluster(ctx, req.TargetCluster.Addrs, req.TargetCluster.Password)
	if err != nil {
		jsonResponse(w, map[string]interface{}{"code": 500, "message": "åˆ†æç›®æ ‡ç«¯é›†ç¾¤å¤±è´¥: " + err.Error()})
		return
	}

	// ç”Ÿæˆæ¨èé…ç½®
	config := generateRecommendedConfig(sourceInfo, targetInfo)

	log.Info("Config recommended", map[string]interface{}{
		"worker_count":    config.WorkerCount,
		"estimated_speed": config.EstimatedSpeed,
	})

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data": map[string]interface{}{
			"source_info": sourceInfo,
			"target_info": targetInfo,
			"recommended": config,
		},
	})
}

// generateRecommendedConfig ç”Ÿæˆæ¨èé…ç½®
func generateRecommendedConfig(source, target *ClusterInfo) *RecommendedConfig {
	config := &RecommendedConfig{
		ScanBatchSize:     1000,
		LargeKeyThreshold: 10 * 1024 * 1024, // 10MB
	}

	var reasons []string

	// 1. è®¡ç®—Workeræ•°é‡
	// åŸºäºå¤šä¸ªå› ç´ ï¼šæºç«¯è´Ÿè½½ã€è¿æ¥æ•°é™åˆ¶ã€CPUæ ¸å¿ƒæ•°
	cpuCores := runtime.NumCPU()
	maxWorkersByCPU := cpuCores * 4 // æ¯æ ¸4ä¸ªworker

	// åŸºäºè¿æ¥æ•°é™åˆ¶
	sourceMaxConns := source.MaxClients - source.ConnectedClients
	targetMaxConns := target.MaxClients - target.ConnectedClients
	if sourceMaxConns < 100 {
		sourceMaxConns = 100
	}
	if targetMaxConns < 100 {
		targetMaxConns = 100
	}
	maxWorkersByConn := min(sourceMaxConns/3, targetMaxConns/3) // æ¯workeréœ€è¦çº¦3ä¸ªè¿æ¥

	// åŸºäºæºç«¯å½“å‰è´Ÿè½½
	var maxWorkersByLoad int
	if source.InstantaneousOPS < 1000 {
		// ä½è´Ÿè½½ï¼Œå¯ä»¥æ¿€è¿›é…ç½®
		maxWorkersByLoad = 100
		reasons = append(reasons, "æºç«¯è´Ÿè½½è¾ƒä½(OPS<1000)ï¼Œå¯ä½¿ç”¨è¾ƒå¤šWorker")
	} else if source.InstantaneousOPS < 10000 {
		// ä¸­ç­‰è´Ÿè½½
		maxWorkersByLoad = 50
		reasons = append(reasons, "æºç«¯è´Ÿè½½ä¸­ç­‰ï¼ŒWorkeræ•°é‡é€‚ä¸­")
	} else {
		// é«˜è´Ÿè½½ï¼Œä¿å®ˆé…ç½®
		maxWorkersByLoad = 20
		reasons = append(reasons, "æºç«¯è´Ÿè½½è¾ƒé«˜ï¼ŒWorkeræ•°é‡ä¿å®ˆè®¾ç½®")
	}

	// å–æœ€å°å€¼
	config.WorkerCount = min(maxWorkersByCPU, min(maxWorkersByConn, maxWorkersByLoad))
	if config.WorkerCount < 4 {
		config.WorkerCount = 4
	}
	if config.WorkerCount > 100 {
		config.WorkerCount = 100
	}

	// 2. è®¡ç®—QPSé™åˆ¶
	// æºç«¯ï¼šé¢„ç•™70%ç»™ä¸šåŠ¡ï¼Œè¿ç§»ä½¿ç”¨30%
	if source.InstantaneousOPS < 100 {
		// å‡ ä¹æ— ä¸šåŠ¡ï¼Œä¸é™åˆ¶
		config.SourceQPS = 0
		reasons = append(reasons, "æºç«¯å‡ ä¹æ— ä¸šåŠ¡è´Ÿè½½ï¼Œä¸é™åˆ¶QPS")
	} else {
		// ä¼°ç®—æœ€å¤§å®¹é‡ï¼ˆå‡è®¾å½“å‰æ˜¯ä¸šåŠ¡è´Ÿè½½çš„50%ï¼‰
		estimatedMaxOPS := source.InstantaneousOPS * 2
		if estimatedMaxOPS < 50000 {
			estimatedMaxOPS = 50000 // æœ€ä½å‡è®¾5ä¸‡
		}
		config.SourceQPS = int(estimatedMaxOPS * 30 / 100) // ä½¿ç”¨30%
		reasons = append(reasons, fmt.Sprintf("æºç«¯QPSé™åˆ¶ä¸ºé¢„ä¼°å®¹é‡çš„30%%(%d)", config.SourceQPS))
	}

	// ç›®æ ‡ç«¯ï¼šé€šå¸¸å¯ä»¥æ›´æ¿€è¿›
	if target.InstantaneousOPS < 100 {
		config.TargetQPS = 0
		reasons = append(reasons, "ç›®æ ‡ç«¯å‡ ä¹æ— è´Ÿè½½ï¼Œä¸é™åˆ¶QPS")
	} else {
		estimatedMaxOPS := target.InstantaneousOPS * 2
		if estimatedMaxOPS < 50000 {
			estimatedMaxOPS = 50000
		}
		config.TargetQPS = int(estimatedMaxOPS * 50 / 100) // ä½¿ç”¨50%
	}

	// 3. è®¡ç®—è¿æ¥æ•°
	// æ¯ä¸ªWorkeréœ€è¦çº¦2-3ä¸ªè¿æ¥
	config.SourceConnections = config.WorkerCount * 3
	config.TargetConnections = config.WorkerCount * 3

	// ç¡®ä¿ä¸è¶…è¿‡å¯ç”¨è¿æ¥æ•°çš„50%
	if config.SourceConnections > sourceMaxConns/2 {
		config.SourceConnections = sourceMaxConns / 2
	}
	if config.TargetConnections > targetMaxConns/2 {
		config.TargetConnections = targetMaxConns / 2
	}

	// æœ€å°è¿æ¥æ•°
	if config.SourceConnections < 10 {
		config.SourceConnections = 10
	}
	if config.TargetConnections < 10 {
		config.TargetConnections = 10
	}

	// 4. ä¼°ç®—è¿ç§»é€Ÿåº¦å’Œæ—¶é—´
	// åŸºäºkeyå¤§å°ä¼°ç®—å•workeråå
	var singleWorkerSpeed int64
	if source.AvgKeySize < 1024 { // < 1KB
		singleWorkerSpeed = 500
	} else if source.AvgKeySize < 10*1024 { // < 10KB
		singleWorkerSpeed = 200
	} else if source.AvgKeySize < 100*1024 { // < 100KB
		singleWorkerSpeed = 50
	} else { // >= 100KB
		singleWorkerSpeed = 10
	}

	config.EstimatedSpeed = singleWorkerSpeed * int64(config.WorkerCount)

	// QPSé™åˆ¶å¯èƒ½æˆä¸ºç“¶é¢ˆ
	if config.SourceQPS > 0 && int64(config.SourceQPS) < config.EstimatedSpeed*4 {
		// æ¯ä¸ªkeyéœ€è¦çº¦4æ¬¡æ“ä½œï¼ŒQPSé™åˆ¶å¯èƒ½å½±å“é€Ÿåº¦
		config.EstimatedSpeed = int64(config.SourceQPS) / 4
	}

	// ä¼°ç®—æ—¶é—´
	if config.EstimatedSpeed > 0 && source.TotalKeys > 0 {
		seconds := source.TotalKeys / config.EstimatedSpeed
		config.EstimatedTime = formatDuration(seconds)
	} else {
		config.EstimatedTime = "æ— æ³•ä¼°ç®—"
	}

	// 5. å¤§Keyé˜ˆå€¼
	if source.AvgKeySize > 1024*1024 { // å¹³å‡å¤§äº1MB
		config.LargeKeyThreshold = 5 * 1024 * 1024 // 5MB
		reasons = append(reasons, "æ£€æµ‹åˆ°è¾ƒå¤§çš„å¹³å‡Keyå¤§å°ï¼Œè°ƒä½å¤§Keyé˜ˆå€¼")
	}

	config.Reason = strings.Join(reasons, "ï¼›")

	return config
}

// è¾…åŠ©å‡½æ•°
func parseMaxMemoryFromInfo(info string) int64 {
	for _, line := range strings.Split(info, "\n") {
		line = strings.TrimSpace(line)
		if strings.HasPrefix(line, "maxmemory:") {
			memStr := strings.TrimPrefix(line, "maxmemory:")
			mem, _ := strconv.ParseInt(memStr, 10, 64)
			return mem
		}
	}
	return 0
}

func parseOPSFromInfo(info string) int64 {
	for _, line := range strings.Split(info, "\n") {
		line = strings.TrimSpace(line)
		if strings.HasPrefix(line, "instantaneous_ops_per_sec:") {
			opsStr := strings.TrimPrefix(line, "instantaneous_ops_per_sec:")
			ops, _ := strconv.ParseInt(opsStr, 10, 64)
			return ops
		}
	}
	return 0
}

func parseConnectedClientsFromInfo(info string) int {
	for _, line := range strings.Split(info, "\n") {
		line = strings.TrimSpace(line)
		if strings.HasPrefix(line, "connected_clients:") {
			clientsStr := strings.TrimPrefix(line, "connected_clients:")
			clients, _ := strconv.Atoi(clientsStr)
			return clients
		}
	}
	return 0
}

func formatBytes(bytes int64) string {
	if bytes < 1024 {
		return fmt.Sprintf("%d B", bytes)
	} else if bytes < 1024*1024 {
		return fmt.Sprintf("%.2f KB", float64(bytes)/1024)
	} else if bytes < 1024*1024*1024 {
		return fmt.Sprintf("%.2f MB", float64(bytes)/(1024*1024))
	}
	return fmt.Sprintf("%.2f GB", float64(bytes)/(1024*1024*1024))
}

func formatDuration(seconds int64) string {
	if seconds < 60 {
		return fmt.Sprintf("%dç§’", seconds)
	} else if seconds < 3600 {
		return fmt.Sprintf("%dåˆ†%dç§’", seconds/60, seconds%60)
	} else if seconds < 86400 {
		return fmt.Sprintf("%då°æ—¶%dåˆ†", seconds/3600, (seconds%3600)/60)
	}
	return fmt.Sprintf("%då¤©%då°æ—¶", seconds/86400, (seconds%86400)/3600)
}

func min(a, b int) int {
	if a < b {
		return a
	}
	return b
}
func templatesHandler(w http.ResponseWriter, r *http.Request, log *logger.RequestLogger) {
	switch r.Method {
	case "GET":
		listTemplates(w, r, log)
	case "POST":
		createTemplate(w, r, log)
	default:
		http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
	}
}

// listTemplates è·å–æ¨¡æ¿åˆ—è¡¨
func listTemplates(w http.ResponseWriter, r *http.Request, log *logger.RequestLogger) {
	templateMu.RLock()
	defer templateMu.RUnlock()

	items := make([]*TaskTemplate, 0, len(templates))
	for _, t := range templates {
		items = append(items, t)
	}

	// æŒ‰åˆ›å»ºæ—¶é—´æ’åº
	sort.Slice(items, func(i, j int) bool {
		return items[i].CreatedAt > items[j].CreatedAt
	})

	log.Debug("Templates listed", map[string]interface{}{"count": len(items)})

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data": map[string]interface{}{
			"items": items,
			"total": len(items),
		},
	})
}

// createTemplate åˆ›å»ºæ¨¡æ¿
func createTemplate(w http.ResponseWriter, r *http.Request, log *logger.RequestLogger) {
	var req struct {
		Name          string `json:"name"`
		Description   string `json:"description"`
		SourceCluster struct {
			Addrs    []string `json:"addrs"`
			Password string   `json:"password"`
		} `json:"source_cluster"`
		TargetCluster struct {
			Addrs    []string `json:"addrs"`
			Password string   `json:"password"`
		} `json:"target_cluster"`
		MigrationMode string       `json:"migration_mode"`
		Options       *TaskOptions `json:"options"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		log.Error("Failed to decode request", map[string]interface{}{"error": err.Error()})
		jsonResponse(w, map[string]interface{}{"code": 400, "message": err.Error()})
		return
	}

	if req.Name == "" {
		jsonResponse(w, map[string]interface{}{"code": 400, "message": "æ¨¡æ¿åç§°ä¸èƒ½ä¸ºç©º"})
		return
	}

	template := &TaskTemplate{
		ID:             uuid.New().String(),
		Name:           req.Name,
		Description:    req.Description,
		SourceCluster:  strings.Join(req.SourceCluster.Addrs, ","),
		TargetCluster:  strings.Join(req.TargetCluster.Addrs, ","),
		SourcePassword: req.SourceCluster.Password,
		TargetPassword: req.TargetCluster.Password,
		MigrationMode:  req.MigrationMode,
		Options:        req.Options,
		CreatedAt:      time.Now().Format(time.RFC3339),
		UpdatedAt:      time.Now().Format(time.RFC3339),
	}

	templateMu.Lock()
	templates[template.ID] = template
	templateMu.Unlock()

	log.Info("Template created", map[string]interface{}{
		"template_id":   template.ID,
		"template_name": template.Name,
	})

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data":    map[string]string{"template_id": template.ID},
	})
}

// templateHandler å¤„ç†å•ä¸ªæ¨¡æ¿è¯·æ±‚
func templateHandler(w http.ResponseWriter, r *http.Request, log *logger.RequestLogger) {
	path := strings.TrimPrefix(r.URL.Path, "/api/v1/templates/")
	parts := strings.Split(path, "/")

	if len(parts) == 0 || parts[0] == "" {
		http.NotFound(w, r)
		return
	}

	id := parts[0]
	action := ""
	if len(parts) > 1 {
		action = parts[1]
	}

	switch r.Method {
	case "GET":
		getTemplate(w, r, id, log)
	case "PUT":
		updateTemplate(w, r, id, log)
	case "DELETE":
		deleteTemplate(w, r, id, log)
	case "POST":
		if action == "create-task" {
			createTaskFromTemplate(w, r, id, log)
		} else {
			http.NotFound(w, r)
		}
	default:
		http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
	}
}

// getTemplate è·å–æ¨¡æ¿è¯¦æƒ…
func getTemplate(w http.ResponseWriter, r *http.Request, id string, log *logger.RequestLogger) {
	templateMu.RLock()
	template, ok := templates[id]
	templateMu.RUnlock()

	if !ok {
		jsonResponse(w, map[string]interface{}{"code": 404, "message": "Template not found"})
		return
	}

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data":    template,
	})
}

// updateTemplate æ›´æ–°æ¨¡æ¿
func updateTemplate(w http.ResponseWriter, r *http.Request, id string, log *logger.RequestLogger) {
	templateMu.Lock()
	template, ok := templates[id]
	if !ok {
		templateMu.Unlock()
		jsonResponse(w, map[string]interface{}{"code": 404, "message": "Template not found"})
		return
	}

	var req struct {
		Name          string `json:"name"`
		Description   string `json:"description"`
		SourceCluster struct {
			Addrs    []string `json:"addrs"`
			Password string   `json:"password"`
		} `json:"source_cluster"`
		TargetCluster struct {
			Addrs    []string `json:"addrs"`
			Password string   `json:"password"`
		} `json:"target_cluster"`
		MigrationMode string       `json:"migration_mode"`
		Options       *TaskOptions `json:"options"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		templateMu.Unlock()
		jsonResponse(w, map[string]interface{}{"code": 400, "message": err.Error()})
		return
	}

	if req.Name != "" {
		template.Name = req.Name
	}
	if req.Description != "" {
		template.Description = req.Description
	}
	if len(req.SourceCluster.Addrs) > 0 {
		template.SourceCluster = strings.Join(req.SourceCluster.Addrs, ",")
		template.SourcePassword = req.SourceCluster.Password
	}
	if len(req.TargetCluster.Addrs) > 0 {
		template.TargetCluster = strings.Join(req.TargetCluster.Addrs, ",")
		template.TargetPassword = req.TargetCluster.Password
	}
	if req.MigrationMode != "" {
		template.MigrationMode = req.MigrationMode
	}
	if req.Options != nil {
		template.Options = req.Options
	}
	template.UpdatedAt = time.Now().Format(time.RFC3339)
	templateMu.Unlock()

	log.Info("Template updated", map[string]interface{}{"template_id": id})

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
	})
}

// deleteTemplate åˆ é™¤æ¨¡æ¿
func deleteTemplate(w http.ResponseWriter, r *http.Request, id string, log *logger.RequestLogger) {
	templateMu.Lock()
	_, ok := templates[id]
	if !ok {
		templateMu.Unlock()
		jsonResponse(w, map[string]interface{}{"code": 404, "message": "Template not found"})
		return
	}
	delete(templates, id)
	templateMu.Unlock()

	log.Info("Template deleted", map[string]interface{}{"template_id": id})

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
	})
}

// createTaskFromTemplate ä»æ¨¡æ¿åˆ›å»ºä»»åŠ¡
func createTaskFromTemplate(w http.ResponseWriter, r *http.Request, id string, log *logger.RequestLogger) {
	templateMu.RLock()
	template, ok := templates[id]
	templateMu.RUnlock()

	if !ok {
		jsonResponse(w, map[string]interface{}{"code": 404, "message": "Template not found"})
		return
	}

	// å¯é€‰ï¼šå…è®¸è¦†ç›–éƒ¨åˆ†å‚æ•°
	var req struct {
		Name string `json:"name"`
	}
	json.NewDecoder(r.Body).Decode(&req)

	taskName := req.Name
	if taskName == "" {
		taskName = template.Name + "-" + time.Now().Format("0102-1504")
	}

	mode := template.MigrationMode
	if mode == "" {
		mode = "full_and_incremental"
	}

	options := template.Options
	if options == nil {
		options = &TaskOptions{
			WorkerCount:       4,
			ScanBatchSize:     1000,
			ConflictPolicy:    "skip_full_only",
			LargeKeyThreshold: 10485760,
			EnableCompression: true,
			KeyFilter:         &KeyFilter{Mode: "all"},
		}
	} else if options.KeyFilter == nil || options.KeyFilter.Mode == "" {
		if options.KeyFilter == nil {
			options.KeyFilter = &KeyFilter{Mode: "all"}
		} else {
			options.KeyFilter.Mode = "all"
		}
	}

	task := &Task{
		ID:             uuid.New().String(),
		Name:           taskName,
		Status:         "pending",
		Progress:       0,
		SourceCluster:  template.SourceCluster,
		TargetCluster:  template.TargetCluster,
		SourcePassword: template.SourcePassword,
		TargetPassword: template.TargetPassword,
		MigrationMode:  mode,
		CreatedAt:      time.Now().Format(time.RFC3339),
		UpdatedAt:      time.Now().Format(time.RFC3339),
		Phase:          "full",
		Options:        options,
	}

	tasksMu.Lock()
	tasks[task.ID] = task
	tasksMu.Unlock()

	log.Info("Task created from template", map[string]interface{}{
		"task_id":     task.ID,
		"template_id": id,
		"task_name":   taskName,
	})

	logger.WithTask(task.ID).Info("Task created from template", map[string]interface{}{
		"name":        task.Name,
		"template_id": id,
		"source":      task.SourceCluster,
		"target":      task.TargetCluster,
	})

	jsonResponse(w, map[string]interface{}{
		"code":    0,
		"message": "success",
		"data":    map[string]string{"task_id": task.ID},
	})
}
